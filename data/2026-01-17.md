<div id=toc></div>

# Table of Contents

- [Ziniu Wu](#Ziniu Wu) [Total: 3]
- [cs.DB](#cs.DB) [Total: 5]
- [learned "cost model"](#learned "cost model") [Total: 6]
- [Google Scholar](#Google Scholar) [Total: 3]


<div id='Ziniu Wu'></div>

# Ziniu Wu [[Back]](#toc)

### [1] [Markovian Pre-Trained Transformer for Next-Item Recommendation](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.08275&hl=zh-CN&sa=X&d=4748469596494327208&ei=3NlqaYuJI7ux6rQPmKbEyA4&scisig=AHkA5jSFQdKBtVneTXYrtYrgD9ma&oi=scholaralrt&hist=Pxo5FIAAAAAJ:11734074576040036222:AHkA5jT4qH64J8nZejvQlyvZo1xT&html=&pos=0&folt=rel)
*C Xu,G Li,J Wang,W Zhang*

Main category: Ziniu Wu

TL;DR: 提出了基于马尔可夫链预训练的通用推荐模型 MPT，仅通过微调轻量适配器即可在多个真实数据集上达到先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有先进序列推荐模型实际上主要依赖用户最近一次交互进行预测，历史行为仅用于推断用户非序列化的身份特征；因此，理想的通用推荐模型应能有效总结用户行为序列，尤其关注最新交互。为构建具备这种能力且可迁移的模型，作者提出在合成马尔可夫链上进行全模型预训练。

Method: 设计 Markovian Pre-trained Transformer (MPT)，在大量可控合成马尔可夫链上进行完全预训练，使其学习从上下文估计状态转移概率并聚焦于最后一个状态；在下游任务中仅微调一个轻量级适配器进行迁移。

Result: 在来自三个平台的五个公开数据集上进行了广泛实验，结果表明 MPT 在性能上优于传统推荐预训练方法和近期基于语言模型的预训练范式。

Conclusion: MPT 证明了在合成马尔可夫链上预训练的可行性与有效性，揭示了序列推荐中的“马尔可夫性”本质，并提供了一种高效、可迁移的通用推荐建模范式。

Abstract: We introduce the Markovian Pre-trained Transformer (MPT) for next-item recommendation, a transferable model fully pre-trained on synthetic Markov chains, yet capable of achieving state-of-the-art performance by fine-tuning a lightweight adaptor. This counterintuitive success stems from the observation of the `Markovian' nature: advanced sequential recommenders coincidentally rely on the latest interaction to make predictions, while the historical interactions serve mainly as auxiliary cues for inferring the user's general, non-sequential identity. This characteristic necessitates the capabilities of a universal recommendation model to effectively summarize the user sequence, with particular emphasis on the latest interaction. MPT inherently has the potential to be universal and transferable. On the one hand, when trained to predict the next state of Markov chains, it acquires the capabilities to estimate transition probabilities from the context (one adaptive manner for summarizing sequences) and attend to the last state to ensure accurate state transitions. On the other hand, unlike the heterogeneous interaction data, an unlimited amount of controllable Markov chains is available to boost the model capacity. We conduct extensive experiments on five public datasets from three distinct platforms to validate the superiority of Markovian pre-training over traditional recommendation pre-training and recent language pre-training paradigms.

</details>


### [2] [Serverless Elasticsearch: the Transition from Stateful to Stateless](https://scholar.google.com/scholar_url?url=https://www.cs.ubc.ca/~brendan/papers/serverless-elasticsearch.pdf&hl=zh-CN&sa=X&d=4663664700620309864&ei=3NlqaYuJI7ux6rQPmKbEyA4&scisig=AHkA5jQ42xzoAOOsQlkP2_egdEN7&oi=scholaralrt&hist=Pxo5FIAAAAAJ:11734074576040036222:AHkA5jT4qH64J8nZejvQlyvZo1xT&html=&pos=1&folt=rel)
*I Psaroudakis,P Salehi,J Bryan,FF Castaño,B Cully…*

Main category: Ziniu Wu

TL;DR: 本文提出了一种面向Elasticsearch的新型无服务器架构，通过将计算与存储解耦，并将数据卸载至高可用、低成本的对象存储，简化了原有的多层硬件配置为仅索引和搜索两层，同时保持原有API和读写语义。该架构显著降低了存储上传成本，并实现了线性扩展和更高的索引吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统Elasticsearch采用有状态的共享无架构，计算与存储耦合，需依赖多级硬件（如hot、warm、cold、frozen）来平衡成本、性能与可用性，管理复杂且扩展受限。为解决这些问题，作者旨在设计一种无状态、无服务器的新架构，以提升可扩展性并简化系统运维。

Method: 作者将Elasticsearch的计算与存储解耦，将索引文件、事务日志和集群元数据等数据卸载到高可用的对象存储中。通过自定义批处理提交格式封装索引数据，批量上传事务日志，并优化对象存储中的文件删除机制，从而大幅降低上传开销。同时，维持原有API兼容性和读写一致性语义。

Result: 实验表明，相比传统有状态Elasticsearch，在相同计算硬件下，该无服务器架构可实现高达2倍的索引吞吐量，并支持索引与搜索层独立线性扩展。此外，索引数据上传成本最多降低100倍，事务日志上传成本最多降低30倍。

Conclusion: 所提出的无服务器Elasticsearch架构通过计算与存储解耦，不仅简化了系统层级结构，还显著提升了可扩展性与成本效益，为大规模数据分析场景提供了高效、弹性的解决方案。

Abstract: Elasticsearch (ES) is a distributed search and analytics database consisting of a cluster of nodes, each hosting a disjoint subset of the data. Until recently ES had a shared-nothing architecture that relied on local disks to store cluster data such as index files, transaction logs, and cluster state metadata. In this stateful architecture, compute is coupled with storage, and this leads to different tiers (eg, hot, warm, cold, and frozen) of hardware and configurations that the administrator chooses from to balance cost, performance, and high availability. In this paper, we show the technical design of a new serverless architecture, focusing on decoupling compute from storage, and offloading data to an affordable highly available object store, while supporting the same APIs and read-after-write semantics. Specifically, we show why and how this stateless architecture simplifies the data tiers to just two: indexing and search. This architecture supports indexing and searching practically limitless data while scaling each tier independently. Furthermore, we describe how we wrap index data into a custom batch commit format onto the object store to decrease upload costs by up to 100x, how we batch transaction log uploads to decrease upload costs by up to 30x, and how we delete files from the object store. We experimentally show that Serverless ES can get twice the indexing throughput of (stateful) ES on comparable compute hardware by using cloud object storage for durability instead of replication, and can scale linearly to match ingestion workloads.

</details>


### [3] [Automated Data Quality Frameworks for Healthcare Data Lakes](https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Narasimha-Chaitanya-Samineni/publication/399697229_Automated_Data_Quality_Frameworks_for_Healthcare_Data_Lakes/links/69657c35c906f117f2a45653/Automated-Data-Quality-Frameworks-for-Healthcare-Data-Lakes.pdf&hl=zh-CN&sa=X&d=5626713935160258952&ei=3NlqaYuJI7ux6rQPmKbEyA4&scisig=AHkA5jQYqaaNBdGUKxtpksZ-lWCB&oi=scholaralrt&hist=Pxo5FIAAAAAJ:11734074576040036222:AHkA5jT4qH64J8nZejvQlyvZo1xT&html=&pos=2&folt=rel)
*NC Samineni*

Main category: Ziniu Wu

TL;DR: 本文提出了一种面向医疗数据湖的自动化数据质量（ADQ）框架，融合规则与机器学习方法，支持异常检测与治理对齐的质量评分，旨在提升医疗数据在摄入、转换和消费各阶段的质量保障能力。


<details>
  <summary>Details</summary>
Motivation: 随着医疗数据湖中数据规模与异构性不断增长，传统人工数据质量检查难以应对模式漂移、缺失值、临床编码错误、摄入错误及时间依赖性不一致等问题，亟需可扩展的自动化解决方案。

Method: 提出一个综合性的ADQ框架，结合基于规则与机器学习的验证机制，引入异常检测技术，并构建与治理策略对齐的数据质量评分模型；采用元数据驱动、自动化、实时操作且符合监管要求的设计架构。

Result: 提供了该ADQ框架的架构蓝图、大规模数据质量指标表、验证指南及实施建议，适用于医疗机构、支付方和分析平台，有效支持医疗数据湖全生命周期的数据质量管理。

Conclusion: 所提出的ADQ框架通过自动化、元数据驱动和合规性设计，能够有效应对医疗数据湖中复杂多变的数据质量问题，为数字健康生态系统的高质量数据需求提供可靠支撑。

Abstract: Healthcare data lakes serve as unified repositories that integrate electronic health records (EHR), claims data, laboratory systems, imaging metadata, device-generated patient streams, and financial information into scalable analytical environments. As these data ecosystems expand in volume and heterogeneity, ensuring data quality becomes increasingly difficult. Manual data quality checks are insufficient for detecting schema drift, missing values, inaccurate clinical codes, ingestion errors, or time-dependent inconsistencies. Automated Data Quality (ADQ) frameworks introduce scalable mechanisms to evaluate, score, and enforce data quality rules across ingestion, transformation, and consumption layers. This research article proposes a comprehensive ADQ framework for healthcare data lakes, integrates rule-based and machine-learning-driven validation, introduces anomaly detection techniques, and formalizes a governance-aligned scoring model. The study contributes an architectural blueprint, large data quality metrics tables, validation guidelines, and implementation recommendations for health systems, payers, and analytics platforms. The framework emphasizes automation, metadata-driven design, regulatory alignment, and real-time operability to meet the evolving data needs of digital health ecosystems.[1][3][6][8][10]

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [4] [Multiverse: Transactional Memory with Dynamic Multiversioning](https://arxiv.org/abs/2601.09735)
*Gaetano Coccimiglio,Trevor Brown,Srivatsan Ravi*

Main category: cs.DB

TL;DR: Multiverse 是一种新型软件事务内存（STM）系统，结合了无版本 STM 和多版本 STM 的优势，支持无版本事务与版本化事务并发执行，在常规工作负载下性能媲美现有无版本 STM，同时在存在长读和频繁更新的场景中显著优于其他 STM。


<details>
  <summary>Details</summary>
Motivation: 现有 STM 系统难以高效支持访问大量频繁更新地址的长读操作；多版本机制虽可解决此问题，但会损害无需版本控制的事务性能。因此，亟需一种兼顾两类工作负载的 STM 设计。

Method: 提出 Multiverse，一种混合 STM 架构，同时支持无版本事务和版本化事务，并允许二者并发执行。其设计目标是在保留无版本事务高性能的同时，高效支持长读所需的版本化事务。

Result: 实验表明，Multiverse 在无长读的常规工作负载下性能与当前最优无版本 STM 相当甚至更优；而在包含长读和频繁更新的工作负载下，其吞吐量比现有 STM 高出数个数量级。

Conclusion: Multiverse 成功融合了无版本与多版本 STM 的优点，在不同工作负载下均表现出色，尤其在支持长读操作方面具有显著优势，为 STM 的实际应用提供了更灵活高效的解决方案。

Abstract: Software transactional memory (STM) allows programmers to easily implement concurrent data structures. STMs simplify atomicity. Recent STMs can achieve good performance for some workloads but they have some limitations. In particular, STMs typically cannot support long-running reads which access a large number of addresses that are frequently updated. Multiversioning is a common approach used to support this type of workload. However, multiversioning is often expensive and can reduce the performance of transactions where versioning is not necessary. In this work we present Multiverse, a new STM that combines the best of both unversioned TM and multiversioning. Multiverse features versioned and unversioned transactions which can execute concurrently. A main goal of Multiverse is to ensure that unversioned transactions achieve performance comparable to the state of the art unversioned STM while still supporting fast versioned transactions needed to enable long running reads. We implement Multiverse and compare it against several STMs. Our experiments demonstrate that Multiverse achieves comparable or better performance for common case workloads where there are no long running reads. For workloads with long running reads and frequent updates Multiverse significantly outperforms existing STMS. In several cases for these workloads the throughput of Multiverse is several orders of magnitude faster than other STMs.

</details>


### [5] [The "I" in FAIR: Translating from Interoperability in Principle to Interoperation in Practice](https://arxiv.org/abs/2601.10008)
*Evan Morris,Gaurav Vaidya,Phil Owen,Jason Reilly,Karamarie Fecho,Patrick Wang,Yaphet Kebede,E. Kathleen Carter,Chris Bizon*

Main category: cs.DB

TL;DR: 该论文提出了两个工具Babel和ORION，分别解决科学数据资源中标识符不一致和数据模型异构的问题，从而弥合FAIR原则与实际互操作之间的差距，并构建了一个完全互操作的知识库集合。


<details>
  <summary>Details</summary>
Motivation: 尽管许多科学数据资源遵循FAIR原则并采用标准化词汇和元数据，但由于标识符方案和数据模型的差异，这些资源在实践中仍难以有效互操作。

Method: 开发了两个工具：Babel通过生成经过人工校对的标识符映射，构建等价标识符团（cliques）并通过高性能API提供服务；ORION则将不同知识库摄入并转换为统一的、社区管理的数据模型。

Result: Babel和ORION成功支持了数据互操作，构建了一个可下载使用的完全互操作知识库集合（https://robokop.renci.org）。

Conclusion: 通过Babel和ORION，可以在实际应用中实现FAIR原则所倡导的互操作性，显著提升科学数据资源的整合与利用效率。

Abstract: The FAIR (Findable, Accessible, Interoperable, and Reusable) data principles [1] promote the interoperability of scientific data by encouraging the use of persistent identifiers, standardized vocabularies, and formal metadata structures. Many resources are created using vocabularies that are FAIR-compliant and well-annotated, yet the collective ecosystem of these resources often fails to interoperate effectively in practice. This continued challenge is mainly due to variation in identifier schemas and data models used in these resources. We have created two tools to bridge the chasm between interoperability in principle and interoperation in practice. Babel solves the problem of multiple identifier schemes by producing a curated set of identifier mappings to create cliques of equivalent identifiers that are exposed through high-performance APIs. ORION solves the problems of multiple data models by ingesting knowledge bases and transforming them into a common, community-managed data model. Here, we describe Babel and ORION and demonstrate their ability to support data interoperation. A library of fully interoperable knowledge bases created through the application of Babel and ORION is available for download and use at https://robokop.renci.org.

</details>


### [6] [Redundancy-Driven Top-$k$ Functional Dependency Discovery](https://arxiv.org/abs/2601.10130)
*Xiaolong Wan,Xixian Han*

Main category: cs.DB

TL;DR: SDP is a novel algorithm for efficiently discovering the top-$k$ functional dependencies (FDs) in relational data by ranking them based on redundancy count and pruning the search space using a monotonic upper bound, significantly outperforming exhaustive FD discovery methods in speed and memory usage.


<details>
  <summary>Details</summary>
Motivation: Existing FD discovery algorithms suffer from high computational complexity and produce excessively large result sets, making them impractical for large-scale or high-dimensional data and hindering the identification of truly useful FDs.

Method: The authors propose SDP (Selective-Discovery-and-Prune), which ranks FDs by redundancy count and uses a provably monotonic upper bound on this measure to prune unpromising search branches. The method is enhanced with three optimizations: attribute ordering by partition cardinality, use of a Partition Cardinality Matrix for tighter bounds, and a global scheduler for prioritizing promising branches.

Result: Experiments on over 40 datasets demonstrate that SDP is significantly faster and consumes less memory compared to exhaustive FD discovery approaches.

Conclusion: SDP effectively addresses the scalability and usability issues of traditional FD discovery by focusing on the most informative dependencies and leveraging efficient pruning strategies, making FD discovery practical for real-world, large-scale datasets.

Abstract: Functional dependencies (FDs) are basic constraints in relational databases and are used for many data management tasks. Most FD discovery algorithms find all valid dependencies, but this causes two problems. First, the computational cost is prohibitive: computational complexity grows quadratically with the number of tuples and exponentially with the number of attributes, making discovery slow on large-scale and high-dimensional data. Second, the result set can be huge, making it hard to identify useful dependencies. We propose SDP (Selective-Discovery-and-Prune), which discovers the top-$k$ FDs ranked by redundancy count. Redundancy count measures how much duplicated information an FD explains and connects directly to storage overhead and update anomalies. SDP uses an upper bound on redundancy to prune the search space. It is proved that this upper bound is monotone: adding attributes refines partitions and thus decreases the bound. Once the bound falls below the top-$k$ threshold, the entire branch can be skipped. We improve SDP with three optimizations: ordering attributes by partition cardinality, using pairwise statistics in a Partition Cardinality Matrix to tighten bounds, and a global scheduler to explore promising branches first. Experiments on over 40 datasets show that SDP is much faster and uses less memory than exhaustive methods.

</details>


### [7] [Improving Database Performance by Application-side Transaction Merging](https://arxiv.org/abs/2601.10596)
*Xueyuan Ren,Frank Li,Yang Wang*

Main category: cs.DB

TL;DR: 该论文提出通过在应用端合并结构相似的SQL语句或事务来提升事务处理性能，并设计了名为TransactionMerger的中间件，结合静态分析工具识别合并机会，在TPC-C和Spree应用中分别实现了最高2.65倍和3.52倍的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 传统事务处理系统在面对大量结构相似但独立提交的事务时，存在冗余操作和资源争用问题，限制了整体吞吐性能。作者旨在通过在应用侧合并这些事务，减少冗余读取、利用SQL语义优化语句，并预计算跨事务的聚合效果，从而提升系统效率。

Method: 作者提出TransactionMerger中间件，用于跨客户端收集并合并结构相似的事务；同时开发静态分析工具，在不违反事务隔离性的前提下识别可合并机会。具体技术包括：1）基于SQL语义合并相似语句；2）消除冗余读操作；3）跨事务合并竞争语句并通过预计算其聚合效果实现优化。

Result: 在TPC-C基准测试中，事务合并使吞吐量提升最高达2.65倍；在真实电商应用Spree中，吞吐量提升高达3.52倍，验证了所提方法的有效性。

Conclusion: 通过在应用层对结构相似事务进行合并，可在保持隔离性的同时显著提升数据库系统的吞吐性能，TransactionMerger及其配套静态分析工具为实际系统提供了可行的优化路径。

Abstract: This paper explores a new opportunity to improve the performance of transaction processing at the application side by merging structurely similar statements or transactions. Concretely, we re-write transactions to 1) merge similar statements using specific SQL semantics; 2) eliminate redundant reads; and 3) merge contending statements across transactions by pre-computing their aggregated effect. Following this idea, we present the design of TransactionMerger, a middleware to collect and merge transactions across different clients. We further present a static analysis tool to identify the merging opportunity without violating isolation as well as our experience of re-writing transactions in TPC-C and Spree, a popular real-world application. Our evaluation shows that such transaction merging can improve TPC-C throughput by up to 2.65X and Spree throughput by 3.52X.

</details>


### [8] [Translating database mathematical schemes into relational database software applications with MatBase](https://arxiv.org/abs/2601.10604)
*Christian Mancas,Diana Christina Mancas*

Main category: cs.DB

TL;DR: 本文提出了一种将初等数学数据模型转换为关系模型及非关系约束的伪代码算法，并证明其高效、可靠、完备且最优，还通过家谱树子宇宙建模进行了应用示例。


<details>
  <summary>Details</summary>
Motivation: 为支持智能数据库管理系统 MatBase，需将数学数据模型自动、正确地转换为可实现的关系数据库结构及相应的非关系约束。

Method: 设计并形式化一种伪代码算法，用于将（初等）数学数据模型方案翻译成关系模式和非关系约束集；并通过理论分析证明其性能与完备性；同时以家谱树建模为例进行应用验证，并提供 SQL 与 VBA 实现非关系约束的示例和开发指南。

Result: 该算法被证明具有快速性、稳固性、完备性和最优性；成功应用于家谱树子宇宙的数学建模，并提供了具体的非关系约束实现代码示例。

Conclusion: 所提出的算法有效支持了从抽象数学数据模型到具体关系数据库实现的自动转换，具备良好的理论性质和实用价值，尤其适用于需要复杂语义约束的领域如家谱建模。

Abstract: We present a pseudocode algorithm for translating our (Elementary) Mathematical Data Model schemes into relational ones and associated sets of non-relational constraints, used by MatBase, our intelligent database management system prototype. We prove that this algorithm is very fast, solid, complete, and optimal. We apply it to a mathematical scheme modeling the genealogical trees subuniverse. We also provide examples of SQL and VBA code for enforcing some of its non-relational constraints, as well as guidelines to develop code for enforcing such constraints.

</details>


<div id='learned "cost model"'></div>

# learned "cost model" [[Back]](#toc)

### [9] [Cost-Integrated AI Meta-Models for Mine-to-Mill Optimisation: Linking Fragmentation, Throughput, and Operating Costs Across the Value Chain](https://scholar.google.com/scholar_url?url=https://www.mdpi.com/2075-163X/16/1/73&hl=zh-CN&sa=X&d=16844214200484325598&ei=sx1qafjzNIeUywSZ68jgDw&scisig=AHkA5jQ9bRqECazo8fXt-ghAmU3E&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=0&folt=kw-top)
*P Nobahar,C Xu,P Dowd*

Main category: learned "cost model"

TL;DR: 本文提出了一种集成的、成本感知的人工智能元建模框架，用于矿山到选厂（mine-to-mill）全流程优化，结合高保真仿真与数据驱动建模，实现对上游爆破参数如何影响下游成本与产能的快速预测与优化。


<details>
  <summary>Details</summary>
Motivation: 传统矿山优化通常局限于单阶段，缺乏对全价值链中技术参数与成本之间复杂非线性关系的系统性理解，亟需一种能够整合物理仿真、机器学习与经济指标的综合决策支持方法。

Method: 利用集成提取仿真器（IES）生成超三百万个场景，构建基于随机森林的元模型以预测不同爆破设计参数（如孔距、装药密度等）对筛分、破碎、堆存和磨矿等下游环节的影响；同时建立分阶段成本模型，将工艺变量与单位吨成本关联，并通过AI元模型实现快速评估。

Result: 元模型预测准确率超过90%；研究发现更细的破碎粒度虽增加炸药成本，但可显著降低总粉碎成本；半自磨（SAG）机负荷与转速与成本呈U型关系，存在明确最优操作窗口。

Conclusion: 所提框架通过融合物理仿真、机器学习与成本建模，实现了从分阶段优化向全链条、财务驱动的智能决策转变，为构建支持实时自适应优化的数字孪生系统奠定基础，有助于提升资源开采效率与可持续性。

Abstract: This study presents an integrated, cost-aware artificial intelligence (AI) meta-modelling framework for mine-to-mill optimisation that couples high-fidelity simulation with data-driven predictive modelling. Using over three million scenarios generated in the Integrated Extraction Simulator (IES), the framework quantifies how upstream design parameters such as burden, spacing, hole diameter, and explosive density propagate through screening, crushing, stockpiling, and grinding to affect downstream costs and throughput. Random Forest-based meta-models achieved predictive accuracies above 90%, enabling the rapid evaluation of technical and financial trade-offs across the mining value chain. Stage-wise cost models were formulated for drilling, blasting, comminution, and material handling to link process variables to costs per tonne. The results reveal clear non-linear cost responses: finer fragmentation reduces the total comminution cost despite higher explosive expenditure, while SAG mill load and speed exhibit U-shaped cost relationships with distinct optimal operating windows. By combining physics-based simulations, machine learning, and cost integration, the framework transforms traditional stage-wise optimisation into a holistic, financially informed decision-support system. The proposed methodology supports real-time, AI-enabled digital twins capable of adaptive mine-to-mill optimisation, paving the way for more efficient and sustainable resource extraction.

</details>


### [10] [QuantEval: A Benchmark for Financial Quantitative Tasks in Large Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.08689&hl=zh-CN&sa=X&d=3836145994708819270&ei=sx1qafjzNIeUywSZ68jgDw&scisig=AHkA5jTC6iFaz1BLsxww27E38kPg&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=2&folt=kw-top)
*Z Kang,J Gong,W Hu,S Yin,K Jiang,Z Fang,Y He…*

Main category: learned "cost model"

TL;DR: 提出了QuantEval，一个涵盖金融知识问答、量化数学推理和策略编码的多维基准，并引入CTA风格回测框架以评估LLM在量化金融中的真实能力。


<details>
  <summary>Details</summary>
Motivation: 现有对大语言模型（LLM）在金融量化任务中的评估较为零散，主要局限于知识型问答，缺乏对数学推理和策略编码能力的系统性评测。

Method: 构建QuantEval基准，包含三个维度：知识问答、量化数学推理和量化策略编码；引入确定性CTA风格回测框架，对模型生成的交易策略进行执行与绩效评估；并对主流开源与闭源LLM进行评测，同时开展有监督微调和强化学习实验。

Result: 评测发现当前LLM在量化推理和策略编码方面与人类专家存在显著差距；通过领域对齐数据的微调和强化学习可带来一致性能提升。

Conclusion: QuantEval为评估LLM在量化金融中的能力提供了更全面、真实的基准，有助于推动相关研究并促进LLM在实际交易流程中的应用；同时公开了完整的回测配置以确保可复现性。

Abstract: Large Language Models (LLMs) have shown strong capabilities across many domains, yet their evaluation in financial quantitative tasks remains fragmented and mostly limited to knowledge-centric question answering. We introduce QuantEval, a benchmark that evaluates LLMs across three essential dimensions of quantitative finance: knowledge-based QA, quantitative mathematical reasoning, and quantitative strategy coding. Unlike prior financial benchmarks, QuantEval integrates a CTA-style backtesting framework that executes model-generated strategies and evaluates them using financial performance metrics, enabling a more realistic assessment of quantitative coding ability. We evaluate some state-of-the-art open-source and proprietary LLMs and observe substantial gaps to human experts, particularly in reasoning and strategy coding. Finally, we conduct large-scale supervised fine-tuning and reinforcement learning experiments on domain-aligned data, demonstrating consistent improvements. We hope QuantEval will facilitate research on LLMs' quantitative finance capabilities and accelerate their practical adoption in real-world trading workflows. We additionally release the full deterministic backtesting configuration (asset universe, cost model, and metric definitions) to ensure strict reproducibility.

</details>


### [11] [Instruction Tuning for Large Language Models: RLHF, Supervised Fine-Tuning, and Alignment Strategies](https://scholar.google.com/scholar_url?url=https://www.techrxiv.org/doi/pdf/10.36227/techrxiv.176784494.43758400&hl=zh-CN&sa=X&d=5741429599054319445&ei=sx1qafjzNIeUywSZ68jgDw&scisig=AHkA5jQUyt4JfaDTY50oUgJw6dtv&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=4&folt=kw-top)
*Y Ding,X Han,J Yang,T Wang,Z Bi,X Song,J Hao…*

Main category: learned "cost model"

TL;DR: 本综述全面回顾了大语言模型（LLM）指令微调的完整流程，涵盖数据收集、全参数与参数高效微调方法以及评估协议，并探讨了未来在自动化数据生成、自适应优化和鲁棒评估方面的发展方向。


<details>
  <summary>Details</summary>
Motivation: 为使大语言模型更好地对齐人类意图、安全约束和特定领域需求，需系统梳理指令微调的关键技术环节，以指导研究者和实践者构建高效且可靠对齐的模型。

Method: 综述性分析，将数据构建分为专家标注、大模型蒸馏和自改进机制三类；微调方法包括全参数监督训练与低秩适配（LoRA）、前缀微调等参数高效策略；评估方面关注多语言多模态下的忠实性、效用性和安全性，并考察医疗、法律、金融等领域的专用基准。

Result: 明确了不同数据构建范式在质量、可扩展性和资源成本之间的权衡；总结了各类微调方法在计算效率与模型复用性方面的表现；揭示了当前评估体系在复杂场景下面临的挑战，并指出了领域专用评估的发展趋势。

Conclusion: 推动指令微调LLM的发展需更紧密地融合数据、算法与人类反馈，未来应聚焦于自动化数据生成、自适应优化及鲁棒评估框架的构建。

Abstract: Instruction tuning is a pivotal technique for aligning large language models (LLMs) with human intentions, safety constraints, and domain-specific requirements. This survey provides a comprehensive overview of the full pipeline, encompassing (i) data collection methodologies,(ii) full-parameter and parameterefficient fine-tuning strategies, and (iii) evaluation protocols. We categorized data construction into three major paradigms: expert annotation, distillation from larger models, and self-improvement mechanisms, each offering distinct trade-offs between quality, scalability, and resource cost. Fine-tuning techniques range from conventional supervised training to lightweight approaches, such as low-rank adaptation (LoRA) and prefix tuning, with a focus on computational efficiency and model reusability. We further examine the challenges of evaluating faithfulness, utility, and safety across multilingual and multimodal scenarios, highlighting the emergence of domain-specific benchmarks in healthcare, legal, and financial applications. Finally, we discuss promising directions for automated data generation, adaptive optimization, and robust evaluation frameworks, arguing that a closer integration of data, algorithms, and human feedback is essential for advancing instruction-tuned LLMs. This survey aims to serve as a practical reference for researchers and practitioners seeking to design LLMs that are both effective and reliably aligned with human intentions.

</details>


### [12] [GeoMIP: A Geometric-Topological and Dynamic Programming Framework for Enhanced Computational Tractability of Minimum Information Partition in Integrated …](https://scholar.google.com/scholar_url?url=https://www.mdpi.com/2076-3417/16/2/809&hl=zh-CN&sa=X&d=15624358317915932604&ei=sx1qafjzNIeUywSZ68jgDw&scisig=AHkA5jREyUcBuyy_JHrp3bf5JbgZ&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=5&folt=kw-top)
*J Díaz*

Main category: learned "cost model"

TL;DR: GeoMIP is a geometric–topological framework that accelerates the identification of the Minimum Information Partition (MIP) in Integrated Information Theory (IIT) by modeling system states as vertices on an n-dimensional hypercube graph and using dynamic programming and graph symmetries to drastically reduce computational cost.


<details>
  <summary>Details</summary>
Motivation: The exponential computational cost of finding the MIP in IIT limits its application to systems with more than ~15–20 variables, hindering analysis of realistic neural and complex systems.

Method: GeoMIP reformulates the MIP search as a graph optimization problem on the hypercube, where system states are vertices and edges reflect Hamming distance. It uses tensor-decomposed transition probabilities, dynamic programming over graph neighborhoods, BFS-like exploration by Hamming levels, and hypercube symmetries to avoid redundant computations.

Result: GeoMIP achieves 165–326× speedups over PyPhi with 98–100% agreement in MIP identification and enables IIT analyses up to ~25 variables through heuristic extensions.

Conclusion: By exploiting the hypercube’s graph structure and symmetries, GeoMIP transforms the intractable combinatorial MIP search into a scalable, graph-based procedure, significantly expanding the practical applicability of IIT.

Abstract: The computational tractability of Integrated Information Theory (IIT) is fundamentally constrained by the exponential cost of identifying the Minimum Information Partition (MIP), which is required to quantify integrated information (Φ). Existing approaches become impractical beyond ~15–20 variables, limiting IIT analyses on realistic neural and complex systems. We introduce GeoMIP, a geometric–topological framework that recasts the MIP search as a graph-based optimization problem on the n-dimensional hypercube graph: discrete system states are modeled as graph vertices, and Hamming distance adjacency defines edges and shortest-path structures. Building on a tensor-decomposed representation of the transition probabilities, GeoMIP constructs a transition-cost (ground cost) structure by dynamic programming over graph neighborhoods and BFS-like exploration by Hamming levels, exploiting hypercube symmetries to reduce redundant evaluations. We validate GeoMIP against PyPhi, ensuring reliability of MIP identification and Φ computation. Across multiple implementations, GeoMIP achieves 165–326× speedups over PyPhi while maintaining 98–100% agreement in partition identification. Heuristic extensions further enable analyses up to ~25 variables, substantially expanding the practical IIT regime. Overall, by leveraging the hypercube’s explicit graph structure (vertices, edges, shortest paths, and automorphisms), GeoMIP turns an intractable combinatorial search into a scalable graph-based procedure for IIT partitioning.

</details>


### [13] [Efficient Maintenance of Leiden Communities in Large Dynamic Graphs](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.08554&hl=zh-CN&sa=X&d=10445304588978131053&ei=sx1qafjzNIeUywSZ68jgDw&scisig=AHkA5jTs0Hw2ut028dZbVBaA7K_9&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=6&folt=kw-top)
*C Lin,Y Xie,Y Fang,Y Hu,Y Hu,C Cheng*

Main category: learned "cost model"

TL;DR: 本文针对动态图中Leiden社区检测算法更新效率低的问题，通过理论分析指出已有方法缺乏有界性保障，并提出一种高效增量维护算法HIT-Leiden，利用连通分量与层次社区结构显著缩小受影响顶点范围，在多种数据集上实现最高达五个数量级的加速。


<details>
  <summary>Details</summary>
Motivation: Leiden算法在大语言模型（如Graph-RAG）、异常检测和生物分析等场景中广泛应用，但面对频繁更新的大规模动态图，从头运行Leiden代价高昂；现有增量维护方法缺乏理论支撑且效率低下，亟需更高效且理论严谨的解决方案。

Method: 作者首先利用有界性分析（boundedness analysis）对现有增量算法进行理论评估，揭示其无界性缺陷，并分析图变化时顶点社区归属的变化规律；在此基础上，提出Hierarchical Incremental Tree Leiden（HIT-Leiden）算法，通过维护连通分量和层次化社区结构，有效限制受影响顶点的传播范围，从而实现高效增量更新。

Result: 在多个真实和合成数据集上的实验表明，HIT-Leiden显著优于现有方法，更新速度最高提升五个数量级。

Conclusion: HIT-Leiden通过结合理论分析与层次化增量维护策略，为动态图中的Leiden社区检测提供了一种高效、可扩展的解决方案，具有重要的实践价值。

Abstract: As a well-known community detection algorithm, Leiden has been widely used in various scenarios such as large language model generation (e.g., Graph-RAG), anomaly detection, and biological analysis. In these scenarios, the graphs are often large and dynamic, where vertices and edges are inserted and deleted frequently, so it is costly to obtain the updated communities by Leiden from scratch when the graph has changed. Recently, one work has attempted to study how to maintain Leiden communities in the dynamic graph, but it lacks a detailed theoretical analysis, and its algorithms are inefficient for large graphs. To address these issues, in this paper, we first theoretically show that the existing algorithms are relatively unbounded via the boundedness analysis (a powerful tool for analyzing incremental algorithms on dynamic graphs), and also analyze the memberships of vertices in communities when the graph changes. Based on theoretical analysis, we develop a novel efficient maintenance algorithm, called Hierarchical Incremental Tree Leiden (HIT-Leiden), which effectively reduces the range of affected vertices by maintaining the connected components and hierarchical community structures. Comprehensive experiments in various datasets demonstrate the superior performance of HIT-Leiden. In particular, it achieves speedups of up to five orders of magnitude over existing methods.

</details>


### [14] [THE OTHER SIDE OF THE WALL: A STUDY ON THE IMPACT OF PATERNAL INCARCERATION ON AFRICAN AMERICAN CHILDREN](https://scholar.google.com/scholar_url?url=https://dsc.duq.edu/cgi/viewcontent.cgi%3Farticle%3D3460%26context%3Detd&hl=zh-CN&sa=X&d=5536115719767251735&ei=sx1qafjzNIeUywSZ68jgDw&scisig=AHkA5jST1jBvoRqjC2MwKa0vRQTu&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=8&folt=kw-top)
*CM Wideman*

Main category: learned "cost model"

TL;DR: 该论文探讨了父亲入狱对成年子女在心理、人际关系和系统层面的长期影响。


<details>
  <summary>Details</summary>
Motivation: 研究旨在深入理解父系监禁作为一种童年逆境，如何通过心理、依恋和家庭系统机制持续影响子女成年后的发展与福祉。

Method: 采用质性研究方法，以童年不良经历（ACE）理论、依恋理论和家庭过程理论为指导框架，围绕两个核心研究问题展开分析。

Result: 暂未提供具体研究结果。

Conclusion: 暂未提供明确结论。

Abstract: This qualitative dissertation explores the long-term psychological, relational, and systemic impacts of paternal incarceration on adult children. Guided by Adverse Childhood Experiences (ACE) Theory, Attachment Theory, and Family Process Theory, this study examines two research questions:

</details>


<div id='Google Scholar'></div>

# Google Scholar [[Back]](#toc)

### [15] [Acyclic Join Sampling under Selections: Dichotomy, Union Sampling, and Enumeration](https://scholar.google.com/scholar_url?url=http://www.cse.cuhk.edu.hk/~taoyf/paper/icdt26.pdf&hl=en&sa=X&d=4831470035080384130&ei=62NpaYfqKtrJieoPjeLumAg&scisig=AHkA5jRGY0icDpoIemtIN9CdEiPp&oi=scholaralrt&hist=Pxo5FIAAAAAJ:6303853285888070373:AHkA5jQg0xFJQ1T_sclEfqikcIfn&html=&pos=0&folt=rel)
*J Huang,Y Tao,S Wang*

Main category: Google Scholar

TL;DR: 该论文系统研究了在运行时给定等值选择条件下，从无环连接结果中采样的复杂性问题，针对合取条件和析取范式条件分别提出了最优的O(IN)空间、O(1)采样时间的数据结构，并解决了相关的集合并采样与随机枚举问题。


<details>
  <summary>Details</summary>
Motivation: 以往关于连接采样的研究主要集中在不含选择条件的连接上，而实际数据库查询中普遍存在选择条件。因此，作者旨在系统研究在运行时给定等值选择条件下，从无环连接结果中高效采样的可行性与复杂性。

Method: 针对合取条件，基于广泛接受的复杂性假设，提出一个二分判定准则，刻画是否存在O(IN)空间、O(1)采样时间的预计算结构，并在可行情形下构造最优结构；针对析取范式条件，将其核心挑战归约为集合并采样问题，设计最优算法并构建相应采样结构。

Result: 1）对合取条件下的采样问题给出了基于条件形式的二分判定结果，并在所有可行情况下实现了O(IN)空间和O(1)采样时间的最优结构；2）对析取范式条件，提出了集合并采样的最优算法，并据此构建了最优采样结构；3）导出了关于随机枚举问题的新结果。

Conclusion: 该工作首次系统地解决了带运行时等值选择条件的无环连接采样问题，为合取与析取条件分别提供了理论完备性和实践最优性的解决方案，同时推动了相关随机枚举问题的研究。

Abstract: Previous research on join sampling has focused on joins without selection conditions, even though such conditions are prevalent in everyday queries in database systems. Motivated by this, we undertake a systematic investigation on the complexity of sampling from the result of an acyclic join under equality conditions given only at runtime. When conditions are conjunctive, the goal is to understand when it is possible to precompute a feasible structure that uses O (IN) space and supports sampling in O (1) time, where IN is the input size. We present a dichotomy to characterize (subject to a widely-accepted conjecture) the existence of such structures based on the conditions supplied and, in every feasible scenario, give an optimal structure of O (IN) space and O (1) sample time. We then extend our investigation to conditions expressed in disjunctive normal form, where the core challenge reduces to the fundamental set union sampling problem. We overcome the challenge with an optimal algorithm and utilize it to develop optimal sampling structures. Our findings also lead to new results on the closely-related random enumeration problem.

</details>


### [16] [Lower Bounds for the Algorithmic Complexity of Learned Indexes](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.06629&hl=en&sa=X&d=5601705435374249079&ei=62NpaYfqKtrJieoPjeLumAg&scisig=AHkA5jTZGx1GkeGOJ8OP_R5Q8rWG&oi=scholaralrt&hist=Pxo5FIAAAAAJ:6303853285888070373:AHkA5jQg0xFJQ1T_sclEfqikcIfn&html=&pos=1&folt=rel)
*LA Croquevielle,R Sokolovskii,T Heinis*

Main category: Google Scholar

TL;DR: 该论文提出一个通用框架，用于证明学习型索引结构在给定空间开销和模型类下的查询时间下界，揭示其内在的时空权衡与理论局限。


<details>
  <summary>Details</summary>
Motivation: 现有学习型索引虽在实践中有效，但其理论限制尚不清晰；作者旨在通过形式化方法建立查询时间的下界，以理解不同模型类和空间开销对性能的根本制约。

Method: 作者将学习型索引建模为分段模型预测器，结合概率工具（处理从分布中采样的数据）与逼近论方法（如量化和Kolmogorov宽度），分析如何用特定模型类最优逼近累积分布函数，从而推导查询时间下界。

Result: 在多种建模假设（尤其是分段线性和分段常数模型）和分布假设下，推导出学习型索引的查询时间下界，明确了其时空权衡关系。

Conclusion: 学习型索引的性能受限于所用模型类的逼近能力和空间开销，逼近论工具可有效刻画这些限制，为理解其理论边界提供了基础。

Abstract: Learned index structures aim to accelerate queries by training machine learning models to approximate the rank function associated with a database attribute. While effective in practice, their theoretical limitations are not fully understood. We present a general framework for proving lower bounds on query time for learned indexes, expressed in terms of their space overhead and parameterized by the model class used for approximation. Our formulation captures a broad family of learned indexes, including most existing designs, as piecewise model-based predictors. We solve the problem of lower bounding query time in two steps: first, we use probabilistic tools to control the effect of sampling when the database attribute is drawn from a probability distribution. Then, we analyze the approximation-theoretic problem of how to optimally represent a cumulative distribution function with approximators from a given model class. Within this framework, we derive lower bounds under a range of modeling and distributional assumptions, paying particular attention to the case of piecewise linear and piecewise constant model classes, which are common in practical implementations. Our analysis shows how tools from approximation theory, such as quantization and Kolmogorov widths, can be leveraged to formalize the space-time tradeoffs inherent to learned index structures. The resulting bounds illuminate core limitations of these methods.

</details>


### [17] [Database Theory in Action: Direct Access to Query Answers](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.06013&hl=en&sa=X&d=15361939681310771091&ei=62NpaYfqKtrJieoPjeLumAg&scisig=AHkA5jQxfzkEdYAo67lZ3g03S3Ix&oi=scholaralrt&hist=Pxo5FIAAAAAJ:6303853285888070373:AHkA5jQg0xFJQ1T_sclEfqikcIfn&html=&pos=2&folt=rel)
*J Hu,N Tziavelis*

Main category: Google Scholar

TL;DR: 该论文实现了支持多种查询和排序的直接访问（direct access）系统，填补了理论研究与实际性能评估之间的空白，并探讨了数据库系统在直接访问任务中的表现及其与单次访问（single-access）的关系。


<details>
  <summary>Details</summary>
Motivation: 尽管支持直接访问的数据结构在时间复杂度方面已有深入研究，且对许多查询和常见排序已知高效算法，但其实际性能却鲜有关注。本文旨在通过实现一个覆盖广泛查询和排序的系统，来研究这些方法在实践中的表现。

Method: 作者实现了一个支持多种查询类型和排序策略的直接访问系统，用于实证评估不同数据库系统在直接访问任务中的性能，并比较直接访问与其单次访问变体之间的关系。

Result: 实验揭示了不同数据库系统在直接访问场景下的性能差异，并提供了关于直接访问与单次访问之间性能关联的实证观察。

Conclusion: 直接访问的实际性能值得深入研究；通过广泛的实现和实验，本文为理解现有数据库系统在支持直接访问方面的优劣以及其与单次访问的联系提供了重要见解。

Abstract: Direct access asks for the retrieval of query answers by their ranked position, given a query and a desired order. While the time complexity of data structures supporting such accesses has been studied in depth, and efficient algorithms for many queries and common orders are known, their practical performance has received little attention. We provide an implementation covering a wide range of queries and orders; it allows us to investigate intriguing practical aspects, including the comparative performance of database systems and the relationship between direct access and its single-access counterpart.

</details>
