<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 4]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [AHA: Scalable Alternative History Analysis for Operational Timeseries Applications](https://arxiv.org/abs/2601.04432)
*Harshavardhan Kamarthi,Harshil Shah,Henry Milner,Sayan Sinha,Yan Li,B. Aditya Prakash,Vyas Sekar*

Main category: cs.DB

TL;DR: 该论文提出了一种名为AHA（Alternative History Analytics）的系统，用于高效且精确地对高维运维时序数据进行“替代历史分析”（如回溯性异常检测、告警配置实验等）。相比传统方案（如数据仓库、采样、大数据系统），AHA在保证100%分析准确率的同时，可将总拥有成本（计算+存储）降低高达85倍。


<details>
  <summary>Details</summary>
Motivation: 运维系统（如ISP、CDN、视频服务）常需对包含丰富元数据的高维时序性能指标进行回溯性分析（如评估异常检测算法、测试告警配置等），作者将此类负载称为“替代历史分析”。然而，现有数据处理方案在此类场景下面临高昂运营成本或无法保证重放准确性的问题。

Method: AHA系统的设计基于三方面洞察：1）底层统计量的可分解性；2）属性-值组合所定义的子群体在活跃数量上的稀疏性；3）现代分析数据库中聚合操作的高效结构。利用这些特性，AHA实现了兼顾成本效益与保真度的高维数据处理。

Result: 在多个真实世界数据集及一家大型视频分析公司的生产管道案例研究中，AHA在广泛的下游任务中实现了100%的准确性，并将总拥有成本（计算+存储）降低了高达85倍。

Conclusion: AHA系统通过利用替代历史分析工作负载的独特特性，成功解决了传统方法在成本和准确性上的权衡问题，为高维运维数据的回溯性分析提供了一个高效且精确的解决方案。

Abstract: Many operational systems collect high-dimensional timeseries data about users/systems on key performance metrics. For instance, ISPs, content distribution networks, and video delivery services collect quality of experience metrics for user sessions associated with metadata (e.g., location, device, ISP). Over such historical data, operators and data analysts often need to run retrospective analysis; e.g., analyze anomaly detection algorithms, experiment with different configurations for alerts, evaluate new algorithms, and so on. We refer to this class of workloads as alternative history analysis for operational datasets. We show that in such settings, traditional data processing solutions (e.g., data warehouses, sampling, sketching, big-data systems) either pose high operational costs or do not guarantee accurate replay. We design and implement a system, called AHA (Alternative History Analytics), that overcomes both challenges to provide cost efficiency and fidelity for high-dimensional data. The design of AHA is based on analytical and empirical insights about such workloads: 1) the decomposability of underlying statistics; 2) sparsity in terms of active number of subpopulations over attribute-value combinations; and 3) efficiency structure of aggregation operations in modern analytics databases. Using multiple real-world datasets and as well as case-studies on production pipelines at a large video analytics company, we show that AHA provides 100% accuracy for a broad range of downstream tasks and up to 85x lower total cost of ownership (i.e., compute + storage) compared to conventional methods.

</details>


### [2] [Structural Indexing of Relational Databases for the Evaluation of Free-Connex Acyclic Conjunctive Queries](https://arxiv.org/abs/2601.04757)
*Cristian Riveros,Benjamin Scheidt,Nicole Schweikardt*

Main category: cs.DB

TL;DR: 本文提出了一种基于数据库内部结构对称性的新型索引结构，通过构建辅助数据库 $D_{col}$，可在预处理时间与 $D_{col}$ 大小成线性关系的前提下，对任意自由连接无环合取查询（fc-ACQ）实现常数延迟枚举或计数。该索引利用关系着色精化（relational color refinement）技术，其大小在许多情况下远小于原始数据库，甚至可为常数。


<details>
  <summary>Details</summary>
Motivation: 传统索引方法（如B+树）依赖值或顺序信息，无法有效利用数据库中元组间的结构对称性。作者旨在开发一种能捕捉并利用这些内部结构对称性的索引机制，以提升fc-ACQ查询的评估效率，特别是在预处理和查询延迟方面取得优于数据库规模的性能。

Method: 作者为给定数据库 $D$ 构建一个辅助数据库 $D_{col}$，其构造基于Scheidt和Schweikardt（2025）提出的“关系着色精化”方法。该方法通过对数据库元组进行着色以识别结构对称性，$D_{col}$ 的大小等于着色后的颜色数量。利用该索引，可在 $O(|D_{col}|)$ 预处理时间内，对任意fc-ACQ实现答案计数或常数延迟枚举。

Result: 该方法在多种数据库结构上展现出显著优势：例如，对于二叉树，$D_{col}$ 的大小为对数级；对于正则图，其大小为常数。即使在最坏情况下（无任何结构对称性），$D_{col}$ 的大小仍为 $O(|D|)$。因此，这是首个利用数据库内部结构对称性实现优于数据库规模查询性能的基础性索引结果。

Conclusion: 本文提出的基于结构对称性的索引结构为高效评估fc-ACQ提供了一种新范式，其性能取决于数据库的内在对称性而非其绝对规模，在理论和实践上均具有重要意义。

Abstract: We present an index structure to boost the evaluation of free-connex acyclic conjunctive queries (fc-ACQs) over relational databases. The main ingredient of the index associated with a given database $D$ is an auxiliary database $D_{col}$. Our main result states that for any fc-ACQ $Q$ over $D$, we can count the number of answers of $Q$ or enumerate them with constant delay after a preprocessing phase that takes time linear in the size of $D_{col}$.
  Unlike previous indexing methods based on values or order (e.g., B+ trees), our index is based on structural symmetries among tuples in a database, and the size of $D_{col}$ is related to the number of colors assigned to $D$ by Scheidt and Schweikardt's "relational color refinement" (2025). In the particular case of graphs, this coincides with the minimal size of an equitable partition of the graph. For example, the size of $D_{col}$ is logarithmic in the case of binary trees and constant for regular graphs. Even in the worst-case that $D$ has no structural symmetries among tuples at all, the size of $D_{col}$ is still linear in the size of $D$.
  Given that the size of $D_{col}$ is bounded by the size of $D$ and can be much smaller (even constant for some families of databases), our index is the first foundational result on indexing internal structural symmetries of a database to evaluate all fc-ACQs with performance potentially strictly smaller than the database size.

</details>


### [3] [LGTD: Local-Global Trend Decomposition for Season-Length-Free Time Series Analysis](https://arxiv.org/abs/2601.04820)
*Chotanansub Sophaken,Thanadej Rattanakornphan,Piyanon Charoenpoonpanich,Thanapol Phungtua-eng,Chainarong Amornbunchornvej*

Main category: cs.DB

TL;DR: LGTD is a season-length-free time series decomposition method that models seasonal patterns as emergent from adaptive local trends, eliminating the need for predefined or estimated season lengths and improving robustness in heterogeneous and drifting periodic settings.


<details>
  <summary>Details</summary>
Motivation: Existing seasonal-trend decomposition methods often require user-specified or estimated season lengths and assume stable periodicity, which limits their robustness and applicability to real-world time series with irregular, drifting, or multi-scale recurring patterns.

Method: LGTD decomposes a time series into a smooth global trend, adaptive local trends (via an error-driven module called AutoTrend that segments the detrended signal into piecewise-linear regimes), and a residual. Seasonality emerges implicitly from repeated local trend structures rather than being explicitly modeled with fixed periods.

Result: Experiments on synthetic benchmarks show LGTD achieves robust and balanced decomposition performance across scenarios with fixed, transitive, and variable season lengths—particularly outperforming period-based methods when periodicity is unstable or weak. The method scales linearly with time series length under mild conditions.

Conclusion: LGTD provides a practical, automated decomposition framework that enhances deployability in large, heterogeneous time series collections by removing reliance on explicit season-length specification and adapting to complex temporal structures.

Abstract: Time series decomposition into trend, seasonal structure, and residual components is a core primitive for downstream analytics such as anomaly detection, change-point detection, and forecasting. However, most existing seasonal-trend decomposition methods rely on user-specified or estimated season lengths and implicitly assume stable periodic structure. These assumptions limit robustness and deployability in large, heterogeneous collections where recurring patterns may drift, appear intermittently, or exist at multiple time scales.
  We propose LGTD (Local-Global Trend Decomposition), a season-length-free decomposition framework that represents a time series as the sum of a smooth global trend, adaptive local trends whose recurrence induces implicit (emergent) seasonal structure, and a residual component. Rather than explicitly modeling seasonality through a fixed or estimated period, LGTD treats seasonal structure as an emergent property arising from repeated local trend regimes. Concretely, LGTD first estimates a global trend capturing long-term evolution, then applies AutoTrend, an adaptive error-driven local linear trend inference module, to segment the detrended signal into short-lived piecewise-linear regimes. Residuals are obtained after removing both global and local trends.
  By eliminating manual season-length specification, LGTD supports automated, low-touch deployment across time series with irregular, drifting, or weakly periodic structure. We analyze computational complexity and show that LGTD scales linearly with series length under mild conditions. Experiments on synthetic benchmarks demonstrate robust and balanced decomposition performance across fixed, transitive, and variable season-length settings, especially where period-based methods degrade.

</details>


### [4] [Rule Rewriting Revisited: A Fresh Look at Static Filtering for Datalog and ASP](https://arxiv.org/abs/2601.05108)
*Philipp Hanisch,Markus Krötzsch*

Main category: cs.DB

TL;DR: 该论文重新审视并推广了Kifer和Lozinskii于1986年提出的静态过滤方法，将其扩展至现代Datalog系统及回答集编程（ASP），尽管其通用形式具有双指数复杂度，但作者提出了可处理的近似算法，在典型场景中仍能显著提升规则系统的性能。


<details>
  <summary>Details</summary>
Motivation: 静态过滤作为一种数据无关的Datalog优化方法，虽早在1986年就被提出，但在近期研究与系统开发中被忽视，且其特例被反复独立重新发现。因此，有必要以现代术语重新阐述该方法，并拓展其适用范围。

Method: 作者使用更新的术语和更通用的过滤谓词重新表述原始静态过滤方法，并将其扩展到回答集编程（ASP）；同时提出复杂度更低的可处理近似算法以应对通用方法的高计算复杂度。

Result: 扩展后的静态过滤方法在理论上更通用但复杂度更高（一般情况下为双指数，有界元数下为单指数）；所提出的近似算法在实际应用中（如处理真实世界数据的规则系统）可带来数量级级别的性能提升。

Conclusion: 静态过滤是一种被低估的优化技术，通过现代化表述和近似策略，可在保持实用性的同时显著增强Datalog和ASP系统的效率。

Abstract: Static filtering is a data-independent optimisation method for Datalog, which generalises algebraic query rewriting techniques from relational databases. In spite of its early discovery by Kifer and Lozinskii in 1986, the method has been overlooked in recent research and system development, and special cases are being rediscovered independently. We therefore recall the original approach, using updated terminology and more general filter predicates that capture features of modern systems, and we show how to extend its applicability to answer set programming (ASP). The outcome is strictly more general but also more complex than the classical approach: double exponential in general and single exponential even for predicates of bounded arity. As a solution, we propose tractable approximations of the algorithm that can still yield much improved logic programs in typical cases, e.g., it can improve the performance of rule systems over real-world data in the order of magnitude.

</details>
