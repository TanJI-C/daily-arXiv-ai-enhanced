<div id=toc></div>

# Table of Contents

- [Ziniu Wu](#Ziniu Wu) [Total: 1]
- [cs.DB](#cs.DB) [Total: 4]
- ["query optimization"](#"query optimization") [Total: 5]
- [learned "cost model"](#learned "cost model") [Total: 10]


<div id='Ziniu Wu'></div>

# Ziniu Wu [[Back]](#toc)

### [1] [Structural Indexing of Relational Databases for the Evaluation of Free-Connex Acyclic Conjunctive Queries](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.04757&hl=zh-CN&sa=X&d=79746272002302705&ei=-KJkaa-WKtaOieoP3p6U4Q8&scisig=AHkA5jR55R6iNFq2lI-R1i4Fdysv&oi=scholaralrt&hist=Pxo5FIAAAAAJ:11734074576040036222:AHkA5jT4qH64J8nZejvQlyvZo1xT&html=&pos=0&folt=rel)
*C Riveros,B Scheidt,N Schweikardt*

Main category: Ziniu Wu

TL;DR: 本文提出了一种基于数据库内部结构对称性的新型索引结构，用于高效评估自由连通无环合取查询（fc-ACQs），可在与数据库大小线性相关的预处理时间内实现常数延迟的枚举或计数。


<details>
  <summary>Details</summary>
Motivation: 现有基于值或顺序的索引方法（如B+树）无法有效利用数据库中元组间的结构对称性，限制了对fc-ACQs的高效评估。作者旨在通过挖掘并利用这种内部结构对称性，构建更紧凑、高效的索引以提升查询性能。

Method: 提出一种新索引结构，其核心是为给定数据库构造一个辅助数据库D'。该辅助数据库的大小由Scheidt和Schweikardt提出的“关系着色细化”（relational color refinement）算法所分配的颜色数决定，反映了原始数据库中元组的结构对称性。预处理阶段在O(|D|)时间内完成，之后可对任意fc-ACQ进行常数延迟的答案枚举或计数。

Result: 对于任意fc-ACQ，该方法可在O(|D|)预处理时间后实现常数延迟的枚举或计数。辅助数据库D'的大小|D'| ≤ |D|，且在许多结构化数据（如二叉树、正则图）上远小于|D|（甚至为常数）。即使在最坏情况下（无结构对称性），|D'|仍为O(|D|)。

Conclusion: 该工作首次建立了利用数据库内部结构对称性进行索引的理论基础，所提出的索引结构能以优于或等于传统线性方法的性能评估所有fc-ACQs，尤其在具有高度对称性的数据上优势显著。

Abstract: We present an index structure to boost the evaluation of free-connex acyclic conjunctive queries (fc-ACQs) over relational databases. The main ingredient of the index associated with a given database is an auxiliary database . Our main result states that for any fc-ACQ over , we can count the number of answers of or enumerate them with constant delay after a preprocessing phase that takes time linear in the size of . Unlike previous indexing methods based on values or order (e.g., B+ trees), our index is based on structural symmetries among tuples in a database, and the size of is related to the number of colors assigned to by Scheidt and Schweikardt's "relational color refinement" (2025). In the particular case of graphs, this coincides with the minimal size of an equitable partition of the graph. For example, the size of is logarithmic in the case of binary trees and constant for regular graphs. Even in the worst-case that has no structural symmetries among tuples at all, the size of is still linear in the size of . Given that the size of is bounded by the size of and can be much smaller (even constant for some families of databases), our index is the first foundational result on indexing internal structural symmetries of a database to evaluate all fc-ACQs with performance potentially strictly smaller than the database size.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [2] [Parallel Dynamic Spatial Indexes](https://arxiv.org/abs/2601.05347)
*Ziyang Men,Bo Huang,Yan Gu,Yihan Sun*

Main category: cs.DB

TL;DR: 本文系统研究了并行空间索引，提出了两种支持高效批量更新的新结构：P-Orth tree（并行Orth-tree）和SPaC-tree（并行R-tree/BVH），在保持良好查询性能的同时显著优于现有方法的更新性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用中的空间数据集高度动态，需低延迟地进行批量更新，但现有并行空间索引在此方面研究匮乏。

Method: 针对低延迟更新优化的两类空间索引（Orth-tree 和 R-tree/BVH），分别设计了并行版本 P-Orth tree 与 SPaC-tree，并对其批量更新与查询性能进行了系统评估。

Result: 所提出的 P-Orth tree 和 SPaC-tree 在批量更新性能上显著优于现有的并行 kd-tree 和 Orth-tree，同时在查询性能上保持相当或更优。

Conclusion: P-Orth tree 和 SPaC-tree 是面向高动态负载下高效并行更新的有效空间索引结构，为实际应用提供了高性能解决方案。

Abstract: Maintaining spatial data (points in two or three dimensions) is crucial and has a wide range of applications, such as graphics, GIS, and robotics. To handle spatial data, many data structures, called spatial indexes, have been proposed, e.g. kd-trees, oct/quadtrees (also called Orth-trees), R-trees, and bounding volume hierarchies (BVHs). In real-world applications, spatial datasets tend to be highly dynamic, requiring batch updates of points with low latency. This calls for efficient parallel batch updates on spatial indexes. Unfortunately, there is very little work that achieves this.
  In this paper, we systematically study parallel spatial indexes, with a special focus on achieving high-performance update performance for highly dynamic workloads. We select two types of spatial indexes that are considered optimized for low-latency updates: Orth-tree and R-tree/BVH. We propose two data structures: the P-Orth tree, a parallel Orth-tree, and the SPaC-tree family, a parallel R-tree/BVH. Both the P-Orth tree and the SPaC-tree deliver superior performance in batch updates compared to existing parallel kd-trees and Orth-trees, while preserving better or competitive query performance relative to their corresponding Orth-tree and R-tree counterparts. We also present comprehensive experiments comparing the performance of various parallel spatial indexes and share our findings at the end of the paper.

</details>


### [3] [RISE: Rule-Driven SQL Dialect Translation via Query Reduction](https://arxiv.org/abs/2601.05579)
*Xudong Xie,Yuwei Zhang,Wensheng Dou,Yu Gao,Ziyu Cui,Jiansen Song,Rui Yang,Jun Wei*

Main category: cs.DB

TL;DR: 提出RISE方法，通过简化复杂SQL查询并提取方言翻译规则，显著提升基于大语言模型的SQL方言翻译准确率。


<details>
  <summary>Details</summary>
Motivation: 传统SQL方言翻译依赖人工规则，成本高；现有大语言模型在处理复杂长SQL查询时表现不佳。

Method: 首先对复杂源查询进行方言感知的简化，去除与目标方言无关的元素，得到简化查询；然后利用大语言模型翻译简化查询，并从中自动提取方言翻译规则；最后将该规则应用于原始复杂查询，完成翻译。

Result: 在TPC-DS和SQLProcBench两个真实基准上，RISE分别达到97.98%和100%的翻译准确率，相较基线方法平均提升24.62%和238.41%。

Conclusion: RISE能有效解决复杂SQL方言翻译难题，显著优于传统规则方法和直接使用大语言模型的方法，为数据库迁移提供了高效自动化方案。

Abstract: Translating SQL dialects across different relational database management systems (RDBMSs) is crucial for migrating RDBMS-based applications to the cloud. Traditional SQL dialect translation tools rely on manually-crafted rules, necessitating significant manual effort to support new RDBMSs and dialects. Although large language models (LLMs) can assist in translating SQL dialects, they often struggle with lengthy and complex SQL queries.
  In this paper, we propose RISE, a novel LLM-based SQL dialect translation approach that can accurately handle lengthy and complex SQL queries. Given a complex source query $Q_c$ that contains a SQL dialect $d$, we first employ a dialect-aware query reduction technique to derive a simplified query $Q_{s}$ by removing $d$-irrelevant SQL elements from $Q_c$. Subsequently, we utilize LLMs to translate $Q_{s}$ into $Q_{s^{'}}$, and automatically extract the translation rule $r_d$ for dialect $d$ based on the relationship between $Q_{s}$ and $Q_{s^{'}}$. By applying $r_d$ to $Q_c$, we can effectively translate the dialect $d$ within $Q_c$, thereby bypassing the complexity of the source query $Q_c$. We evaluate RISE on two real-world benchmarks, i.e., TPC-DS and SQLProcBench, comparing its performance against both the traditional rule-based tools and the LLM-based approaches with respect to translation accuracy. RISE achieves accuracies of 97.98% on TPC-DS and 100% on SQLProcBench, outperforming the baselines by an average improvement of 24.62% and 238.41%, respectively.

</details>


### [4] [The Importance of Parameters in Ranking Functions](https://arxiv.org/abs/2601.06001)
*Christoph Standke,Nikolaos Tziavelis,Wolfgang Gatterbauer,Benny Kimelfeld*

Main category: cs.DB

TL;DR: 该论文研究了在排序函数中列权重对元组排名影响的可解释性问题，采用Grohe等人提出的SHAP分数框架，系统分析了不同排序函数（如字典序、基于求和/最小值/最大值的排序）与效应函数（全局、top-k、局部）组合下的计算复杂性，并刻画了精确计算的多项式可解性与#P-难性边界。


<details>
  <summary>Details</summary>
Motivation: 为了解释排序结果中某一列权重的重要性，作者旨在形式化并计算该权重的SHAP分数，从而提供对排序机制的可解释性分析。这一问题在数据库查询解释、公平性评估等场景中具有实际意义。

Method: 作者基于Grohe等人[ICDT'24]提出的SHAP分数框架，将问题建模为依赖三个要素（排序函数、效应函数、权重分布）的计算任务。他们针对多种基础排序函数（字典序、sum/min/max-based score orders）和效应函数（全局、top-k、局部），在列间权重独立的有限概率分布假设下，分析其SHAP分数的精确计算复杂性，并探讨近似算法（FPRAS）的存在性。

Result: 所有设定下均存在加性完全多项式时间随机近似方案（FPRAS）；但精确计算的复杂性因函数组合而异：部分情形可在多项式时间内求解，其余则被证明为#P-难。此外，这些复杂性结论可推广至计算整列（而非仅其权重）Shapley值的问题。

Conclusion: 该工作系统刻画了基于SHAP的列权重重要性度量在典型排序与效应函数下的计算边界，揭示了精确计算的内在难度，并表明近似方法是可行的通用策略。同时，其复杂性结果对更广泛的列级Shapley值计算具有指导意义。

Abstract: How important is the weight of a given column in determining the ranking of tuples in a table? To address such an explanation question about a ranking function, we investigate the computation of SHAP scores for column weights, adopting a recent framework by Grohe et al.[ICDT'24]. The exact definition of this score depends on three key components: (1) the ranking function in use, (2) an effect function that quantifies the impact of using alternative weights on the ranking, and (3) an underlying weight distribution. We analyze the computational complexity of different instantiations of this framework for a range of fundamental ranking and effect functions, focusing on probabilistically independent finite distributions for individual columns.
  For the ranking functions, we examine lexicographic orders and score-based orders defined by the summation, minimum, and maximum functions. For the effect functions, we consider global, top-k, and local perspectives: global measures quantify the divergence between the perturbed and original rankings, top-k measures inspect the change in the set of top-k answers, and local measures capture the impact on an individual tuple of interest. Although all cases admit an additive fully polynomial-time randomized approximation scheme (FPRAS), we establish the complexity of exact computation, identifying which cases are solvable in polynomial time and which are #P-hard. We further show that all complexity results, lower bounds and upper bounds, extend to a related task of computing the Shapley value of whole columns (regardless of their weight).

</details>


### [5] [Database Theory in Action: Direct Access to Query Answers](https://arxiv.org/abs/2601.06013)
*Jiayin Hu,Nikolaos Tziavelis*

Main category: cs.DB

TL;DR: 该论文实现了支持多种查询和排序的直接访问数据结构，并通过实验分析了其实际性能，包括与数据库系统的性能对比以及直接访问与其单次访问变体之间的关系。


<details>
  <summary>Details</summary>
Motivation: 尽管支持按排名位置检索查询结果（即直接访问）的数据结构在时间复杂度方面已有深入研究，且对许多查询和常见排序已知高效算法，但其实际性能尚未得到充分关注。

Method: 作者实现了一个涵盖广泛查询类型和排序方式的直接访问系统，用于实证研究其在真实场景下的性能表现。

Result: 实验揭示了若干有趣的实践现象，包括不同数据库系统在直接访问任务中的性能差异，以及直接访问与其单次访问版本之间的性能关联。

Conclusion: 该工作填补了直接访问算法理论研究与实际应用之间的空白，为未来优化和部署此类数据结构提供了实证依据。

Abstract: Direct access asks for the retrieval of query answers by their ranked position, given a query and a desired order. While the time complexity of data structures supporting such accesses has been studied in depth, and efficient algorithms for many queries and common orders are known, their practical performance has received little attention. We provide an implementation covering a wide range of queries and orders; it allows us to investigate intriguing practical aspects, including the comparative performance of database systems and the relationship between direct access and its single-access counterpart.

</details>


<div id='"query optimization"'></div>

# "query optimization" [[Back]](#toc)

### [6] [Feedback-Driven Query Optimization: Design and Infrastructure](https://scholar.google.com/scholar_url?url=http://reports-archive.adm.cs.cmu.edu/anon/anon/2025/CMU-CS-25-150.pdf&hl=zh-CN&sa=X&d=1054692481536127846&ei=WnFkafrLGbK16rQPn86DwQ8&scisig=AHkA5jRD_qNeSQlxC0kHhQ3wwEbI&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=0&folt=kw-top)
*Y Liang*

Main category: "query optimization"

TL;DR: 该论文研究了一种利用查询反馈的自适应查询优化策略，通过缓存优化状态和细粒度运行时选择性信息，提升未来查询的计划质量，并在 PostgreSQL 中验证了其开销可控。


<details>
  <summary>Details</summary>
Motivation: 现代数据栈常在 DBMS 之外生成缺乏统计摘要的数据文件（如 Parquet），导致查询优化器因缺乏数据分布信息而依赖不准确的猜测进行基数和代价估计，从而难以选出最优执行计划。

Method: 1）构建基于 Cascades 框架的转换式搜索引擎，引入 memo 表以缓存等价子计划并检测重复，支持跨查询的优化状态复用；2）提出在执行时测量合取谓词各前缀真实选择性的方法，使优化器能更广泛地利用运行时反馈；3）在 PostgreSQL 中实现并评估该方法的性能开销。

Result: 实验表明：1）具有相似子计划模式的后续查询可有效复用先前缓存的优化信息；2）细粒度的选择性收集机制带来恒定的运行时开销，且不随合取项数量增加而增长；3）运行时反馈虽能提升计划准确性，但需权衡覆盖范围与收集开销。

Conclusion: 通过结合优化状态缓存与细粒度运行时反馈，可构建有效的自适应查询优化机制；将此类优化器作为可扩展服务集成到现有 DBMS 中，需仔细权衡设计选择与工程实现之间的折衷。

Abstract: Query optimizers are critical components in database management systems (DBMSs) that turn a query that might otherwise take hours to run into one that completes in seconds. However, modern data stacks allow applications to generate data files (eg, Parquet) outside the DBMS’s purview that lack statistical summaries. Without information about data distribution, optimizers fall back to ungrounded guesses when computing cardinality and cost estimates needed to select the best plan from an exponential number of candidates. This thesis investigates the design of an adaptive query optimization strategy that leverages query feedback to improve planning for future queries. We first study how an optimizer should memoize query plans during optimization so that it can associate past traces with future queries. To facilitate this investigation, we built a Cascades-style transformational search engine with a memo table that organizes equivalent sub-plans and detects duplicates. We measured the amount of reuse when maintaining the optimization state across queries. Our results showed that future queries can reuse the cached optimization information from previous queries when they exhibit similar subplan patterns. We then analyze runtime feedback collection methods, focusing on integration strategies and the resulting artifacts. Our study reveals a tension between the query coverage provided by runtime artifacts and the collection overhead they entail. Although most DBMSs can provide runtime row-count profiles at the operator level, the optimizer can only use this information for future queries that match the exact predicates. To address this limitation, we present a method that measures the true selectivity of each prefix in the conjunctive predicate during execution, which the optimizer can then leverage to improve planning for a broader range of queries. We implemented this method in PostgreSQL and measured the runtime performance overhead. Our experiments show that fine-grained instrumentation incurs a constant overhead regardless of the number of conjuncts. We conclude the thesis by discussing the design choices and trade-offs of integrating an extensible query optimizer service with existing DBMSs.

</details>


### [7] [Surprising Interactions Between Filters In Equi-Join Processing](https://scholar.google.com/scholar_url?url=http://reports-archive.adm.cs.cmu.edu/anon/anon/2025/CMU-CS-25-152.pdf&hl=zh-CN&sa=X&d=7000732317522279578&ei=WnFkafrLGbK16rQPn86DwQ8&scisig=AHkA5jRyrxrNSLlc9rdYXMtgMUMx&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=2&folt=kw-top)
*M Khare*

Main category: "query optimization"

TL;DR: 该论文研究了在现代数据湖和存储计算分离架构下，传统查询优化器因统计信息不准确而失效的问题，重点评估了基于min-max过滤器和Bloom过滤器的连接预过滤（join pre-filtering）技术，并指出当前方法尚不足以广泛适用于各类分析型负载。


<details>
  <summary>Details</summary>
Motivation: 在现代DBMS中，数据常存储于对象存储或开放文件格式中，缺乏准确的统计信息，导致传统基于代价的连接优化策略失效；因此需要探索减少对优化器依赖的新方法，如连接预过滤。

Method: 论文综述了连接预过滤的基本概念与现有技术的设计差异，并通过修改后的Dynamic Predicate Transfer（RPT+）实现，对min-max过滤器和Bloom过滤器两种预过滤方法进行了性能评估。

Result: 实验分析揭示了不同过滤器类型之间的性能交互特性，表明当前预过滤策略在通用性和适应性方面仍存在不足。

Conclusion: 现有连接预过滤技术尚未形成适用于广泛工作负载的通用策略，未来需进一步研究以提升其在现代DBMS中的实用性与鲁棒性。

Abstract: The current era of data storage is defined by the widespread adoption of data lakes, and the disaggregation of storage and compute hardware. Modern database management systems (DBMSs) are often operating on large volumes of data stored in object stores (like Amazon’s S3), open file formats (like Apache’s Parquet), or otherwise have outdated or nonexistent statistics. In join-heavy analytical workloads, the traditional approach of optimizing query plans to minimize the cost of joins breaks down if the available information to estimate cardinalities and costs is inaccurate. In recent years, a class of techniques known as “join pre-filtering” has gained focus as an attempt to reduce the reliance on a good optimizer for minimizing join costs by reducing the inputs of joins to the minimum set of tuples needed to produce the output. This thesis explores the current state-of-the-art in pre-filtering, and concludes that more work must be done to create a strategy that is applicable across a wide range of workloads. First, we provide an overview of the fundamental concepts of pre-filtering, and describe the key design decisions of and differences between currently studied techniques. We then evaluate two pre-filtering methods, min-max filters and Bloom filters, using a modified implementation of Dynamic Predicate Transfer (RPT+), a leading contemporary technique for join pre-filtering. Our analysis focuses on the performance interactions between these filter types. Finally, we discuss the implications of the results for modern DBMSs, and future directions of study in this area.

</details>


### [8] [Machine Learning-Enhanced Database Cache Management: A Comprehensive Performance Analysis and Comparison of Predictive Replacement Policies](https://scholar.google.com/scholar_url?url=https://www.mdpi.com/2076-3417/16/2/666&hl=zh-CN&sa=X&d=3500587795608487708&ei=WnFkafrLGbK16rQPn86DwQ8&scisig=AHkA5jQoK1Dayd-6XT-D2rx-kK-M&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=3&folt=kw-top)
*M Abbasi,P Váz,J Silva,F Cardoso,F Sá,P Martins*

Main category: "query optimization"

TL;DR: 该论文提出了一种模块化的、基于机器学习的缓存管理框架，通过集成多种机器学习模型（如随机森林、LSTM、SVM 和梯度提升）来优化数据库缓存替换策略，在多个标准基准和真实应用场景中显著提升了缓存命中率、查询延迟和系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统缓存替换策略（如 LRU 和 LFU）依赖简单启发式规则，无法有效捕捉现代数据密集型应用中复杂的访问时序与频率模式，导致数据库性能瓶颈。因此，亟需一种能智能识别访问模式并动态优化缓存决策的新方法。

Method: 作者设计了一个模块化机器学习增强型缓存管理框架，集成了随机森林、LSTM、SVM 和梯度提升等模型，并构建了从查询访问模式中提取时序、频率和上下文特征的特征工程流水线，可无缝嵌入现有数据库系统。

Result: 在 TPC-C、TPC-H、YCSB 和 LinkBench 等基准及真实生产数据集上的实验表明，该方法相比传统和先进自适应策略（如 ARC、LIRS、Clock-Pro、TinyLFU、LECAR），缓存命中率提升 8.4%–19.2%，查询延迟降低 15.3%–28.7%，系统吞吐量提高 18.9%–31.4%；其中随机森林以仅 3.1% 的计算开销实现 18.7% 的性能提升。在电商、金融和内容管理场景中，还带来 8.3% 的转化率提升和年增 12.7 万美元收入。

Conclusion: 将机器学习引入数据库缓存管理可显著提升性能与业务指标，且模块化设计确保了实用性和可部署性；随机森林因其高性价比成为最实用的选择，验证了数据驱动缓存策略在现代数据库系统中的巨大潜力。

Abstract: The exponential growth of data-driven applications has intensified performance demands on database systems, where cache management represents a critical bottleneck. Traditional cache replacement policies such as Least Recently Used (LRU) and Least Frequently Used (LFU) rely on simple heuristics that fail to capture complex temporal and frequency patterns in modern workloads. This research presents a modular machine learning-enhanced cache management framework that leverages pattern recognition to optimize database performance through intelligent replacement decisions. Our approach integrates multiple machine learning models—Random Forest classifiers, Long Short-Term Memory (LSTM) networks, Support Vector Machines (SVM), and Gradient Boosting methods—within a modular architecture enabling seamless integration with existing database systems. The framework incorporates sophisticated feature engineering pipelines extracting temporal, frequency, and contextual characteristics from query access patterns. Comprehensive experimental evaluation across synthetic workloads, real-world production datasets, and standard benchmarks (TPC-C, TPC-H, YCSB, and LinkBench) demonstrates consistent performance improvements. Machine learning-enhanced approaches achieve 8.4% to 19.2% improvement in cache hit rates, 15.3% to 28.7% reduction in query latency, and 18.9% to 31.4% increase in system throughput compared to traditional policies and advanced adaptive methods including ARC, LIRS, Clock-Pro, TinyLFU, and LECAR. Random Forest emerges as the most practical solution, providing 18.7% performance improvement with only 3.1% computational overhead. Case study analysis across e-commerce, financial services, and content management applications demonstrates measurable business impact, including 8.3% conversion rate improvements and USD 127,000 annual revenue increases. Statistical validation (p<0.001, Cohen’s d>0.8) confirms both statistical and practical significance.

</details>


### [9] [A Frugal Hybrid Architecture for Local AI Marrying Tiny Recursive Models and External Memory](https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Jean-Michel-Cornu/publication/399576127_A_Frugal_Hybrid_Architecture_for_Local_AI_Marrying_Tiny_Recursive_Models_and_External_Memory/links/695fc76f54906834b6889761/A-Frugal-Hybrid-Architecture-for-Local-AI-Marrying-Tiny-Recursive-Models-and-External-Memory.pdf&hl=zh-CN&sa=X&d=10265963048937985134&ei=WnFkafrLGbK16rQPn86DwQ8&scisig=AHkA5jRhxt_mGoCQ_wveV_h7CI_o&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=4&folt=kw-top)
*JM Cornu*

Main category: "query optimization"

TL;DR: 本文提出一种混合架构，将参数规模约10⁸–10⁹的扩展型Tiny Recursive Model（TRM）与一个约7B参数的小型大语言模型（LLM）结合，后者作为高容量语义记忆模块。目标是在消费级硬件上以较低成本和延迟，在通用文本查询任务中逼近更大规模LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管小型LLM可在消费级硬件上运行，但其参数受限导致处理能力下降且易产生幻觉；而TRM虽在抽象推理任务中表现优异但难以覆盖开放域自然语言任务。因此，作者旨在探索TRM与小型LLM协同是否能在保持低资源消耗的同时提升通用语言任务性能。

Method: 构建TRM–LLM混合架构：TRM负责推理决策，小型LLM作为外部语义记忆；设计外部记忆类型（如RAG或冻结LLM）、记忆准备方法、TRM访问记忆的决策策略、查询构造方式以及检索与重排序机制。

Result: 论文未报告具体实验结果，但提出了完整的TRM–LLM协同架构设计，包括外部记忆的分类、查询生成、检索与重排序流程，为后续实证研究奠定基础。

Conclusion: TRM与小型LLM结合的架构有望在有限计算资源下兼顾抽象推理能力和开放域语言理解，为高效、低成本的语言模型部署提供新思路。

Abstract: State-of-the-art Large Language Models (LLMs) require substantial computational resources. Nevertheless, a compact LLM can run on a standard consumer hardware. With a more constrained parameter budget (on the order of about 7B), such a model can still leverage very large corpora that encode a substantial fraction of explicit textual knowledge, at the cost of reduced processing capacity and a higher risk of hallucinations. Recent work has shown that another class of neural networks, Tiny Recursive Models (TRMs), can achieve, on certain abstract reasoning benchmarks, reasoning performance close to that of very large LLMs while using only a few million parameters. However, these models remain insufficient to cover very broad natural language domains (for example, open-domain text interactions) and are instead better suited to bounded domains (structured tasks, puzzle solving, etc.).
We describe an architecture in which an enlarged TRM (approx108text–109 parameters) is coupled with a compact LLM (about 7B) acting as a high-capacity semantic memory. Our goal is to explore whether, on common textual queries, such a TRM–LLM architecture can approximate some of the performance of larger LLMs while keeping cost and latency compatible with consumer hardware. This paper presents a typology of external memories (RAG and/or frozen LLM) and their preparation, the TRM’s decision policy for accessing them, the formulation of queries, and the retrieval/reranking of passages.

</details>


### [10] [Tietokantajärjestelmien optimointitekniikoiden testaus ja vertailu](https://scholar.google.com/scholar_url?url=https://lutpub.lut.fi/bitstream/handle/10024/171143/Kandidaatintyo_Wahlsten_Jeremias.pdf%3Fsequence%3D1%26isAllowed%3Dy&hl=zh-CN&sa=X&d=13647691387558786409&ei=WnFkafrLGbK16rQPn86DwQ8&scisig=AHkA5jRfhPkab2Vyn3sn6xKOBkHK&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=5&folt=kw-top)
*J Wahlsten*

Main category: "query optimization"

TL;DR: 本文通过实验评估了索引和反规范化两种优化技术对多种关系型与非关系型数据库查询性能的影响，结果表明合理使用这些技术可显著提升查询响应速度。


<details>
  <summary>Details</summary>
Motivation: 随着数据中心数量和数据量的激增，数据库系统在现代信息系统中的作用愈发关键。高效的数据存储、检索和处理对数字服务至关重要，而高性能要求也使得数据库优化成为提升能效和实现可持续数据处理的关键环节。然而，不同优化技术在各类数据库架构中的实际效果尚需系统性实证研究。

Method: 作者采用真实且大规模的芬兰Kela学生补助数据集，在多个关系型与非关系型数据库系统中执行结构一致的查询。实验分为三组：无优化、仅使用索引、以及同时使用索引与反规范化结构。通过多次运行测量查询响应时间，系统评估两种优化技术对查询性能的影响。

Result: 实验结果显示，优化技术对数据库查询性能具有显著影响。索引通常能大幅提升过滤和限制类查询的速度，但不当的索引选择反而可能降低性能。当索引与反规范化结合使用时，多数情况下查询响应时间缩短至原始时间的一小部分。

Conclusion: 索引和反规范化是提升数据库查询性能的有效手段，尤其在组合使用时效果更为显著。然而，优化策略需根据具体查询模式和数据库架构谨慎设计，以避免性能劣化。该研究为不同数据库环境下的性能优化提供了实证依据。

Abstract: Datakeskusten määrän kasvu sekä tiedon jatkuva lisääntyminen ovat nostaneet tietokantojen ja tietokantajärjestelmien merkityksen keskeiseksi osaksi nykyaikaisia tietojärjestelmiä. Lähes kaikki digitaaliset palvelut perustuvat tehokkaaseen tiedon tallennukseen, hakuun ja käsittelyyn, ja näille toiminnoille asetetut suorituskykyvaatimukset ovat erittäin korkeita. Tämän vuoksi tietokantojen optimointi on keskeisessä roolissa myös energiatehokkuuden ja kestävän tietojenkäsittelyn kannalta. Tietokantajärjestelmien optimointiin on olemassa useita erilaisia tekniikoita, kuten indeksointi, denormalisointi, kyselyoptimointi, välimuistin käyttö ja datan lohkominen. Tämä kandidaatintyö pyrkii osaltaan paikkaamaan tätä tutkimusaukkoa tarkastelemalla optimointitekniikoiden vaikutusta tiedonhakukyselyiden suorituskykyyn systemaattisen kokeellisen testauksen avulla. Työssä testattiin kahden yleisesti käytetyn optimointitekniikan, indeksoinnin ja denormalisoinnin, vaikutuksia useissa eri tietokantajärjestelmissä. Mukana oli sekä relaatio- että ei-relaatiotietokantoja, jotta optimointien vaikutuksia voitiin arvioida erilaisissa tietokanta-arkkitehtuureissa. Empiirisessä osuudessa käytettiin todellista ja laajaa aineistoa, joka perustui Kelan julkaisemiin opintotukitietoihin. Sama data ja rakenteeltaan vastaavat kyselyt ajettiin jokaisessa tietokantajärjestelmässä ilman optimointeja, indeksien kanssa sekä indeksien ja denormalisoitujen tietorakenteiden yhdistelmällä. Suorituskykyä mitattiin kyselyiden vasteaikojen avulla useiden toistojen perusteella. Tutkimuksen tulokset osoittavat, että optimointitekniikoilla on erittäin merkittävä vaikutus tietokantakyselyiden suorituskykyyn. Indeksointi paransi useimmissa tapauksissa erityisesti suodattavien ja rajattujen kyselyiden nopeutta, vaikka väärin valitut indeksit saattoivat joissakin tilanteissa heikentää suorituskykyä. Kun indeksointi ja denormalisointi yhdistettiin, kyselyiden vasteajat lyhenivät monissa tapauksissa murto-osaan alkuperäisestä.

</details>


<div id='learned "cost model"'></div>

# learned "cost model" [[Back]](#toc)

### [11] [Integrating Machine Learning with Parametric Models for Rapid Cost Trades in Early-Stage Missile Design](https://scholar.google.com/scholar_url?url=https://arc.aiaa.org/doi/abs/10.2514/6.2026-1120&hl=zh-CN&sa=X&d=12723657998052281529&ei=WnFkabHlHrOlieoPyq27-AE&scisig=AHkA5jRpY_220ykxGBUD-vnGgUlb&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=0&folt=kw-top)
*L Wattebled,A Jazzini,A Cox,D Mavris*

Main category: learned "cost model"

TL;DR: 本文提出MCDAT（导弹成本数据驱动分析工具），一种混合成本建模平台，将经济因素整合到导弹早期设计阶段，通过机器学习、参数化公式和历史成本分析对关键子系统进行成本估算，并结合学习曲线、研发成本及30年维护成本预测，验证显示其对法国VT-1导弹成本预测误差仅为5.8%。


<details>
  <summary>Details</summary>
Motivation: 传统导弹设计方法在后期才使用基于总重的粗略估算评估成本，缺乏在设计早期考虑成本效益的能力，难以支持低成本导弹系统的开发。

Method: MCDAT将导弹分解为固体火箭发动机、惯性测量单元、结构等关键子系统，结合机器学习模型、经验参数公式和历史成本数据，估算单件生产与全生命周期成本；同时引入学习曲线分析、研发（RDT&E）估算及基于蒙特卡洛模拟的30年维护成本预测。

Result: 在对法国VT-1地空导弹的成本验证中，MCDAT的预测误差为5.8%，且子系统成本分配与战术导弹行业基准一致。

Conclusion: MCDAT将成本作为设计循环中的主动驱动因素而非事后约束，能够在概念阶段识别并优先选择经济可行的架构，指导设计空间探索，并在工程自由度最高时提供量化经济反馈。

Abstract: In classic missile design methodology the development of cost is assessed late using rough estimators based almost exclusively on the overall weight. This approach offers minimal leverage for shaping design decisions with consideration for cost-effectiveness, thereby restricting its uses in the development of low-cost missile systems. To compensate for this gap, this paper presents MCDAT (Missile Cost Data-Driven Analysis Tool), a hybrid cost model platform created to incorporate economic factors into early stage implementation considerations. MCDAT decomposes the missile into critical subsystems such as solid rocket motor, inertial measurement unit or structures and exploits machine learning models, empirical parametric formulas and historical cost analysis for estimating unit production and life-cycle costs. Besides the first-unit production cost, the framework includes learning curve analysis, RDT&E, research and development estimation, and 30 years of maintenance forecasting cost using Monte Carlo simulation. Validation against French VT-1 surface-to-air missile cost showed 5.8% prediction errors and the subsystem allocation was consistent with tactical missile industry benchmarks. By design with cost being implemented as an active driver in the design loop versus a post-hoc constraint, MCDAT enables identifying and prioritizing affordable architectures at concept time, directing design space exploration, providing quantitative economic feedback when the engineering freedom of action is high.

</details>


### [12] [Spacecraft Component and Configuration Design with Reinforcement Learning Trained Transformers](https://scholar.google.com/scholar_url?url=https://arc.aiaa.org/doi/abs/10.2514/6.2026-0908&hl=zh-CN&sa=X&d=15266716691371453094&ei=WnFkabHlHrOlieoPyq27-AE&scisig=AHkA5jSWDDp20i6Z7CluZaAO3U_4&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=1&folt=kw-top)
*A Demagall,D Selva*

Main category: learned "cost model"

TL;DR: 该论文提出使用基于强化学习训练的Transformer模型，同时优化航天器组件设计与构型设计，以更全面地探索设计空间；实验表明该方法虽能生成有效设计方案，但在目标优化方面不如遗传算法。


<details>
  <summary>Details</summary>
Motivation: 航天器设计是一个复杂的多领域问题，传统上依赖工程师经验，限制了设计空间的充分探索；同时，现有方法通常将组件设计与构型设计分开处理，未能充分利用二者之间的耦合关系。

Method: 采用强化学习训练的Transformer模型，联合处理航天器组件设计与构型设计，并与遗传算法和随机搜索在相同设计空间中进行对比。

Result: 强化学习训练的Transformer能够学习生成有效的航天器设计方案，但在有效设计空间内对性能目标的优化能力弱于遗传算法；这与仅考虑构型设计时的结果形成对比。

Conclusion: 尽管所提方法在生成可行性设计方面有效，但其优化能力有限，需进一步研究该方法在联合设计任务中的潜力与局限性。

Abstract: Spacecraft design, including both component design and configuration design, is a complex, multi-domain problem with a large design space. Thorough exploration of the design space is important for minimizing cost while achieving performance objectives, as improvements in the early stage of design can have large downstream effects. This process relies heavily on the experience and intuition of engineers which limits the exploration of the design space. Given this, a method which will help teams of engineers explore the design space of spacecraft designs more thoroughly is desirable. For this problem both spacecraft component design and spacecraft configuration design are considered simultaneously instead of separately as they are typically treated both in the literature and in practice. To solve the problem, a transformer is trained using reinforcement learning. This model is then compared to a genetic algorithm and random search exploring the joint spacecraft component and configuration design space. Results indicate that while the reinforcement learning-trained transformer successfully learns to generate valid designs, it struggles to optimize on the objectives within the space of valid designs when compared to a genetic algorithm. This is in contrast to results found in the configuration-only problem, and further exploration into the capabilities and limitations of the method is necessary.

</details>


### [13] [FEDERATED LEARNING-AWARE MULTI-OBJECTIVE SCHEDULING FOR DISTRIBUTED EDGE-CLOUD ENVIRONMENTS](https://scholar.google.com/scholar_url?url=http://www.upubscience.com/upload/20251229113340.pdf&hl=zh-CN&sa=X&d=5872920987359840254&ei=WnFkabHlHrOlieoPyq27-AE&scisig=AHkA5jTzBMd_u89Ha4TSLzQoOuGZ&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=2&folt=kw-top)
*A Malhotra,F Baumann*

Main category: learned "cost model"

TL;DR: 本文提出了一种面向联邦学习（Federated Learning）的多目标调度策略，通过自适应遗传算法动态优化边缘-云协同环境中的资源分配，在保障联邦学习协议完整性的同时，显著降低系统成本并加速模型收敛。


<details>
  <summary>Details</summary>
Motivation: 随着物联网应用和延迟敏感型计算任务的增长，传统资源调度方法难以满足联邦学习在分布式边缘-云架构中的特殊需求，如模型收敛时间、通信开销和数据异构性，亟需一种兼顾多目标优化与联邦学习特性的调度机制。

Method: 设计了一种分层调度架构，协调移动边缘计算服务器、基站与云数据中心之间的任务调度；采用具有动态交叉与变异概率的自适应多目标优化算法，根据联邦学习训练进度和系统状态动态调整资源分配。

Result: 实验表明，所提方法相比固定参数方法可降低约28%的系统总成本，并提升35%的模型收敛速度，有效平衡了计算卸载决策与联邦学习通信模式。

Conclusion: 该工作为将联邦学习工作流集成到生产级分布式计算基础设施中奠定了基础，成功应对了隐私保护机器学习范式带来的独特调度挑战。

Abstract: The proliferation of Internet of Things applications and latency-sensitive computing tasks has necessitated novel approaches to resource management across distributed edge-cloud architectures. Federated Learning has emerged as a compelling paradigm for collaborative model training without centralizing data, yet integrating Federated Learning into multi-objective scheduling frameworks presents significant challenges. This paper proposes a comprehensive scheduling strategy that accounts for Federated Learning-specific requirements including model convergence time, communication overhead, and data heterogeneity while optimizing multiple conflicting objectives such as makespan, energy consumption, and resource utilization. We develop a hierarchical scheduling architecture that coordinates tasks between mobile edge computing servers, base stations, and cloud data centers while maintaining Federated Learning protocol integrity. The proposed approach employs an adaptive multi-objective optimization algorithm with dynamic crossover and mutation probabilities that adjusts resource allocation based on Federated Learning training progress and system state. Experimental evaluation demonstrates that our Federated Learning-aware scheduling strategy with adaptive genetic algorithm parameters achieves superior performance compared to fixed-parameter approaches, reducing total system cost by approximately 28 percent while improving convergence speed by 35 percent. The framework effectively balances computation offloading decisions with Federated Learning communication patterns across the hierarchical mobile edge computing infrastructure, resulting in enhanced system efficiency for distributed edge-cloud environments. This work establishes foundations for integrating Federated Learning workflows into production-scale distributed computing infrastructures while addressing the unique challenges posed by privacy-preserving machine learning paradigms.

</details>


### [14] [Decision-Aware Trust Signal Alignment for SOC Alert Triage](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.04486&hl=zh-CN&sa=X&d=11728347132915661446&ei=WnFkabHlHrOlieoPyq27-AE&scisig=AHkA5jTV6fHP3i0ephLcG_JNeEso&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=3&folt=kw-top)
*IJ Chowdhury,MAY Tanvir*

Main category: learned "cost model"

TL;DR: 该论文提出了一种面向安全运营中心（SOC）告警分诊的决策敏感型信任信号对应框架，通过结合校准后的置信度、轻量级不确定性提示和代价敏感的决策阈值，在不修改检测模型的前提下提升分析师决策效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的SOC检测系统输出的概率或置信度分数通常未校准且难以在高压下解读，且未考虑安全决策中误报与漏报的非对称代价，导致告警质量差、告警过载，增加分析师负担。

Method: 构建一个独立于模型的决策支持层，整合后验校准的置信度、轻量级不确定性线索以及代价敏感的决策阈值；使用UNSW-NB15数据集上的逻辑回归和随机森林模型进行模拟评估，并计划开展人在回路实验以评估不同信任信号界面下的分析师决策表现。

Result: 模拟结果表明，置信度显示与决策目标不一致会显著放大漏报（false negatives），而采用与决策对齐的信任信号可使代价加权损失降低数个数量级。

Conclusion: 将校准置信度、不确定性提示与代价敏感机制融合为统一的决策支持框架，能有效缓解告警过载问题并显著提升安全分析中的决策质量，未来需通过人在回路实验进一步验证其人机协同效果。

Abstract: Detection systems that utilize machine learning are progressively implemented at Security Operations Centers (SOCs) to help an analyst to filter through high volumes of security alerts. Practically, such systems tend to reveal probabilistic results or confidence scores which are ill-calibrated and hard to read when under pressure. Qualitative and survey based studies of SOC practice done before reveal that poor alert quality and alert overload greatly augment the burden on the analyst, especially when tool outputs are not coherent with decision requirements, or signal noise. One of the most significant limitations is that model confidence is usually shown without expressing that there are asymmetric costs in decision making where false alarms are much less harmful than missed attacks. The present paper presents a decision-sensitive trust signal correspondence scheme of SOC alert triage. The framework combines confidence that has been calibrated, lightweight uncertainty cues, and cost-sensitive decision thresholds into coherent decision-support layer, instead of making changes to detection models. To enhance probabilistic consistency, the calibration is done using the known post-hoc methods and the uncertainty cues give conservative protection in situations where model certainty is low. To measure the model-independent performance of the suggested model, we apply the Logistic Regression and the Random Forest classifiers to the UNSW-NB15 intrusion detection benchmark. According to simulation findings, false negatives are greatly amplified by the presence of misaligned displays of confidence, whereas cost weighted loss decreases by orders of magnitude between models with decision aligned trust signals. Lastly, we describe a human-in-the-loop study plan that would allow empirically assessing the decision-making of the analysts with aligned and misaligned trust interfaces.

</details>


### [15] [Nightmare Dreamer: Dreaming About Unsafe States And Planning Ahead](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.04686&hl=zh-CN&sa=X&d=8204465855844892032&ei=WnFkabHlHrOlieoPyq27-AE&scisig=AHkA5jRkJn_LBLjh3eohqrSQyKtJ&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=6&folt=kw-top)
*O Oseni,S Wang,J Zhu,M Corah*

Main category: learned "cost model"

TL;DR: Nightmare Dreamer 是一种基于模型的安全强化学习算法，通过学习世界模型预测安全违规并在仅使用图像观测的情况下，在 Safety Gymnasium 任务中实现近乎零安全违规和近20倍的效率提升。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在机器人控制等现实应用中取得了显著成功，但其采用仍受限于缺乏足够的安全性保障。

Method: 提出 Nightmare Dreamer 算法，利用学习到的世界模型预测潜在的安全违规，并据此规划动作，从而在保证安全的同时最大化奖励。

Result: 在仅使用图像观测的 Safety Gymnasium 任务中，Nightmare Dreamer 相比无模型基线方法实现了近20倍的效率提升，并达到近乎零的安全违规。

Conclusion: Nightmare Dreamer 有效解决了强化学习中的安全性问题，在保持高奖励的同时显著减少安全违规，提升了算法的实用性和可靠性。

Abstract: Reinforcement Learning (RL) has shown remarkable success in real-world applications, particularly in robotics control. However, RL adoption remains limited due to insufficient safety guarantees. We introduce Nightmare Dreamer, a model-based Safe RL algorithm that addresses safety concerns by leveraging a learned world model to predict potential safety violations and plan actions accordingly. Nightmare Dreamer achieves nearly zero safety violations while maximizing rewards. Nightmare Dreamer outperforms model-free baselines on Safety Gymnasium tasks using only image observations, achieving nearly a 20x improvement in efficiency.

</details>


### [16] [The Evolution of Software Effort Estimation Models](https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Syed-Mubashir-Ali/publication/398037538_The_Evolution_of_Software_Effort_Estimation_Models/links/694f980a0c98040d4822f877/The-Evolution-of-Software-Effort-Estimation-Models.pdf&hl=zh-CN&sa=X&d=2776877621587102348&ei=WnFkabHlHrOlieoPyq27-AE&scisig=AHkA5jTPBj2pX8GP5-YTUJl52-y0&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=7&folt=kw-top)
*MA Latif,MK Khan,U Khan,S Akbar,SM Ali*

Main category: learned "cost model"

TL;DR: 本文探讨了COCOMO模型，并结合软计算与机器学习方法以提升软件成本估算的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统COCOMO模型依赖回归方程，在面对复杂和多变的软件项目时可能存在精度不足的问题，因此需要引入软计算和机器学习技术来增强其预测能力。

Method: 将统计学习理论与软计算、机器学习方法相结合，对COCOMO模型进行改进或优化，以提高软件开发成本估算的性能。

Result: 所提出的方法在实验中表现出优于传统COCOMO模型的成本估算精度和鲁棒性。

Conclusion: 融合软计算与机器学习技术能有效提升COCOMO模型在软件成本估算中的适用性和准确性，为软件工程管理提供更可靠的决策支持。

Abstract: … COCOMO (Constructive Cost Model): One of the strongest algorithmic cost estimating models with regression-based equations based on … computation, and the theory of statistical learning to use soft computing and machine learning …

</details>


### [17] [Deployment of a cloud-based passive defecation monitoring system for continuous gut health monitoring](https://scholar.google.com/scholar_url?url=https://www.nature.com/articles/s41596-025-01296-9&hl=zh-CN&sa=X&d=12934547536064418714&ei=WnFkabHlHrOlieoPyq27-AE&scisig=AHkA5jTLGYHtdVRT9PUIclDyyt3s&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=9&folt=kw-top)
*Z Song,M Kim,J Lee,TH Kwon,J Kim,JH Kim,SG Kim…*

Main category: learned "cost model"

TL;DR: PHIND is a smart-toilet system that passively and automatically monitors defecation using optical/pressure sensors and cloud-based CNNs to classify stool form (Bristol Scale) and track defecatory parameters, offering objective, real-time data without user input.


<details>
  <summary>Details</summary>
Motivation: Current stool analysis relies on self-reported diaries, which suffer from recall bias and poor adherence; there is a need for an accurate, effortless, and passive monitoring solution for home-based health tracking.

Method: The PHIND system integrates optical and pressure sensors with cloud-based convolutional neural networks. It involves three stages: (1) hardware assembly on a standard toilet, (2) training CNN models for stool classification and event detection, and (3) real-time image acquisition, cloud deployment, data storage, and visualization.

Result: PHIND delivers objective, near real-time stool classification and defecation metrics (e.g., event duration, time to first stool drop) with high accuracy, eliminating recall bias and enabling reliable longitudinal monitoring of gastrointestinal health.

Conclusion: PHIND provides a robust, user-transparent platform for automated stool monitoring that outperforms traditional diary-based methods, supporting early detection and long-term management of GI disorders with minimal setup time (2 days excluding PCB fabrication).

Abstract: With the growing demand for accurate yet effortless health monitoring at home, most current approaches to stool analysis rely on self-reported diaries that are prone to recall bias and low adherence. Here we present a fully passive alternative: the Precision Health Integrated Diagnostic (PHIND) system, a smart-toilet-based platform that enables automated defecation monitoring without requiring users to alter their daily routines. By integrating optical and pressure sensors with cloud-based convolutional neural networks, the PHIND system classifies stool form according to the Bristol Stool Form Scale and records key defecatory parameters, including total event time, defecation duration and time to first stool drop. The protocol proceeds in three principal stages: (1) assembling and mounting the hardware onto a conventional toilet; (2) training convolutional neural network models for stool classification and event detection; and (3) image acquisition and deploying cloud infrastructure for real-time analysis, data storage and visualization. Compared with traditional methods that depend on user-reported stool diaries, PHIND provides objective, near real-time data free from recall error, enabling more reliable early detection and long-term management of gastrointestinal conditions. Researchers and clinicians can expect high classification accuracy and robust, longitudinal insights into defecation patterns. The complete protocol—from hardware setup to system validation—can typically be completed within 2 d, excluding printed circuit board manufacturing, which generally requires up to 15 d depending on the manufacturing provider.

</details>


### [18] [Real-Time Quantized YOLO Object Detection on Serverless Cloud Functions: An Experimental and Analytical Study](https://scholar.google.com/scholar_url?url=https://www.researchsquare.com/article/rs-8446241/latest.pdf&hl=zh-CN&sa=X&d=18049560817206778172&ei=hKtlae3BJbK16rQPitqooQg&scisig=AHkA5jS_wwHQE8rS0Ye0TRWWmKnW&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=0&folt=kw-top)
*MDR HASAN,S Biswas*

Main category: learned "cost model"

TL;DR: 本文系统研究了在无服务器平台（AWS Lambda）上部署量化YOLO目标检测模型的延迟与成本权衡，发现INT8量化显著降低热启动推理延迟，但冷启动仍是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算因其自动扩展、细粒度计费和低运维开销，适用于机器学习推理等突发性负载，但在实时计算机视觉任务中仍面临CPU限制、内存约束和冷启动延迟等挑战。

Method: 在AWS Lambda上使用ONNX Runtime部署量化后的YOLO模型，通过实验与分析方法研究INT8量化对目标检测任务在实际部署条件下的延迟与成本影响。

Result: 实验表明，INT8量化显著降低了热启动推理延迟，但冷启动延迟仍是整体性能的主要瓶颈。

Conclusion: 尽管量化技术能有效优化无服务器环境中模型推理的热启动性能，但要实现实时计算机视觉流水线，仍需解决冷启动问题。

Abstract: Serverless computing has emerged as an attractive execution model for event-driven applicationsdue to its automatic scalability, fine-grained billing, and minimal operational overhead. Theseproperties make serverless platforms appealing for machine learning inference workloads with variableor bursty demand. However, deploying real-time computer vision pipelines on serverless infrastructuresremains challenging due to CPU-only execution, memory constraints, and cold-start overheads [5, 6]. This paper presents a systematic experimental and analytical study of quantized object detectiondeployed on serverless cloud functions. Using quantized YOLO-based models executed on AWSLambda with ONNX Runtime, we investigate latency–cost trade-offs under practical deploy-ment constraints. Experimental results show that INT8 quantization substantially reduces warm-startinference latency, while cold-start behavior remains the dominant bottleneck.

</details>


### [19] [Enhancing Elementary Literacy Through the'Class Lit-eracy Tree': A Strategic Case Study of the'One Week One Book'Program](https://scholar.google.com/scholar_url?url=https://jse.rezkimedia.org/index.php/jse/article/download/707/258&hl=zh-CN&sa=X&d=2159830123818094372&ei=hKtlae3BJbK16rQPitqooQg&scisig=AHkA5jReZdsox4Zw76HkptOp1z7O&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=1&folt=kw-top)
*N Sumartina,H Hidayat*

Main category: learned "cost model"

TL;DR: 本研究提出并验证了一种名为“班级识字树”（POLIKEL）的低成本、创新教学策略，结合“一周一书”（SAMI SAKU）项目，在印尼苏门当县两所小学五年级实施，显著提升了学生的阅读兴趣、文本理解能力与基础写作技能。


<details>
  <summary>Details</summary>
Motivation: 正式教育常因资源限制难以持续推动识字教育，因此需要一种低成本且可持续的教学策略来弥合这一差距。

Method: 采用质性描述性案例研究设计，在印尼苏门当县两所小学开展；通过观察、深度访谈和文献分析收集数据，并使用互动模型进行分析。

Result: “班级识字树”策略营造了愉悦、参与性强且目标导向的学习环境，有效培养了学生持续的读写习惯；学生在阅读兴趣、文本内在要素理解及基础写作技能方面均有显著提升。

Conclusion: 该策略的成功依赖于强有力的学校领导和配套的管理循环，验证了PDCA循环作为可持续教学创新框架的有效性，是一种可推广、可持续的小学基础识字教育替代模式。

Abstract: Creating a literate generation requires a sustained pedagogical process, a goal often challenged by resource limitations in formal education. This study addresses this gap by examining an innovative, low-cost strategy. Introduction This research describes a literacy learning strategy using a'Class Literacy Tree'(POLIKEL) to support the'One Week One Book'(SAMI SAKU) program and its impact on the reading interest and literacy skills of fifth-grade elementary students. Method A qualitative, descriptive case study design was employed at two elementary schools in Sumedang Regency, Indonesia. Data were collected via observation, in-depth interviews, and documentation analysis, then analyzed using an interactive model. Results The'Class Literacy Tree'strategy successfully creates a joyful, participatory, and goal-oriented learning environment. The program effectively fosters continuous reading and writing habits, with teachers acting as crucial facilitators. Students' consistent, tangible engagement with the'tree'demonstrated marked improvement in reading interest, comprehension of intrinsic text elements, and basic writing skills. Discussion The strategy's success is contingent on a supporting management cycle strong school leadership, validating the PDCA cycle as a framework for sustainable pedagogical innovation. This strategy serves as an effective, sustainable, and scalable alternative model to enhance foundational literacy in elementary education.





</details>


### [20] [Design under uncertainty of an isolated microgrid powered by offshore renewable energy sources](https://scholar.google.com/scholar_url?url=https://theses.hal.science/tel-05450784/file/EBNOU.pdf&hl=zh-CN&sa=X&d=13400850301655297292&ei=hKtlae3BJbK16rQPitqooQg&scisig=AHkA5jTGW6DaqZEaEEkeV6Xkf7PH&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=3&folt=kw-top)
*H Ebnou*

Main category: learned "cost model"

TL;DR: 本文提出了一种结合TimeGAN深度学习与多目标随机优化的孤立微电网优化设计方法，通过频率分解预处理提升场景生成质量，并在留尼汪岛案例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 应对离岸可再生能源微电网设计中电力需求与生产不确定性的问题，提升系统在自主性、低碳性和抗不确定性方面的性能。

Method: 开发了一套综合工具：首先利用基于频率分解的预处理增强TimeGAN模型对气候与用电数据多尺度特征的捕捉能力，生成长期合成场景；随后通过统计与谱分析方法验证场景质量，并将其作为多目标随机优化模型的输入，用于微电网容量配置。

Result: 在留尼汪岛的实际案例中成功应用该方法，生成的场景有效表征了不确定性，优化后的微电网具备高自主性、低碳排放和强鲁棒性。

Conclusion: 所提方法能有效支持考虑多重不确定性的孤立微电网优化设计，为高比例可再生能源系统的规划提供了可靠技术路径。

Abstract: This thesis presents a methodology for the optimal design of an isolated microgrid powered by offshore renewable resources (wind, solar, storage, diesel), while accounting for uncertainties in electricity demand and production. A comprehensive tool is developed, combining long-term scenario generation using the TimeGAN deep learning model with multi-objective stochastic optimization of the energy system. To address TimeGAN’s limitations on long sequences, a dedicated preprocessing step based on frequency decomposition is introduced, enabling the capture of multi-scale patterns in climatic and consumption data. The generated synthetic scenarios are rigorously validated using statistical and spectral criteria, and are then used as inputs for the sizing phase. The methodology is applied to a real case study on La Réunion Island, demonstrating its effectiveness for designing autonomous, low-carbon, and uncertainty-resilient energy systems.

</details>
