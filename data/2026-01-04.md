<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 60]
- [cs.DB](#cs.DB) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [CAT: A Metric-Driven Framework for Analyzing the Consistency-Accuracy Relation of LLMs under Controlled Input Variations](https://arxiv.org/abs/2512.23711)
*Paulo Cavalin,Cassia Sanctos,Marcelo Grave,Claudio Pinhanez,Yago Primerano*

Main category: cs.CL

TL;DR: 本文提出了CAT框架，用于评估和可视化大语言模型（LLM）在可控输入变化下准确性与响应一致性之间的相互关系，引入CAR曲线和CORE指标以量化二者权衡，并在多项选择基准上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估主要关注准确性或基准得分，而一致性作为高风险应用场景中的关键属性日益受到重视；作者认为需同时独立评估并考察二者间的相互依赖关系，以实现更细致的模型评估。

Method: 提出CAT框架，核心包括一致性-准确性关系（CAR）曲线和最小一致性准确性（MCA）度量，用于刻画随着一致性要求提高时准确性的变化；并设计一致性导向鲁棒性估计（CORE）指数，综合CAR曲线的面积与形状以全局衡量准确性与一致性的权衡；在多种通用及领域特定LLM和多项选择基准上进行实验，并讨论框架向开放式长文本任务的扩展。

Result: 在多个通用和领域特定的大语言模型及多项选择基准上的实验证明了CAT框架的有效性，能够揭示不同模型在准确性与一致性之间的权衡特性，并通过CORE指标提供可比较的综合评估。

Conclusion: CAT框架为LLM评估提供了新视角，强调准确性与一致性的联合分析，其提出的CAR曲线和CORE指标有助于更全面地理解模型行为，并具备向非多项选择任务扩展的潜力。

Abstract: We introduce \textsc{CAT}, a framework designed to evaluate and visualize the \emph{interplay} of \emph{accuracy} and \emph{response consistency} of Large Language Models (LLMs) under controllable input variations, using multiple-choice (MC) benchmarks as a case study. Current evaluation practices primarily focus on model capabilities such as accuracy or benchmark scores and, more recently, measuring consistency is being considered an essential property for deploying LLMs in high-stake, real-world applications. We argue in this paper that although both dimensions should still be evaluated independently, their inter-dependency also need to be considered for a more nuanced evaluation of LLMs. At the core of \textsc{CAT} are the \emph{Consistency-Accuracy Relation (CAR)} curves, which visualize how model accuracy varies with increasing consistency requirements, as defined by the \emph{Minimum-Consistency Accuracy (MCA)} metric. We further propose the \emph{Consistency-Oriented Robustness Estimate (CORE)} index, a global metric that combines the area and shape of the CAR curve to quantify the trade-off between accuracy and consistency. We present a practical demonstration of our framework across a diverse set of generalist and domain-specific LLMs, evaluated on multiple MC benchmarks. We also outline how \textsc{CAT} can be extended beyond MC tasks to support long-form, open-ended evaluations through adaptable scoring functions.

</details>


### [2] [PyBangla at BLP-2025 Task 2: Enhancing Bangla-to-Python Code Generation with Iterative Self-Correction and Multilingual Agents](https://arxiv.org/abs/2512.23713)
*Jahidul Islam,Md Ataullha,Saiful Azad*

Main category: cs.CL

TL;DR: 提出BanglaCodeAct框架，通过多智能体提示与迭代自修正机制，在无需任务特定微调的情况下，显著提升低资源语言（孟加拉语）到Python代码的生成性能，Qwen3-8B在该框架下达到当前最优结果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在英文提示下的代码生成表现优异，但在低资源语言（如孟加拉语）中进展有限。现有方法通常依赖任务特定微调，难以泛化且成本高，亟需一种高效、可扩展的解决方案来支持低资源语言的NL2Code任务。

Method: 提出BanglaCodeAct，一个基于多智能体的框架，利用开源多语言LLM，在Thought-Code-Observation循环中实现动态代码生成、测试与迭代修正。不依赖微调，而是通过提示工程和自纠错机制提升生成质量，并在mHumanEval数据集上评估多个小参数开源模型。

Result: 在Bangla NL2Code任务中，Qwen3-8B结合BanglaCodeAct在开发集上达到94.0%的pass@1准确率，在盲测集上达71.6%，显著优于现有方法，为孟加拉语到Python代码生成设立了新基准。

Conclusion: BanglaCodeAct证明了基于智能体的推理机制能有效提升低资源语言的代码生成可靠性，无需微调即可取得优异性能，为其他低资源语言的NL2Code任务提供了可推广的范式。

Abstract: LLMs excel at code generation from English prompts, but this progress has not extended to low-resource languages. We address Bangla-to-Python code generation by introducing BanglaCodeAct, an agent-based framework that leverages multi-agent prompting and iterative self-correction. Unlike prior approaches relying on task-specific fine-tuning, BanglaCodeAct employs an open-source multilingual LLM within a Thought-Code-Observation loop, enabling dynamic generation, testing, and refinement of code from Bangla instructions. We benchmark several small-parameter open-source LLMs and evaluate their effectiveness on the mHumanEval dataset for Bangla NL2Code. Our results show that Qwen3-8B, when deployed with BanglaCodeAct, achieves the best performance, with pass@1 accuracy of 94.0\% on the development set and 71.6\% on the blind test set. These results establish a new benchmark for Bangla-to-Python translation and highlight the potential of agent-based reasoning for reliable code generation in low-resource languages. Experimental scripts are publicly available at github.com/jahidulzaid/PyBanglaCodeActAgent.

</details>


### [3] [PharmaShip: An Entity-Centric, Reading-Order-Supervised Benchmark for Chinese Pharmaceutical Shipping Documents](https://arxiv.org/abs/2512.23714)
*Tingwei Xie,Tianyi Zhou,Yonghong Song*

Main category: cs.CL

TL;DR: PharmaShip 是一个面向中文药品运输单据的真实世界数据集，包含噪声 OCR 和异构模板，支持序列实体识别（SER）、关系抽取（RE）和阅读顺序预测（ROP）三项任务，并采用以实体为中心的评估协议。实验表明，像素与几何信息具有互补性，而引入阅读顺序正则化能显著提升模型性能，尤其在 SER 和 EL 任务上表现最稳健。


<details>
  <summary>Details</summary>
Motivation: 现有预训练文本-布局模型在真实世界复杂文档（如药品运输单据）中的鲁棒性尚未充分验证，尤其是在存在噪声 OCR 和模板高度异构的情况下。为系统评估并推动模型在安全关键领域的文档理解能力，作者构建了 PharmaShip 数据集。

Method: 构建包含 SER、RE 和 ROP 三项任务的中文药品单据数据集 PharmaShip；采用实体中心化评估协议；对五种代表性基线模型（LiLT、LayoutLMv3-base、GeoLayoutLM 及其 RORE 增强版本）进行标准化评测，统一预处理、数据划分与优化设置；引入阅读顺序导向的正则化策略。

Result: 实验发现：1）像素感知与几何感知模型具有互补归纳偏置，但单独使用均不充分；2）注入阅读顺序正则化显著提升 SER 与 EL 性能，获得最稳健配置；3）更长的位置编码覆盖可稳定页面后部预测并减少截断伪影；4）ROP 在词级别准确，但在段落级别因边界模糊和长距离交叉而具挑战性。

Conclusion: PharmaShip 为药品领域安全关键的文档理解提供了可控、可复现的基准，并揭示了序列感知约束作为结构建模中一种可迁移的归纳偏置的重要性。

Abstract: We present PharmaShip, a real-world Chinese dataset of scanned pharmaceutical shipping documents designed to stress-test pre-trained text-layout models under noisy OCR and heterogeneous templates. PharmaShip covers three complementary tasks-sequence entity recognition (SER), relation extraction (RE), and reading order prediction (ROP)-and adopts an entity-centric evaluation protocol to minimize confounds across architectures. We benchmark five representative baselines spanning pixel-aware and geometry-aware families (LiLT, LayoutLMv3-base, GeoLayoutLM and their available RORE-enhanced variants), and standardize preprocessing, splits, and optimization. Experiments show that pixels and explicit geometry provide complementary inductive biases, yet neither alone is sufficient: injecting reading-order-oriented regularization consistently improves SER and EL and yields the most robust configuration, while longer positional coverage stabilizes late-page predictions and reduces truncation artifacts. ROP is accurate at the word level but challenging at the segment level, reflecting boundary ambiguity and long-range crossings. PharmaShip thus establishes a controlled, reproducible benchmark for safety-critical document understanding in the pharmaceutical domain and highlights sequence-aware constraints as a transferable bias for structure modeling. We release the dataset at https://github.com/KevinYuLei/PharmaShip.

</details>


### [4] [Noise-Driven Persona Formation in Reflexive Neural Language Generation](https://arxiv.org/abs/2512.23716)
*Toshiyuki Shigemura*

Main category: cs.CL

TL;DR: 本文提出Luca-Noise Reflex Protocol（LN-RP），通过在大语言模型初始生成状态注入随机噪声种子，观察其在152个生成周期中语言行为的非线性转变，发现三种具有不同熵特征的稳定人格模式，并证明外部噪声可可靠诱发反射生成动态中的相变。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型中由噪声驱动的人格涌现机制，理解外部扰动如何影响模型的反射性生成行为及长期语言一致性。

Method: 在大语言模型的初始生成状态中注入随机噪声种子，追踪152个生成周期中的语言行为变化，分析其熵特征与人格模式稳定性，并进行统计显著性检验（p < 0.01）。

Result: 识别出三种具有显著差异（p < 0.01）且稳定的人格模式，每种模式具有独特的熵特征；外部噪声可可靠诱导反射生成动态中的相变，并保持人格一致性。

Conclusion: LN-RP为研究大语言模型中的反射生成、涌现行为和长程语言一致性提供了一种可复现的计算框架，揭示了噪声在塑造模型行为中的关键作用。

Abstract: This paper introduces the Luca-Noise Reflex Protocol (LN-RP), a computational framework for analyzing noise-driven persona emergence in large language models. By injecting stochastic noise seeds into the initial generation state, we observe nonlinear transitions in linguistic behavior across 152 generation cycles. Our results reveal three stable persona modes with distinct entropy signatures, and demonstrate that external noise sources can reliably induce phase transitions in reflexive generation dynamics. Quantitative evaluation confirms consistent persona retention and significant differences across modes (p < 0.01). The protocol provides a reproducible method for studying reflexive generation, emergent behavior, and longrange linguistic coherence in LLMs.

</details>


### [5] [HarmTransform: Transforming Explicit Harmful Queries into Stealthy via Multi-Agent Debate](https://arxiv.org/abs/2512.23717)
*Shenzhe Zhu*

Main category: cs.CL

TL;DR: HarmTransform 是一个基于多智能体辩论的框架，用于将有害查询系统性地转化为更具隐蔽性的形式，以增强大语言模型（LLM）的安全对齐训练数据；实验证明其优于基线方法，但也揭示了辩论机制可能带来主题偏移和复杂性增加的问题。


<details>
  <summary>Details</summary>
Motivation: 当前 LLM 的安全对齐方法主要关注显性有害内容，忽视了用户通过隐晦措辞伪装恶意意图的情况，导致安全训练数据存在显著缺口。

Method: 提出 HarmTransform 框架，利用多个智能体之间的迭代批评与优化，系统性地生成保留原始有害意图但表层无害的隐蔽查询变体。

Result: 实验表明 HarmTransform 在生成有效隐蔽有害查询方面显著优于标准基线；同时发现多智能体辩论虽能提升隐蔽性，但也可能导致主题偏移和冗余复杂性。

Conclusion: 多智能体辩论在构建全面安全训练数据方面具有潜力，但也需谨慎处理其引入的副作用，如主题漂移和表达冗余。

Abstract: Large language models (LLMs) are equipped with safety mechanisms to detect and block harmful queries, yet current alignment approaches primarily focus on overtly dangerous content and overlook more subtle threats. However, users can often disguise harmful intent through covert rephrasing that preserves malicious objectives while appearing benign, which creates a significant gap in existing safety training data. To address this limitation, we introduce HarmTransform, a multi-agent debate framework for systematically transforming harmful queries into stealthier forms while preserving their underlying harmful intent. Our framework leverages iterative critique and refinement among multiple agents to generate high-quality, covert harmful query transformations that can be used to improve future LLM safety alignment. Experiments demonstrate that HarmTransform significantly outperforms standard baselines in producing effective query transformations. At the same time, our analysis reveals that debate acts as a double-edged sword: while it can sharpen transformations and improve stealth, it may also introduce topic shifts and unnecessary complexity. These insights highlight both the promise and the limitations of multi-agent debate for generating comprehensive safety training data.

</details>


### [6] [Emergent World Beliefs: Exploring Transformers in Stochastic Games](https://arxiv.org/abs/2512.23722)
*Adam Kamel,Tanish Rastogi,Michael Ma,Kailash Ranganathan,Kevin Zhu*

Main category: cs.CL

TL;DR: 该论文研究了基于Transformer的大语言模型（LLM）在不完全信息博弈（以德州扑克为代表）中是否能自发学习环境的随机性表征。通过在扑克手牌历史数据上预训练GPT风格模型并使用非线性探针分析其内部激活，作者发现模型能无监督地学习到确定性结构（如牌力等级）和随机特征（如权益值），且这些表征可被解码并与理论信念状态相关，表明LLM能够构建对部分可观测环境的内部世界模型。


<details>
  <summary>Details</summary>
Motivation: 以往研究表明大语言模型能在完全信息博弈中发展出对应环境隐状态的内部表征（即“涌现世界模型”）。本文旨在将这一研究扩展至不完全信息领域，探究LLM是否也能在部分可观测马尔可夫决策过程（POMDP）中学习到对随机环境的有效表征。

Method: 在德州扑克手牌历史（PHH）数据上预训练一个GPT风格的语言模型，并利用（主要是非线性的）探针（probes）对其内部激活进行分析，以检测模型是否编码了与游戏状态相关的确定性和随机性特征。

Result: 实验结果表明，该模型在没有显式指导的情况下，学习到了诸如手牌等级（确定性结构）和权益（equity，随机特征）等关键概念。通过非线性探针可以有效地解码这些内部表征，并且它们与德州扑克理论中的信念状态（belief states）显著相关。

Conclusion: 研究证实，大语言模型不仅限于处理完全信息任务，还能在复杂的、具有不完全信息的随机环境中（如德州扑克）自发地学习并构建有效的内部世界模型，其内部表征能够捕捉环境的确定性和随机性方面。

Abstract: Transformer-based large language models (LLMs) have demonstrated strong reasoning abilities across diverse fields, from solving programming challenges to competing in strategy-intensive games such as chess. Prior work has shown that LLMs can develop emergent world models in games of perfect information, where internal representations correspond to latent states of the environment. In this paper, we extend this line of investigation to domains of incomplete information, focusing on poker as a canonical partially observable Markov decision process (POMDP). We pretrain a GPT-style model on Poker Hand History (PHH) data and probe its internal activations. Our results demonstrate that the model learns both deterministic structure, such as hand ranks, and stochastic features, such as equity, without explicit instruction. Furthermore, by using primarily nonlinear probes, we demonstrated that these representations are decodeable and correlate with theoretical belief states, suggesting that LLMs are learning their own representation of the stochastic environment of Texas Hold'em Poker.

</details>


### [7] [Break Out the Silverware -- Semantic Understanding of Stored Household Items](https://arxiv.org/abs/2512.23739)
*Michaela Levi-Richter,Reuth Mirsky,Oren Glickman*

Main category: cs.CL

TL;DR: 本文提出了“家用物品存放挑战”（Stored Household Item Challenge）这一新基准任务，用于评估服务机器人对日常物品存放位置的常识推理能力，并发布了包含真实与合成数据的两个数据集。作者还提出了NOAM（Non-visible Object Allocation Model），一种结合场景理解与大语言模型推理的混合智能体，在该任务上显著优于现有基线方法，接近人类表现。


<details>
  <summary>Details</summary>
Motivation: 当前服务机器人虽在视觉与操作方面取得进展，但仍缺乏推断日常物品（如盘子）隐藏存放位置所需的常识推理能力。为系统评估和推动机器人在此类认知任务上的发展，需建立标准化基准。

Method: 提出Stored Household Item Challenge基准，包含100个真实厨房中人工标注的物品-图像对（评估集）和6,500个基于公开厨房图像、带存储区域多边形标注的开发集。同时提出NOAM模型：先将视觉输入转化为自然语言描述（包括空间上下文和可见容器），再利用大语言模型（如GPT-4）推理最可能的隐藏存储位置。

Result: NOAM在预测准确率上显著优于随机选择、纯视觉语言模型（如Grounding-DINO+SAM）、主流多模态大模型（如Gemini、GPT-4o、Kosmos-2等），性能接近人类水平。

Conclusion: 将结构化场景理解与大语言模型结合的混合架构能有效实现家庭环境中的常识推理，NOAM展示了在服务机器人中部署高认知能力智能体的有效路径，并为未来研究提供了数据与方法基础。

Abstract: ``Bring me a plate.'' For domestic service robots, this simple command reveals a complex challenge: inferring where everyday items are stored, often out of sight in drawers, cabinets, or closets. Despite advances in vision and manipulation, robots still lack the commonsense reasoning needed to complete this task. We introduce the Stored Household Item Challenge, a benchmark task for evaluating service robots' cognitive capabilities: given a household scene and a queried item, predict its most likely storage location.
  Our benchmark includes two datasets: (1) a real-world evaluation set of 100 item-image pairs with human-annotated ground truth from participants' kitchens, and (2) a development set of 6,500 item-image pairs annotated with storage polygons over public kitchen images. These datasets support realistic modeling of household organization and enable comparative evaluation across agent architectures.
  To begin tackling this challenge, we introduce NOAM (Non-visible Object Allocation Model), a hybrid agent pipeline that combines structured scene understanding with large language model inference. NOAM converts visual input into natural language descriptions of spatial context and visible containers, then prompts a language model (e.g., GPT-4) to infer the most likely hidden storage location. This integrated vision-language agent exhibits emergent commonsense reasoning and is designed for modular deployment within broader robotic systems.
  We evaluate NOAM against baselines including random selection, vision-language pipelines (Grounding-DINO + SAM), leading multimodal models (e.g., Gemini, GPT-4o, Kosmos-2, LLaMA, Qwen), and human performance. NOAM significantly improves prediction accuracy and approaches human-level results, highlighting best practices for deploying cognitively capable agents in domestic environments.

</details>


### [8] [Entropy-Aware Speculative Decoding Toward Improved LLM Reasoning](https://arxiv.org/abs/2512.23765)
*Tiancheng Su,Meicong Zhang,Guoxiu He*

Main category: cs.CL

TL;DR: 提出了一种无需训练的推测解码增强方法EASD，通过动态熵惩罚机制，在保持效率的同时提升大语言模型推理性能，甚至超越目标模型本身。


<details>
  <summary>Details</summary>
Motivation: 标准推测解码（SD）因草稿模型与目标模型过度对齐而受限于目标模型性能，难以进一步提升推理质量。

Method: 在标准SD基础上引入动态熵惩罚机制：利用采样分布的熵衡量模型不确定性；当草稿模型和目标模型均呈现高熵且top-N预测高度重叠时，拒绝该token并由目标模型重新采样，防止低置信度错误传播。

Result: 在多个推理基准上，EASD consistently优于现有SD方法，并在多数情况下超越目标LLM本身，同时保持与SD相当的效率。

Conclusion: EASD通过引入基于熵的动态验证机制，在不增加训练成本的前提下有效提升了推测解码的准确性和潜力，突破了传统SD对目标模型性能的依赖。

Abstract: Speculative decoding (SD) accelerates large language model (LLM) reasoning by using a small draft model to generate candidate tokens, which the target LLM either accepts directly or regenerates upon rejection. However, excessive alignment between the draft and target models constrains SD to the performance of the target LLM. To address this limitation, we propose Entropy-Aware Speculative Decoding (EASD), a training-free enhancement. Building on standard SD, EASD incorporates a dynamic entropy-based penalty. At each decoding step, we employ the entropy of the sampling distribution to quantify model uncertainty. When both models exhibit high entropy with substantial overlap among their top-N predictions, the corresponding token is rejected and re-sampled by the target LLM. This penalty prevents low-confidence errors from propagating. By incorporating draft-model verification, EASD enables the possibility of surpassing the target model's inherent performance. Experiments across multiple reasoning benchmarks demonstrate that EASD consistently outperforms existing SD methods and, in most cases, surpasses the target LLM itself. We further prove that the efficiency of EASD is comparable to that of SD. The code can be found in the Supplementary Materials.

</details>


### [9] [MiMo-Audio: Audio Language Models are Few-Shot Learners](https://arxiv.org/abs/2512.23808)
*Xiaomi LLM-Core Team,:,Dong Zhang,Gang Wang,Jinlong Xue,Kai Fang,Liang Zhao,Rui Ma,Shuhuai Ren,Shuo Liu,Tao Guo,Weiji Zhuang,Xin Zhang,Xingchen Song,Yihan Yan,Yongzhe He,Cici,Bowen Shen,Chengxuan Zhu,Chong Ma,Chun Chen,Heyu Chen,Jiawei Li,Lei Li,Menghang Zhu,Peidian Li,Qiying Wang,Sirui Deng,Weimin Xiong,Wenshan Huang,Wenyu Yang,Yilin Jiang,Yixin Yang,Yuanyuan Tian,Yue Ma,Yue Yu,Zihan Zhang,Zihao Yue,Bangjun Xiao,Bingquan Xia,Bofei Gao,Bowen Ye,Can Cai,Chang Liu,Chenhong He,Chunan Li,Dawei Zhu,Duo Zhang,Fengyuan Shi,Guoan Wang,Hailin Zhang,Hanglong Lv,Hanyu Li,Hao Tian,Heng Qu,Hongshen Xu,Houbin Zhang,Huaqiu Liu,Jiangshan Duo,Jianguang Zuo,Jianyu Wei,Jiebao Xiao,Jinhao Dong,Jun Shi,Junhao Hu,Kainan Bao,Kang Zhou,Linghao Zhang,Meng Chen,Nuo Chen,Peng Zhang,Qianli Chen,Qiantong Wang,Rang Li,Shaohui Liu,Shengfan Wang,Shicheng Li,Shihua Yu,Shijie Cao,Shimao Chen,Shuhao Gu,Weikun Wang,Wenhan Ma,Xiangwei Deng,Xing Yong,Xing Zhang,Xu Wang,Yifan Song,Yihao Zhao,Yingbo Zhao,Yizhao Gao,Yu Cheng,Yu Tu,Yudong Wang,Zhaojun Huang,Zhengju Tang,Zhenru Lin,Zhichao Song,Zhipeng Xu,Zhixian Zheng,Zihan Jiang*

Main category: cs.CL

TL;DR: 本文提出MiMo-Audio模型，通过在超大规模音频数据（超过一亿小时）上进行下一token预测预训练，展现出强大的少样本学习和跨任务泛化能力。MiMo-Audio-7B-Base在多个开源语音智能与音频理解基准上达到SOTA，并能完成训练数据中未包含的任务（如语音转换、风格迁移等）。进一步通过指令微调和引入思维机制，MiMo-Audio-7B-Instruct在音频理解、对话和指令式语音合成任务上接近或超越闭源模型。


<details>
  <summary>Details</summary>
Motivation: 现有音频语言模型通常依赖任务特定的微调，而人类仅凭少量示例或简单指令即可泛化到新音频任务。受GPT-3在文本领域通过大规模预训练实现强泛化能力的启发，作者认为该范式同样适用于音频领域。

Method: 在超过一亿小时的音频数据上对MiMo-Audio进行大规模下一token预测预训练；随后构建多样化的指令微调语料库，并在音频理解与生成中引入“思维机制”（thinking mechanisms）进行后训练优化。

Result: MiMo-Audio-7B-Base在开源模型中于语音智能和音频理解基准上取得SOTA性能，并能泛化至训练数据中未见的任务（如语音转换、风格迁移、语音编辑），还具备高质量的语音续写能力（如生成脱口秀、朗诵、直播、辩论）。MiMo-Audio-7B-Instruct在MMSU、MMAU、MMAR、MMAU-Pro、Big Bench Audio、MultiChallenge Audio及instruct-TTS等评测中达到开源SOTA，性能接近或超越闭源模型。

Conclusion: 大规模基于下一token预测的音频预训练可有效激发模型的少样本学习与跨任务泛化能力；结合指令微调与思维机制，MiMo-Audio系列模型在多项音频任务上达到领先水平，验证了通用音频语言模型的可行性。

Abstract: Existing audio language models typically rely on task-specific fine-tuning to accomplish particular audio tasks. In contrast, humans are able to generalize to new audio tasks with only a few examples or simple instructions. GPT-3 has shown that scaling next-token prediction pretraining enables strong generalization capabilities in text, and we believe this paradigm is equally applicable to the audio domain. By scaling MiMo-Audio's pretraining data to over one hundred million of hours, we observe the emergence of few-shot learning capabilities across a diverse set of audio tasks. We develop a systematic evaluation of these capabilities and find that MiMo-Audio-7B-Base achieves SOTA performance on both speech intelligence and audio understanding benchmarks among open-source models. Beyond standard metrics, MiMo-Audio-7B-Base generalizes to tasks absent from its training data, such as voice conversion, style transfer, and speech editing. MiMo-Audio-7B-Base also demonstrates powerful speech continuation capabilities, capable of generating highly realistic talk shows, recitations, livestreaming and debates. At the post-training stage, we curate a diverse instruction-tuning corpus and introduce thinking mechanisms into both audio understanding and generation. MiMo-Audio-7B-Instruct achieves open-source SOTA on audio understanding benchmarks (MMSU, MMAU, MMAR, MMAU-Pro), spoken dialogue benchmarks (Big Bench Audio, MultiChallenge Audio) and instruct-TTS evaluations, approaching or surpassing closed-source models. Model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-Audio.

</details>


### [10] [Explaining News Bias Detection: A Comparative SHAP Analysis of Transformer Model Decision Mechanisms](https://arxiv.org/abs/2512.23835)
*Himel Ghosh*

Main category: cs.CL

TL;DR: 该研究通过SHAP解释方法对两种基于Transformer的新闻偏见检测模型进行可解释性对比，发现领域自适应模型在减少误报和对齐归因与预测方面显著优于通用偏见检测模型。


<details>
  <summary>Details</summary>
Motivation: 当前新闻文本中的自动偏见检测模型缺乏对其决策机制和失败原因的理解，限制了其在新闻分析和媒体问责中的可靠应用。

Method: 采用SHAP（SHapley Additive exPlanations）方法对两个在BABE数据集上微调的Transformer模型——一个通用偏见检测器和一个领域自适应的RoBERTa模型——进行词级归因分析，比较其在正确与错误预测中的归因模式。

Result: 两个模型虽关注相似的评价性语言类别，但在信号整合方式上存在显著差异：通用偏见检测器对假阳性赋予更强内部证据，导致中立内容被系统性误标；而领域自适应模型归因与预测结果更一致，假阳性减少63%。错误主要源于语篇层面的模糊性，而非显式偏见线索。

Conclusion: 偏见检测系统的评估需纳入可解释性视角，模型架构与训练策略（如领域自适应）对其可靠性及在新闻场景中的适用性具有关键影响。

Abstract: Automated bias detection in news text is heavily used to support journalistic analysis and media accountability, yet little is known about how bias detection models arrive at their decisions or why they fail. In this work, we present a comparative interpretability study of two transformer-based bias detection models: a bias detector fine-tuned on the BABE dataset and a domain-adapted pre-trained RoBERTa model fine-tuned on the BABE dataset, using SHAP-based explanations. We analyze word-level attributions across correct and incorrect predictions to characterize how different model architectures operationalize linguistic bias. Our results show that although both models attend to similar categories of evaluative language, they differ substantially in how these signals are integrated into predictions. The bias detector model assigns stronger internal evidence to false positives than to true positives, indicating a misalignment between attribution strength and prediction correctness and contributing to systematic over-flagging of neutral journalistic content. In contrast, the domain-adaptive model exhibits attribution patterns that better align with prediction outcomes and produces 63\% fewer false positives. We further demonstrate that model errors arise from distinct linguistic mechanisms, with false positives driven by discourse-level ambiguity rather than explicit bias cues. These findings highlight the importance of interpretability-aware evaluation for bias detection systems and suggest that architectural and training choices critically affect both model reliability and deployment suitability in journalistic contexts.

</details>


### [11] [Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?](https://arxiv.org/abs/2512.23836)
*Dingmin Wang,Ji Ma,Shankar Kumar*

Main category: cs.CL

TL;DR: 该论文研究了在检索增强问答中使用大语言模型（LLM）时，长上下文虽有助于引入相关知识，但也引入了大量无关信息，从而损害性能。为此，作者提出一种自适应提示策略，将检索到的信息分块并依次提示LLM作答，通过调整块大小平衡相关信息与噪声。实验表明该方法在使用更少token的情况下达到与标准提示相当的性能，并发现LLM在信息不足时常生成错误答案而非拒绝回答，这是主要错误来源。


<details>
  <summary>Details</summary>
Motivation: 随着扩展上下文窗口在大语言模型中的成功，检索增强生成系统越来越多地采用更宽泛的上下文。然而，更长的上下文虽然便于整合目标知识，也引入了更多无关信息，干扰模型生成并降低性能。因此，亟需一种策略在保留有用信息的同时减少噪声干扰。

Method: 作者设计了一种自适应提示策略：将检索到的文档划分为较小的块，然后按顺序用每个块单独提示LLM回答问题。通过调节块的大小，可以在引入足够相关信息与避免无关信息之间取得平衡。

Result: 在三个开放域问答数据集上的实验表明，所提出的自适应策略在使用更少token的情况下，达到了与标准提示方法相当的性能。此外，分析发现当信息不足时，LLM倾向于生成错误答案而非拒绝回答，这是错误的主要来源。

Conclusion: 自适应分块提示策略有效缓解了长上下文中无关信息对LLM性能的负面影响，在保持性能的同时降低了计算开销。研究还揭示了LLM在面对信息不足时缺乏“拒绝回答”能力的问题，这为未来提升LLM可靠性指明了方向。

Abstract: The success of expanded context windows in Large Language Models (LLMs) has driven increased use of broader context in retrieval-augmented generation. We investigate the use of LLMs for retrieval augmented question answering. While longer contexts make it easier to incorporate targeted knowledge, they introduce more irrelevant information that hinders the model's generation process and degrades its performance. To address the issue, we design an adaptive prompting strategy which involves splitting the retrieved information into smaller chunks and sequentially prompting a LLM to answer the question using each chunk. Adjusting the chunk size allows a trade-off between incorporating relevant information and reducing irrelevant information. Experimental results on three open-domain question answering datasets demonstrate that the adaptive strategy matches the performance of standard prompting while using fewer tokens. Our analysis reveals that when encountering insufficient information, the LLM often generates incorrect answers instead of declining to respond, which constitutes a major source of error. This finding highlights the need for further research into enhancing LLMs' ability to effectively decline requests when faced with inadequate information.

</details>


### [12] [Adversarial Lens: Exploiting Attention Layers to Generate Adversarial Examples for Evaluation](https://arxiv.org/abs/2512.23837)
*Kaustubh Dhole*

Main category: cs.CL

TL;DR: 该论文提出一种基于大语言模型中间注意力层的对抗样本生成方法，利用模型内部的token预测生成语义合理且与模型生成过程一致的扰动，在ArgQuality数据集上验证其有效性，发现此类对抗样本可显著降低评估性能，但部分替换可能导致语法退化。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示或梯度的对抗攻击方法难以保证生成扰动的语义合理性和模型内部一致性；受机制可解释性研究启发，作者希望探索是否可直接利用中间注意力层的token分布生成更自然、有效的对抗样本，以用于压力测试LLM评估系统。

Method: 从LLaMA-3.1-Instruct-8B模型的中间注意力层提取token级预测分布，选取高概率token作为对抗扰动替换原始输入中的对应token，从而构造对抗样本；在ArgQuality论证质量评估任务中，使用同一模型作为生成器和评估器进行实验。

Result: 基于注意力层生成的对抗样本在保持与原始输入语义相似的同时，显著降低了模型的评估性能；但某些层或位置的token替换会导致语法错误，影响实际可用性。

Conclusion: 利用中间层表示生成对抗样本具有潜力，能有效揭示LLM评估流程的脆弱性，但当前方法仍受限于语法退化等问题，需进一步优化以提升实用性。

Abstract: Recent advances in mechanistic interpretability suggest that intermediate attention layers encode token-level hypotheses that are iteratively refined toward the final output. In this work, we exploit this property to generate adversarial examples directly from attention-layer token distributions. Unlike prompt-based or gradient-based attacks, our approach leverages model-internal token predictions, producing perturbations that are both plausible and internally consistent with the model's own generation process. We evaluate whether tokens extracted from intermediate layers can serve as effective adversarial perturbations for downstream evaluation tasks. We conduct experiments on argument quality assessment using the ArgQuality dataset, with LLaMA-3.1-Instruct-8B serving as both the generator and evaluator. Our results show that attention-based adversarial examples lead to measurable drops in evaluation performance while remaining semantically similar to the original inputs. However, we also observe that substitutions drawn from certain layers and token positions can introduce grammatical degradation, limiting their practical effectiveness. Overall, our findings highlight both the promise and current limitations of using intermediate-layer representations as a principled source of adversarial examples for stress-testing LLM-based evaluation pipelines.

</details>


### [13] [Integrating Domain Knowledge for Financial QA: A Multi-Retriever RAG Approach with LLMs](https://arxiv.org/abs/2512.23848)
*Yukun Zhang,Stefan Elbl Droguett,Samyak Jain*

Main category: cs.CL

TL;DR: 本文提出一种多检索器的RAG系统，结合领域知识与上下文信息，利用最新大语言模型（LLM）提升金融数值推理问答任务的性能。通过领域特定训练（SecBERT）和精心设计的提示策略，分别在神经符号模型和提示驱动模型上超越基线并达到SOTA，但仍不及人类专家水平。


<details>
  <summary>Details</summary>
Motivation: 金融数值推理问答任务对大语言模型仍具挑战性，因其需融合金融领域知识与复杂的多步数值推理能力；现有模型因缺乏领域知识而易出错。

Method: 构建多检索器的RAG系统，同时检索外部金融领域知识与内部问题上下文；采用SecBERT编码器进行领域特定训练以增强神经符号模型，并结合最新LLM设计提示策略用于生成答案。

Result: 领域特定训练显著提升神经符号模型性能，超越FinQA基线；提示驱动的LLM模型取得SOTA结果（提升>7%），但仍未达人类专家水平；研究揭示小模型在幻觉损失与外部知识增益间的权衡，而大模型通常从外部知识中获益更多。

Conclusion: 领域特定训练和外部知识检索能有效提升LLM在金融数值推理任务中的表现；最新LLM在少样本学习设置下展现出更强的数值推理能力，但完全达到人类水平仍具挑战。

Abstract: This research project addresses the errors of financial numerical reasoning Question Answering (QA) tasks due to the lack of domain knowledge in finance. Despite recent advances in Large Language Models (LLMs), financial numerical questions remain challenging because they require specific domain knowledge in finance and complex multi-step numeric reasoning. We implement a multi-retriever Retrieval Augmented Generators (RAG) system to retrieve both external domain knowledge and internal question contexts, and utilize the latest LLM to tackle these tasks. Through comprehensive ablation experiments and error analysis, we find that domain-specific training with the SecBERT encoder significantly contributes to our best neural symbolic model surpassing the FinQA paper's top model, which serves as our baseline. This suggests the potential superior performance of domain-specific training. Furthermore, our best prompt-based LLM generator achieves the state-of-the-art (SOTA) performance with significant improvement (>7%), yet it is still below the human expert performance. This study highlights the trade-off between hallucinations loss and external knowledge gains in smaller models and few-shot examples. For larger models, the gains from external facts typically outweigh the hallucination loss. Finally, our findings confirm the enhanced numerical reasoning capabilities of the latest LLM, optimized for few-shot learning.

</details>


### [14] [Disentangling Learning from Judgment: Representation Learning for Open Response Analytics](https://arxiv.org/abs/2512.23941)
*Conrad Borchers,Manit Patel,Seiyon M. Lee,Anthony F. Botelho*

Main category: cs.CL

TL;DR: 该研究提出一种以分析为导向的框架，将学生开放式回答的内容信号与教师评分倾向分离，通过结合教师历史先验和文本嵌入提升自动评分效果，并使评分过程可审查。


<details>
  <summary>Details</summary>
Motivation: 传统自动评分方法常混淆学生作答内容与教师评分习惯，缺乏透明度和可审计性，难以支持教学反思与改进。

Method: 利用去标识化的ASSISTments数学开放式作答数据，将教师评分历史建模为动态先验，从句子嵌入中提取文本表示，并通过中心化和残差化处理减轻题目提示和教师偏差的影响；采用时间验证的线性模型量化各信号贡献，并通过投影方法可视化模型分歧以供定性检查。

Result: 教师先验对成绩预测影响显著；结合先验与内容嵌入的模型表现最佳（AUC≈0.815），仅用内容嵌入的模型虽高于随机水平但较弱（AUC≈0.626）；校正评分者效应后，内容嵌入的残差表示更精炼，保留了更具信息量的维度，能更好反映学生理解而非表面作答差异。

Conclusion: 该研究提供了一个实用的分析流程，将嵌入从普通特征转化为支持教学反思的学习分析工具，使教师和研究者能够审视评分实践与学生推理证据之间的一致性或冲突。

Abstract: Open-ended responses are central to learning, yet automated scoring often conflates what students wrote with how teachers grade. We present an analytics-first framework that separates content signals from rater tendencies, making judgments visible and auditable via analytics. Using de-identified ASSISTments mathematics responses, we model teacher histories as dynamic priors and derive text representations from sentence embeddings, incorporating centering and residualization to mitigate prompt and teacher confounds. Temporally-validated linear models quantify the contributions of each signal, and a projection surfaces model disagreements for qualitative inspection. Results show that teacher priors heavily influence grade predictions; the strongest results arise when priors are combined with content embeddings (AUC~0.815), while content-only models remain above chance but substantially weaker (AUC~0.626). Adjusting for rater effects sharpens the residual content representation, retaining more informative embedding dimensions and revealing cases where semantic evidence supports understanding as opposed to surface-level differences in how students respond. The contribution presents a practical pipeline that transforms embeddings from mere features into learning analytics for reflection, enabling teachers and researchers to examine where grading practices align (or conflict) with evidence of student reasoning and learning.

</details>


### [15] [Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling](https://arxiv.org/abs/2512.23959)
*Chulun Zhou,Chunkang Zhang,Guoxin Yu,Fandong Meng,Jie Zhou,Wai Lam,Mo Yu*

Main category: cs.CL

TL;DR: 本文提出HGMem，一种基于超图的记忆机制，用于多步检索增强生成（RAG），通过构建动态、高阶关联的知识结构，显著提升模型在全局理解与复杂推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统中的工作记忆模块多为被动存储，仅累积孤立事实，缺乏对高阶事实关联的建模，导致在多步推理中出现碎片化推理和全局理解能力弱的问题。

Method: 引入HGMem机制，将记忆表示为超图结构，其中超边对应不同的记忆单元，从而支持记忆内部高阶交互的逐步形成，连接围绕核心问题的事实与思维，演化为整合性、情境化的知识结构。

Result: 在多个面向全局意义构建的挑战性数据集上，HGMem显著优于强基线系统，有效提升了多步RAG的性能。

Conclusion: HGMem通过将记忆从静态存储转变为动态、表达性强的结构，增强了多步推理中的知识演化与全局理解能力，为复杂任务提供了更强的推理支持。

Abstract: Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.

</details>


### [16] [Efficient Context Scaling with LongCat ZigZag Attention](https://arxiv.org/abs/2512.23966)
*Chen Zhang,Yang Bai,Jiahuan Li,Anchun Gui,Keheng Wang,Feifan Liu,Guanyu Wu,Yuwei Jiang,Defei Bu,Li Wei,Haihang Jing,Hongyin Tang,Xin Chen,Xiangzhou Huang,Fengcun Li,Rongxiang Weng,Yulei Qian,Yifan Lu,Yerui Sun,Jingang Wang,Yuchen Xie,Xunliang Cai*

Main category: cs.CL

TL;DR: 提出了LongCat ZigZag Attention（LoZA），一种稀疏注意力机制，可将全注意力模型高效转换为稀疏版本，在有限算力下显著加速长上下文场景中的预填充和解码过程，并成功应用于LongCat-Flash模型，支持处理高达100万token的长上下文。


<details>
  <summary>Details</summary>
Motivation: 在长上下文场景中，传统全注意力机制计算开销大、效率低，限制了模型在检索增强生成或工具集成推理等任务中的应用。因此，亟需一种能在有限计算资源下有效提升长上下文处理效率的稀疏注意力方案。

Method: 提出LongCat ZigZag Attention（LoZA）稀疏注意力机制，通过特定的ZigZag模式对注意力计算进行稀疏化，并将其集成到现有全注意力模型（如LongCat-Flash）中，在中训阶段进行适配，从而构建能高效处理超长上下文的稀疏注意力模型LongCat-Flash-Exp。

Result: LoZA在预填充密集型和解码密集型的长上下文任务中均实现显著加速；基于LoZA的LongCat-Flash-Exp模型可高效处理高达100万token的输入，支持长期推理与长视野智能体能力。

Conclusion: LoZA是一种高效、实用的稀疏注意力方案，能够在极低计算开销下将全注意力模型转化为适用于超长上下文的稀疏版本，显著提升长序列处理效率，为大规模语言模型在复杂长程任务中的部署提供了可行路径。

Abstract: We introduce LongCat ZigZag Attention (LoZA), which is a sparse attention scheme designed to transform any existing full-attention models into sparse versions with rather limited compute budget. In long-context scenarios, LoZA can achieve significant speed-ups both for prefill-intensive (e.g., retrieval-augmented generation) and decode-intensive (e.g., tool-integrated reasoning) cases. Specifically, by applying LoZA to LongCat-Flash during mid-training, we serve LongCat-Flash-Exp as a long-context foundation model that can swiftly process up to 1 million tokens, enabling efficient long-term reasoning and long-horizon agentic capabilities.

</details>


### [17] [CEC-Zero: Zero-Supervision Character Error Correction with Self-Generated Rewards](https://arxiv.org/abs/2512.23971)
*Zhiming Lin,Kai Zhao,Sophie Zhang,Peilai Yu,Canran Xiao*

Main category: cs.CL

TL;DR: CEC-Zero is a zero-supervision reinforcement learning framework that enables large language models (LLMs) to correct Chinese spelling errors without labeled data, outperforming both supervised methods and fine-tuned LLMs across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing Chinese spelling correction (CSC) approaches—whether based on LLMs or supervised learning—struggle with robustness to unseen error types and depend heavily on expensive annotated data. There is a need for a scalable, annotation-free method that can generalize well to novel errors.

Method: CEC-Zero synthesizes errorful text from clean corpora, then uses a reinforcement learning approach with Proximal Policy Optimization (PPO). It designs a cluster-consensus reward mechanism based on semantic similarity and agreement among candidate corrections, enabling the LLM to learn from its own outputs without external labels.

Result: CEC-Zero achieves 10–13 F₁ points improvement over supervised baselines and 5–8 points over strong LLM fine-tuning methods across nine CSC benchmarks, while offering theoretical guarantees of unbiased reward estimation and policy convergence.

Conclusion: CEC-Zero establishes a label-free, reinforcement learning–based paradigm for Chinese spelling correction that is both robust and scalable, effectively harnessing the power of LLMs in real-world noisy text processing scenarios.

Abstract: Large-scale Chinese spelling correction (CSC) remains critical for real-world text processing, yet existing LLMs and supervised methods lack robustness to novel errors and rely on costly annotations. We introduce CEC-Zero, a zero-supervision reinforcement learning framework that addresses this by enabling LLMs to correct their own mistakes. CEC-Zero synthesizes errorful inputs from clean text, computes cluster-consensus rewards via semantic similarity and candidate agreement, and optimizes the policy with PPO. It outperforms supervised baselines by 10--13 F$_1$ points and strong LLM fine-tunes by 5--8 points across 9 benchmarks, with theoretical guarantees of unbiased rewards and convergence. CEC-Zero establishes a label-free paradigm for robust, scalable CSC, unlocking LLM potential in noisy text pipelines.

</details>


### [18] [Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process](https://arxiv.org/abs/2512.23988)
*Zhenyu Zhang,Shujian Zhang,John Lambert,Wenxuan Zhou,Zhangyang Wang,Mingqing Chen,Andrew Hard,Rajiv Mathews,Lun Wang*

Main category: cs.CL

TL;DR: 提出了一种名为RISE的无监督框架，利用稀疏自编码器（SAE）在激活空间中发现可解释的推理行为向量，从而实现对大语言模型（LLM）推理过程的解耦、可视化与可控干预。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人工定义的概念（如“过度思考”、“反思”）在词元层面监督式地分析LLM的推理行为，难以覆盖全部潜在推理模式，尤其是一些难以在token空间明确定义的行为。

Method: 将思维链（CoT）推理轨迹按句子切分为“步骤”，在步骤级激活上训练稀疏自编码器（SAE），从中提取在激活空间中表征不同推理行为的方向向量（即推理向量）。

Result: 成功识别出如反思、回溯等可解释推理行为，并发现这些行为在解码器列空间中占据可分离区域；通过干预SAE导出的向量可可控地增强或抑制特定行为；此外还能捕捉响应长度等结构属性，并发现人类未标注的新行为（如置信度相关向量）。

Conclusion: 无监督的潜在表示发现方法（如RISE）不仅能有效解释LLM的推理机制，还可用于可控地引导其推理过程，为理解与调控大模型推理提供了新路径。

Abstract: Despite the growing reasoning capabilities of recent large language models (LLMs), their internal mechanisms during the reasoning process remain underexplored. Prior approaches often rely on human-defined concepts (e.g., overthinking, reflection) at the word level to analyze reasoning in a supervised manner. However, such methods are limited, as it is infeasible to capture the full spectrum of potential reasoning behaviors, many of which are difficult to define in token space. In this work, we propose an unsupervised framework (namely, RISE: Reasoning behavior Interpretability via Sparse auto-Encoder) for discovering reasoning vectors, which we define as directions in the activation space that encode distinct reasoning behaviors. By segmenting chain-of-thought traces into sentence-level 'steps' and training sparse auto-encoders (SAEs) on step-level activations, we uncover disentangled features corresponding to interpretable behaviors such as reflection and backtracking. Visualization and clustering analyses show that these behaviors occupy separable regions in the decoder column space. Moreover, targeted interventions on SAE-derived vectors can controllably amplify or suppress specific reasoning behaviors, altering inference trajectories without retraining. Beyond behavior-specific disentanglement, SAEs capture structural properties such as response length, revealing clusters of long versus short reasoning traces. More interestingly, SAEs enable the discovery of novel behaviors beyond human supervision. We demonstrate the ability to control response confidence by identifying confidence-related vectors in the SAE decoder space. These findings underscore the potential of unsupervised latent discovery for both interpreting and controllably steering reasoning in LLMs.

</details>


### [19] [WISE: Web Information Satire and Fakeness Evaluation](https://arxiv.org/abs/2512.24000)
*Gaurab Chhetri,Subasish Das,Tausif Islam Chowdhury*

Main category: cs.CL

TL;DR: 该研究提出WISE框架，评估多个轻量级Transformer模型在区分虚假新闻与讽刺内容任务中的表现，发现MiniLM在准确率上最优，RoBERTa-base在ROC-AUC上最佳，而DistilBERT在效率与性能之间取得良好平衡。


<details>
  <summary>Details</summary>
Motivation: 虚假新闻与讽刺内容在语言特征上高度重叠但意图迥异，导致自动区分二者具有挑战性；现有方法在资源受限场景下的适用性不足，亟需评估轻量级模型在此任务中的有效性。

Method: 构建WISE评估框架，在包含20,000个样本的平衡Fakeddit数据集上，采用分层5折交叉验证，对8个轻量级Transformer模型和2个基线模型进行系统评估，使用包括准确率、F1、ROC-AUC、MCC、Brier分数和预期校准误差在内的多维指标，并通过配对t检验和McNemar检验进行统计显著性分析。

Result: MiniLM取得最高准确率（87.58%），RoBERTa-base获得最高ROC-AUC（95.42%）和高准确率（87.36%），DistilBERT在效率与性能间表现均衡（准确率86.28%，ROC-AUC 93.90%）；统计检验确认模型间性能差异显著。

Conclusion: 轻量级Transformer模型在区分虚假新闻与讽刺内容任务中可媲美甚至超越传统基线模型，适用于资源受限的真实场景部署，为误导信息检测系统的实际应用提供了可行路径。

Abstract: Distinguishing fake or untrue news from satire or humor poses a unique challenge due to their overlapping linguistic features and divergent intent. This study develops WISE (Web Information Satire and Fakeness Evaluation) framework which benchmarks eight lightweight transformer models alongside two baseline models on a balanced dataset of 20,000 samples from Fakeddit, annotated as either fake news or satire. Using stratified 5-fold cross-validation, we evaluate models across comprehensive metrics including accuracy, precision, recall, F1-score, ROC-AUC, PR-AUC, MCC, Brier score, and Expected Calibration Error. Our evaluation reveals that MiniLM, a lightweight model, achieves the highest accuracy (87.58%) among all models, while RoBERTa-base achieves the highest ROC-AUC (95.42%) and strong accuracy (87.36%). DistilBERT offers an excellent efficiency-accuracy trade-off with 86.28\% accuracy and 93.90\% ROC-AUC. Statistical tests confirm significant performance differences between models, with paired t-tests and McNemar tests providing rigorous comparisons. Our findings highlight that lightweight models can match or exceed baseline performance, offering actionable insights for deploying misinformation detection systems in real-world, resource-constrained settings.

</details>


### [20] [iCLP: Large Language Model Reasoning with Implicit Cognition Latent Planning](https://arxiv.org/abs/2512.24014)
*Sijia Chen,Di Niu*

Main category: cs.CL

TL;DR: 本文提出iCLP框架，受人类隐性认知启发，通过学习紧凑的潜在计划（LPs）引导大语言模型（LLMs）在推理过程中进行隐式规划，在数学推理和代码生成任务中显著提升准确率、效率及跨领域泛化能力，同时保持思维链的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖显式文本计划引导LLM进行逐步推理，但LLM易产生幻觉且任务问题多样性高，导致难以生成准确有效的文本计划。受人类隐性认知（即基于经验形成的紧凑、泛化模式进行无意识决策）启发，作者旨在构建一种无需显式表述即可引导推理的机制。

Method: 提出iCLP框架：首先从现有逐步推理轨迹中提炼显式计划；然后利用向量量化自编码器（VQ-VAE）结合码本将这些计划编码为离散的潜在计划（LPs）；最后在潜在计划与对应推理步骤的配对数据上微调LLM，使其学会在推理时进行隐式规划。

Result: 在数学推理和代码生成任务上的实验表明，采用iCLP的LLM能在潜在空间中进行规划、在语言空间中进行推理，显著提升准确率与效率，并展现出强大的跨领域泛化能力。

Conclusion: iCLP通过模拟人类隐性认知机制，使LLM能够以紧凑、可泛化的潜在计划指导推理，在保持思维链可解释性的同时，有效克服显式计划生成中的幻觉与多样性挑战。

Abstract: Large language models (LLMs), when guided by explicit textual plans, can perform reliable step-by-step reasoning during problem-solving. However, generating accurate and effective textual plans remains challenging due to LLM hallucinations and the high diversity of task-specific questions. To address this, we draw inspiration from human Implicit Cognition (IC), the subconscious process by which decisions are guided by compact, generalized patterns learned from past experiences without requiring explicit verbalization. We propose iCLP, a novel framework that enables LLMs to adaptively generate latent plans (LPs), which are compact encodings of effective reasoning instructions. iCLP first distills explicit plans from existing step-by-step reasoning trajectories. It then learns discrete representations of these plans via a vector-quantized autoencoder coupled with a codebook. Finally, by fine-tuning LLMs on paired latent plans and corresponding reasoning steps, the models learn to perform implicit planning during reasoning. Experimental results on mathematical reasoning and code generation tasks demonstrate that, with iCLP, LLMs can plan in latent space while reasoning in language space. This approach yields significant improvements in both accuracy and efficiency and, crucially, demonstrates strong cross-domain generalization while preserving the interpretability of chain-of-thought reasoning.

</details>


### [21] [Beyond Hallucinations: A Composite Score for Measuring Reliability in Open-Source Large Language Models](https://arxiv.org/abs/2512.24058)
*Rohit Kumar Salla,Manoj Saravanan,Shrikar Reddy Kota*

Main category: cs.CL

TL;DR: 本文提出复合可靠性评分（CRS），通过整合校准性、鲁棒性和不确定性量化，为大语言模型提供统一、可解释的可靠性评估指标。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗、法律和金融等关键决策领域应用日益广泛，但其可靠性仍存疑，表现为过度自信错误、输入分布偏移下性能下降及缺乏明确的不确定性估计；现有评估方法碎片化，仅关注单一维度。

Method: 提出Composite Reliability Score (CRS)框架，将校准性、鲁棒性和不确定性量化融合为单一可解释指标，并在五个问答数据集上对十个主流开源大语言模型进行实验，评估其在基线、扰动和校准方法下的表现。

Result: CRS能提供稳定的模型排序，揭示单一指标无法发现的隐藏失效模式，并表明最可靠的系统需在准确性、鲁棒性和校准不确定性之间取得平衡。

Conclusion: CRS作为一个统一的可靠性评估框架，有效弥补了现有评估方法的不足，为大语言模型在高风险领域的部署提供了更全面的可靠性衡量标准。

Abstract: Large Language Models (LLMs) like LLaMA, Mistral, and Gemma are increasingly used in decision-critical domains such as healthcare, law, and finance, yet their reliability remains uncertain. They often make overconfident errors, degrade under input shifts, and lack clear uncertainty estimates. Existing evaluations are fragmented, addressing only isolated aspects. We introduce the Composite Reliability Score (CRS), a unified framework that integrates calibration, robustness, and uncertainty quantification into a single interpretable metric. Through experiments on ten leading open-source LLMs across five QA datasets, we assess performance under baselines, perturbations, and calibration methods. CRS delivers stable model rankings, uncovers hidden failure modes missed by single metrics, and highlights that the most dependable systems balance accuracy, robustness, and calibrated uncertainty.

</details>


### [22] [HY-MT1.5 Technical Report](https://arxiv.org/abs/2512.24092)
*Mao Zheng,Zheng Li,Tao Chen,Mingyang Song,Di Wang*

Main category: cs.CL

TL;DR: 本文提出了两个新型机器翻译模型 HY-MT1.5-1.8B 和 HY-MT1.5-7B，采用多阶段整体训练框架，在多个中英及小语种翻译任务上显著优于现有开源和商用系统，并接近或部分超越顶级闭源模型（如 Gemini-3.0-Pro）的性能。


<details>
  <summary>Details</summary>
Motivation: 当前主流机器翻译系统在参数效率、多语言覆盖（尤其是中文与少数民族语言）以及对术语干预、上下文感知等高级约束的支持方面仍有不足。作者旨在开发兼具高性能、高效率和强可控性的新一代翻译模型。

Method: 提出一种整体式多阶段训练框架，依次整合通用与翻译导向的预训练、有监督微调、策略蒸馏（on-policy distillation）和强化学习，用于训练 HY-MT1.5 系列模型（1.8B 与 7B 参数规模）。

Result: HY-MT1.5-1.8B 在标准中英/英外任务上全面超越更大规模的开源模型（如 Tower-Plus-72B、Qwen3-32B）和主流商用 API（如微软、豆包翻译），达到 Gemini-3.0-Pro 约 90% 的性能；HY-MT1.5-7B 在 Flores-200 上达其 95%，并在 WMT25 与中文-少数民族语言任务上超越 Gemini-3.0-Pro。两模型均支持术语干预、上下文感知和格式保留等高级功能。

Conclusion: HY-MT1.5 系列模型通过高效的训练架构，在各自参数规模下实现了顶尖的翻译性能与鲁棒性，为通用与专业场景提供了极具竞争力的解决方案，并在部分指标上超越当前最强闭源模型。

Abstract: In this report, we introduce our latest translation models, HY-MT1.5-1.8B and HY-MT1.5-7B, a new family of machine translation models developed through a holistic training framework tailored for high-performance translation. Our methodology orchestrates a multi-stage pipeline that integrates general and MT-oriented pre-training, supervised fine-tuning, on-policy distillation, and reinforcement learning. HY-MT1.5-1.8B, the 1.8B-parameter model demonstrates remarkable parameter efficiency, comprehensively outperforming significantly larger open-source baselines (e.g., Tower-Plus-72B, Qwen3-32B) and mainstream commercial APIs (e.g., Microsoft Translator, Doubao Translator) in standard Chinese-foreign and English-foreign tasks. It achieves approximately 90% of the performance of ultra-large proprietary models such as Gemini-3.0-Pro, while marginally trailing Gemini-3.0-Pro on WMT25 and Mandarin-minority language benchmarks, it maintains a substantial lead over other competing models. Furthermore, HY-MT1.5-7B establishes a new state-of-the-art for its size class, achieving 95% of Gemini-3.0-Pro's performance on Flores-200 and surpassing it on the challenging WMT25 and Mandarin-minority language test sets. Beyond standard translation, the HY-MT1.5 series supports advanced constraints, including terminology intervention, context-aware translation, and format preservation. Extensive empirical evaluations confirm that both models offer highly competitive, robust solutions for general and specialized translation tasks within their respective parameter scales.

</details>


### [23] [Training a Huggingface Model on AWS Sagemaker (Without Tears)](https://arxiv.org/abs/2512.24098)
*Liling Tan*

Main category: cs.CL

TL;DR: 本文旨在通过整合关键信息，帮助研究人员从零开始在 AWS SageMaker 上训练首个 Hugging Face 模型，降低云平台使用门槛。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLM）训练所需的本地计算资源日益稀缺，许多研究人员转向 AWS SageMaker 等云服务。然而，云平台陡峭的学习曲线和现有文档中的知识缺口阻碍了习惯于本地环境的研究人员顺利上手。

Method: 作者通过整理并集中展示在 AWS SageMaker 上从零开始训练 Hugging Face 模型所需的关键步骤与信息，提供一个结构化的入门指南。

Result: 该演示论文成功为研究人员提供了清晰、集中的操作路径，填补了现有文档的空白，降低了首次使用云平台训练模型的障碍。

Conclusion: 通过集中必要信息，本文有效促进了云平台在科研社区中的普及，助力更多研究人员顺利开展基于云的大模型训练工作。

Abstract: The development of Large Language Models (LLMs) has primarily been driven by resource-rich research groups and industry partners. Due to the lack of on-premise computing resources required for increasingly complex models, many researchers are turning to cloud services like AWS SageMaker to train Hugging Face models. However, the steep learning curve of cloud platforms often presents a barrier for researchers accustomed to local environments. Existing documentation frequently leaves knowledge gaps, forcing users to seek fragmented information across the web. This demo paper aims to democratize cloud adoption by centralizing the essential information required for researchers to successfully train their first Hugging Face model on AWS SageMaker from scratch.

</details>


### [24] [Activation Steering for Masked Diffusion Language Models](https://arxiv.org/abs/2512.24143)
*Adi Shnaidman,Erin Feiglin,Osher Yaari,Efrat Mentel,Amit Levi,Raz Lapid*

Main category: cs.CL

TL;DR: 本文提出了一种针对掩码扩散语言模型（MDLMs）的激活引导框架，通过对比样本在单次前向传播中计算逐层引导向量，并在每个反向扩散步骤中应用这些方向，从而实现高效推理时控制。


<details>
  <summary>Details</summary>
Motivation: 尽管掩码扩散语言模型（MDLMs）因并行解码和与自回归大语言模型相当的性能而受到关注，但其在推理阶段缺乏有效的控制与引导机制。

Method: 提出一种激活引导框架：利用对比样本，在不模拟去噪轨迹的前提下，通过单次前向传播计算各Transformer层的引导向量，并在每次反向扩散步骤中施加这些引导方向。

Result: 在LLaDA-8B-Instruct上的实验表明，该方法能可靠地调控生成文本的高层属性；消融实验进一步分析了在不同Transformer子模块和token范围（提示词 vs. 回答）中引导的效果。

Conclusion: 所提出的激活引导框架为MDLM提供了一种高效且有效的推理时控制手段，显著增强了对生成内容高层语义属性的可控性。

Abstract: Masked diffusion language models (MDLMs) generate text through an iterative denoising process. They have recently gained attention due to mask-parallel decoding and competitive performance with autoregressive large language models. However, effective mechanisms for inference-time control and steering in MDLMs remain largely unexplored. We present an activation-steering framework for MDLMs that computes layer-wise steering vectors from a single forward pass using contrastive examples, without simulating the denoising trajectory. These directions are applied at every reverse-diffusion step, yielding an efficient inference-time control mechanism. Experiments on LLaDA-8B-Instruct demonstrate reliable modulation of high-level attributes, with ablations examining the effects of steering across transformer sub-modules and token scope (prompt vs.\ response).

</details>


### [25] [Large Emotional World Model](https://arxiv.org/abs/2512.24149)
*Changhao Song,Yazhou Zhang,Hui Gao,Chang Yang,Peng Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种融合情绪建模的大世界模型（LEWM），通过构建包含情绪-原因-行为因果链的EWH数据集，使模型不仅能预测未来状态，还能推理情绪驱动的社会行为，在保持基础任务性能的同时提升了对情绪相关行为的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽能捕捉物理世界的规律，但缺乏对情绪这一关键世界知识成分的系统建模；而情绪在人类决策和世界理解中起重要作用，忽略情绪信息会损害推理能力。

Method: 受心理理论启发，作者构建了Emotion-Why-How（EWH）数据集，将情绪嵌入因果关系中，并在此基础上训练Large Emotional World Model（LEWM），该模型联合建模情绪状态、视觉观测与动作，以同时预测未来世界状态和情绪演变。

Result: 实验表明，LEWM在情绪驱动的社会行为预测上显著优于现有模型，同时在通用世界建模任务上保持相当的性能。

Conclusion: 将情绪显式引入世界模型可有效提升对人类社会行为的理解与预测能力，情绪是构建更全面世界模型不可或缺的组成部分。

Abstract: World Models serve as tools for understanding the current state of the world and predicting its future dynamics, with broad application potential across numerous fields. As a key component of world knowledge, emotion significantly influences human decision-making. While existing Large Language Models (LLMs) have shown preliminary capability in capturing world knowledge, they primarily focus on modeling physical-world regularities and lack systematic exploration of emotional factors. In this paper, we first demonstrate the importance of emotion in understanding the world by showing that removing emotionally relevant information degrades reasoning performance. Inspired by theory of mind, we further propose a Large Emotional World Model (LEWM). Specifically, we construct the Emotion-Why-How (EWH) dataset, which integrates emotion into causal relationships and enables reasoning about why actions occur and how emotions drive future world states. Based on this dataset, LEWM explicitly models emotional states alongside visual observations and actions, allowing the world model to predict both future states and emotional transitions. Experimental results show that LEWM more accurately predicts emotion-driven social behaviors while maintaining comparable performance to general world models on basic tasks.

</details>


### [26] [Training Report of TeleChat3-MoE](https://arxiv.org/abs/2512.24157)
*Xinzhang Liu,Chao Wang,Zhihao Yang,Zhuo Jiang,Xuncheng Zhao,Haoran Wang,Lei Li,Dongdong He,Luobin Liu,Kaizhe Yuan,Han Gao,Zihan Wang,Yitong Yao,Sishi Xiong,Wenmin Deng,Haowei He,Kaidong Yu,Yu Zhao,Ruiyu Fang,Yuhao Jiang,Yingyan Li,Xiaohui Hu,Xi Yu,Jingqi Li,Yanwei Liu,Qingli Li,Xinyu Shi,Junhao Niu,Chengnuo Huang,Yao Xiao,Ruiwen Wang,Fengkai Li,Luwen Pu,Kaipeng Jia,Fubei Yao,Yuyao Huang,Xuewei He,Zhuoru Jiang,Ruiting Song,Rui Xue,Qiyi Xie,Jie Zhang,Zilu Huang,Zhaoxi Zhang,Zhilong Lu,Yanhan Zhang,Yin Zhang,Yanlei Xue,Zhu Yuan,Teng Su,Xin Jiang,Shuangyong Song,Yongxiang Li,Xuelong Li*

Main category: cs.CL

TL;DR: TeleChat3-MoE 是基于 Ascend NPU 集群训练的超大规模 MoE 架构语言模型（105B–1T+ 参数），该技术报告重点介绍了支撑其高效、可靠扩展至前沿规模的底层训练基础设施，包括数值精度验证、多维并行优化、通信调度与算子融合等关键技术，实现了千卡集群上的近线性扩展和显著吞吐提升。


<details>
  <summary>Details</summary>
Motivation: 为支持参数量高达万亿级别的 TeleChat3-MoE 模型在 Ascend NPU 集群上进行端到端训练，亟需构建一套高可靠性、高效率的大规模训练基础设施，以解决硬件异构性、分布式策略一致性、通信瓶颈及计算效率等关键挑战。

Method: 提出了一套系统化的训练基础设施方法，包括：(1) 算子级与端到端的数值精度验证机制；(2) 多项性能优化技术，如交错流水线调度、注意力感知的长序列数据调度、专家并行的分层重叠通信、基于 DVM 的算子融合；(3) 结合解析估计与整数线性规划的多维并行配置优化框架；(4) 针对主机与设备瓶颈的集群级优化策略。

Result: 所提出的基础设施在包含数千个设备的集群上实现了显著的吞吐量提升和接近线性的扩展效率，为在国产硬件生态上开发超大规模语言模型提供了坚实基础。

Conclusion: 通过系统性地设计和优化训练基础设施，成功支撑了万亿参数级 MoE 模型在 Ascend NPU 集群上的高效、稳定训练，验证了该软硬件协同方案在大规模语言模型训练中的可行性与先进性。

Abstract: TeleChat3-MoE is the latest series of TeleChat large language models, featuring a Mixture-of-Experts (MoE) architecture with parameter counts ranging from 105 billion to over one trillion,trained end-to-end on Ascend NPU cluster. This technical report mainly presents the underlying training infrastructure that enables reliable and efficient scaling to frontier model sizes. We detail systematic methodologies for operator-level and end-to-end numerical accuracy verification, ensuring consistency across hardware platforms and distributed parallelism strategies. Furthermore, we introduce a suite of performance optimizations, including interleaved pipeline scheduling, attention-aware data scheduling for long-sequence training,hierarchical and overlapped communication for expert parallelism, and DVM-based operator fusion. A systematic parallelization framework, leveraging analytical estimation and integer linear programming, is also proposed to optimize multi-dimensional parallelism configurations. Additionally, we present methodological approaches to cluster-level optimizations, addressing host- and device-bound bottlenecks during large-scale training tasks. These infrastructure advancements yield significant throughput improvements and near-linear scaling on clusters comprising thousands of devices, providing a robust foundation for large-scale language model development on hardware ecosystems.

</details>


### [27] [MedKGI: Iterative Differential Diagnosis with Medical Knowledge Graphs and Information-Guided Inquiring](https://arxiv.org/abs/2512.24181)
*Qipeng Wang,Rui Sheng,Yafei Li,Huamin Qu,Yushi Sun,Min Zhu*

Main category: cs.CL

TL;DR: 本文提出MedKGI框架，通过整合医学知识图谱、基于信息增益的问题选择机制和结构化状态跟踪，解决大语言模型在临床诊断中存在幻觉、提问冗余及多轮对话不一致的问题，在诊断准确性和问诊效率上均优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在临床诊断中难以模拟真实临床场景中迭代式、假设驱动的推理过程，存在医学内容幻觉、提问效率低下以及多轮对话中逻辑不一致三大关键问题。

Method: 提出MedKGI诊断框架：1）利用医学知识图谱约束推理过程，确保基于已验证的医学本体；2）基于信息增益选择最具判别性的提问以提升诊断效率；3）采用OSCE格式的结构化状态机制，实现跨轮次的一致性证据追踪。

Result: 在临床基准测试中，MedKGI相比强基线大语言模型，在诊断准确率保持领先的同时，平均提升问诊效率30%。

Conclusion: MedKGI有效缓解了大语言模型在临床诊断中的核心缺陷，通过知识引导、高效提问与状态一致性管理，显著提升了模型在真实医疗场景中的实用性与可靠性。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated significant promise in clinical diagnosis. However, current models struggle to emulate the iterative, diagnostic hypothesis-driven reasoning of real clinical scenarios. Specifically, current LLMs suffer from three critical limitations: (1) generating hallucinated medical content due to weak grounding in verified knowledge, (2) asking redundant or inefficient questions rather than discriminative ones that hinder diagnostic progress, and (3) losing coherence over multi-turn dialogues, leading to contradictory or inconsistent conclusions. To address these challenges, we propose MedKGI, a diagnostic framework grounded in clinical practices. MedKGI integrates a medical knowledge graph (KG) to constrain reasoning to validated medical ontologies, selects questions based on information gain to maximize diagnostic efficiency, and adopts an OSCE-format structured state to maintain consistent evidence tracking across turns. Experiments on clinical benchmarks show that MedKGI outperforms strong LLM baselines in both diagnostic accuracy and inquiry efficiency, improving dialogue efficiency by 30% on average while maintaining state-of-the-art accuracy.

</details>


### [28] [LAILA: A Large Trait-Based Dataset for Arabic Automated Essay Scoring](https://arxiv.org/abs/2512.24235)
*May Bashendy,Walid Massoud,Sohaila Eltanbouly,Salam Albatarni,Marwan Sayed,Abrar Abir,Houda Bouamor,Tamer Elsayed*

Main category: cs.CL

TL;DR: 本文提出了LAILA，这是目前规模最大的公开阿拉伯语自动作文评分（AES）数据集，包含7,859篇带有整体和七项维度（相关性、组织、词汇、风格、展开、机械性和语法）评分的作文，并提供了基于先进阿拉伯语与英语模型的基准结果。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏公开可用的数据集，阿拉伯语自动作文评分（AES）研究仍较为有限。为填补这一空白，作者构建并发布了一个大规模高质量的阿拉伯语AES数据集。

Method: 作者设计并收集了包含7,859篇阿拉伯语作文的数据集LAILA，每篇作文均标注了整体评分及七个特定维度的分数；随后在prompt-specific和cross-prompt两种设置下，使用当前最先进的阿拉伯语和英语模型进行基准测试。

Result: 实验提供了在不同设置下的基准性能结果，验证了LAILA数据集对训练和评估阿拉伯语AES系统的有效性。

Conclusion: LAILA是目前最大的公开阿拉伯语AES数据集，显著推动了该语言在自动作文评分领域的研究，为开发鲁棒的评分系统提供了关键资源。

Abstract: Automated Essay Scoring (AES) has gained increasing attention in recent years, yet research on Arabic AES remains limited due to the lack of publicly available datasets. To address this, we introduce LAILA, the largest publicly available Arabic AES dataset to date, comprising 7,859 essays annotated with holistic and trait-specific scores on seven dimensions: relevance, organization, vocabulary, style, development, mechanics, and grammar. We detail the dataset design, collection, and annotations, and provide benchmark results using state-of-the-art Arabic and English models in prompt-specific and cross-prompt settings. LAILA fills a critical need in Arabic AES research, supporting the development of robust scoring systems.

</details>


### [29] [Tracing the Flow of Knowledge From Science to Technology Using Deep Learning](https://arxiv.org/abs/2512.24259)
*Michael E. Rose,Mainak Ghosh,Sebastian Erhardt,Cheng Li,Erik Buunk,Dietmar Harhoff*

Main category: cs.CL

TL;DR: 本文提出并评估了一种适用于专利与科学论文的语义相似性模型 Pat-SPECTER，在预测可信的专利-论文引用关系任务中表现最优，并在两个现实场景中验证其有效性，同时利用该模型检验了美国专利因“坦诚义务”而引用语义相似度较低论文的假设。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在同时处理专利和科学出版物时存在局限，缺乏专门针对专利-论文跨文献类型语义相似性建模的有效方法；此外，美国专利引用行为可能受法律义务（如坦诚义务）影响，值得通过高质量语义模型进行实证检验。

Method: 在 SPECTER2 模型基础上，使用专利数据进行微调，构建 Pat-SPECTER 模型；通过“赛马式”评估，将该模型与其他七种语言相似性模型在预测可信专利-论文引用任务上进行对比；并在两个真实应用场景（区分与预测专利-论文对）中测试其性能；最后利用该模型分析不同司法管辖区（特别是美国）专利所引论文的语义相似度差异。

Result: Pat-SPECTER 在预测专利-论文引用任务中显著优于其他基线模型；在实际应用中能有效区分和预测专利-论文配对；实证结果支持美国专利引用的论文语义相似度低于其他主要司法管辖区，这可能与美国的“坦诚义务”制度有关。

Conclusion: Pat-SPECTER 是一种高效且实用的跨文献类型语义相似性模型，适用于专利与科学论文的联合分析；研究结果揭示了法律制度对专利引用行为的潜在影响；该模型已向学术界和业界开放，有望促进相关领域的进一步研究与应用。

Abstract: We develop a language similarity model suitable for working with patents and scientific publications at the same time. In a horse race-style evaluation, we subject eight language (similarity) models to predict credible Patent-Paper Citations. We find that our Pat-SPECTER model performs best, which is the SPECTER2 model fine-tuned on patents. In two real-world scenarios (separating patent-paper-pairs and predicting patent-paper-pairs) we demonstrate the capabilities of the Pat-SPECTER. We finally test the hypothesis that US patents cite papers that are semantically less similar than in other large jurisdictions, which we posit is because of the duty of candor. The model is open for the academic community and practitioners alike.

</details>


### [30] [Joint Selection for Large-Scale Pre-Training Data via Policy Gradient-based Mask Learning](https://arxiv.org/abs/2512.24265)
*Ziqing Fan,Yuqiao Xian,Yan Sun,Li Shen*

Main category: cs.CL

TL;DR: DATAMASK is a novel, efficient joint learning framework for large-scale pre-training data selection that simultaneously optimizes quality and diversity metrics by formulating data selection as a mask learning problem, reducing selection time by 98.9% and yielding significant model performance gains on subsets of trillion-token datasets.


<details>
  <summary>Details</summary>
Motivation: Existing data selection methods for pre-training LLMs typically consider either quality or diversity metrics in isolation due to computational constraints at trillion-token scale. Relying solely on quality metrics leads to diminishing returns during long-term training, while diversity-focused selection discards too many high-quality samples—both limiting model capabilities.

Method: DATAMASK frames data selection as a mask learning problem. It iteratively samples data masks, computes policy gradients based on joint objectives (e.g., quality and diversity), and updates mask sampling logits via policy gradient optimization. The framework includes acceleration techniques that drastically reduce computational overhead, enabling joint metric optimization over trillion-token corpora like FineWeb.

Result: Applying DATAMASK to the 15 trillion-token FineWeb dataset yields FineWeb-Mask—a 10% subset—that improves performance by 3.2% on a 1.5B dense model and 1.9% on a 7B MoE model across 12 diverse evaluation tasks. Selection time is reduced by 98.9% compared to greedy baselines.

Conclusion: Jointly optimizing quality and diversity in pre-training data selection is essential for maximizing LLM performance, and DATAMASK provides an efficient, scalable solution to achieve this at trillion-token scale, significantly outperforming single-metric selection strategies.

Abstract: A fine-grained data recipe is crucial for pre-training large language models, as it can significantly enhance training efficiency and model performance. One important ingredient in the recipe is to select samples based on scores produced by defined rules, LLM judgment, or statistical information in embeddings, which can be roughly categorized into quality and diversity metrics. Due to the high computational cost when applied to trillion-scale token pre-training datasets such as FineWeb and DCLM, these two or more types of metrics are rarely considered jointly in a single selection process. However, in our empirical study, selecting samples based on quality metrics exhibit severe diminishing returns during long-term pre-training, while selecting on diversity metrics removes too many valuable high-quality samples, both of which limit pre-trained LLMs' capabilities. Therefore, we introduce DATAMASK, a novel and efficient joint learning framework designed for large-scale pre-training data selection that can simultaneously optimize multiple types of metrics in a unified process, with this study focusing specifically on quality and diversity metrics. DATAMASK approaches the selection process as a mask learning problem, involving iterative sampling of data masks, computation of policy gradients based on predefined objectives with sampled masks, and updating of mask sampling logits. Through policy gradient-based optimization and various acceleration enhancements, it significantly reduces selection time by 98.9% compared to greedy algorithm, enabling our study to explore joint learning within trillion-scale tokens. With DATAMASK, we select a subset of about 10% from the 15 trillion-token FineWeb dataset, termed FineWeb-Mask. Evaluated across 12 diverse tasks, we achieves significant improvements of 3.2% on a 1.5B dense model and 1.9% on a 7B MoE model.

</details>


### [31] [Automated Analysis of Sustainability Reports: Using Large Language Models for the Extraction and Prediction of EU Taxonomy-Compliant KPIs](https://arxiv.org/abs/2512.24289)
*Jonathan Schmoll,Adam Jatowt*

Main category: cs.CL

TL;DR: 本文提出首个面向欧盟分类法合规任务的结构化数据集，并系统评估大语言模型（LLMs）在定性识别经济活动与定量预测关键绩效指标（KPIs）两类核心任务上的表现，发现LLMs在定性任务中表现尚可但在定量任务中全面失败，且简洁元数据优于完整非结构化报告；研究认为LLMs尚不能实现全自动合规，但可作为专家辅助工具。


<details>
  <summary>Details</summary>
Motivation: 欧盟分类法合规流程高度依赖人工且资源密集，而当前缺乏公开基准数据集阻碍了利用大语言模型（LLMs）实现自动化的研究。

Method: 构建包含190份企业报告的结构化数据集，涵盖真实经济活动标签和定量KPI；在此基础上对LLMs在定性（识别经济活动）与定量（预测KPI）任务中的性能进行系统评估，采用多步智能体框架提升定性任务精度，并对比使用完整报告与简洁元数据的效果。

Result: LLMs在定性任务中表现中等，多步智能体框架略微提升精度；在零样本设定下，LLMs在定量KPI预测任务中全面失败；简洁元数据的表现优于完整非结构化报告；模型置信度得分校准不佳。

Conclusion: 尽管LLMs尚不足以实现欧盟分类法合规的全自动处理，但可作为人类专家的有效辅助工具；所发布数据集为后续研究提供了公开基准。

Abstract: The manual, resource-intensive process of complying with the EU Taxonomy presents a significant challenge for companies. While Large Language Models (LLMs) offer a path to automation, research is hindered by a lack of public benchmark datasets. To address this gap, we introduce a novel, structured dataset from 190 corporate reports, containing ground-truth economic activities and quantitative Key Performance Indicators (KPIs). We use this dataset to conduct the first systematic evaluation of LLMs on the core compliance workflow. Our results reveal a clear performance gap between qualitative and quantitative tasks. LLMs show moderate success in the qualitative task of identifying economic activities, with a multi-step agentic framework modestly enhancing precision. Conversely, the models comprehensively fail at the quantitative task of predicting financial KPIs in a zero-shot setting. We also discover a paradox, where concise metadata often yields superior performance to full, unstructured reports, and find that model confidence scores are poorly calibrated. We conclude that while LLMs are not ready for full automation, they can serve as powerful assistive tools for human experts. Our dataset provides a public benchmark for future research.

</details>


### [32] [Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking](https://arxiv.org/abs/2512.24297)
*Meiqi Chen,Fandong Meng,Jie Zhou*

Main category: cs.CL

TL;DR: 本文提出FIGR模型，通过端到端强化学习将主动视觉思维融入多轮推理过程，在数学推理任务中显著优于纯文本推理基线。


<details>
  <summary>Details</summary>
Motivation: 纯文本推理难以有效表达复杂问题中的全局结构约束（如空间、几何和结构性关系），限制了模型在复杂推理任务中的表现。

Method: 提出FIGR框架，利用端到端强化学习在多轮推理过程中动态构建视觉表征，外化中间结构假设，并自适应调控视觉推理的调用时机与方式。

Result: 在AIME 2025和BeyondAIME等高难度数学推理基准上，FIGR分别比基线模型提升13.12%和11.00%，显著优于纯文本链式推理方法。

Conclusion: 引入图示引导的多模态推理能有效增强复杂推理过程的稳定性与可靠性，尤其在处理隐含结构关系的问题中具有显著优势。

Abstract: Complex reasoning problems often involve implicit spatial, geometric, and structural relationships that are not explicitly encoded in text. While recent reasoning models have achieved strong performance across many domains, purely text-based reasoning struggles to represent global structural constraints in complex settings. In this paper, we introduce FIGR, which integrates active visual thinking into multi-turn reasoning via end-to-end reinforcement learning. FIGR externalizes intermediate structural hypotheses by constructing visual representations during problem solving. By adaptively regulating when and how visual reasoning should be invoked, FIGR enables more stable and coherent reasoning over global structural properties that are difficult to capture from text alone. Experiments on challenging mathematical reasoning benchmarks demonstrate that FIGR outperforms strong text-only chain-of-thought baselines. In particular, FIGR improves the base model by 13.12% on AIME 2025 and 11.00% on BeyondAIME, highlighting the effectiveness of figure-guided multimodal reasoning in enhancing the stability and reliability of complex reasoning.

</details>


### [33] [QianfanHuijin Technical Report: A Novel Multi-Stage Training Paradigm for Finance Industrial LLMs](https://arxiv.org/abs/2512.24314)
*Shupeng Li,Weipeng Lu,Linyun Liu,Chen Lin,Shaofei Li,Zhendong Tan,Hanjun Zhong,Yucheng Zeng,Chenghao Zhu,Mengyue Liu,Daxiang Dong,Jianmin Wu,Yunting Xiao,Annan Li,Danyu Liu,Jingnan Zhang,Licen Liu,Dawei Yin,Dou Shen*

Main category: cs.CL

TL;DR: 本文提出了面向金融领域的领域大语言模型QianfanHuijin，并设计了一种可泛化的多阶段训练范式，通过持续预训练与细粒度后训练（包括金融SFT、推理强化学习、智能体强化学习及通用强化学习）显著提升了模型在金融知识、推理与智能体能力方面的表现。


<details>
  <summary>Details</summary>
Motivation: 随着金融服务复杂性的提升，仅具备领域知识的金融大模型已难以满足工业应用需求，亟需同时具备金融推理能力和智能体能力的增强型模型。

Method: 采用多阶段训练范式：首先在金融语料上进行持续预训练（CPT）以夯实知识基础，随后依次进行金融指令微调（Financial SFT）、金融推理强化学习（Finance Reasoning RL）、金融智能体强化学习（Finance Agentic RL），最后进行贴合真实业务场景的通用强化学习（General RL）。

Result: QianfanHuijin在多个权威金融基准测试中表现优异；消融实验表明，推理RL和智能体RL阶段分别显著提升了模型的推理能力和智能体能力。

Conclusion: 所提出的细粒度、渐进式后训练方法有效提升了金融大模型的综合能力，有望成为工业级领域大模型增强的主流范式。

Abstract: Domain-specific enhancement of Large Language Models (LLMs) within the financial context has long been a focal point of industrial application. While previous models such as BloombergGPT and Baichuan-Finance primarily focused on knowledge enhancement, the deepening complexity of financial services has driven a growing demand for models that possess not only domain knowledge but also robust financial reasoning and agentic capabilities. In this paper, we present QianfanHuijin, a financial domain LLM, and propose a generalizable multi-stage training paradigm for industrial model enhancement.
  Our approach begins with Continual Pre-training (CPT) on financial corpora to consolidate the knowledge base. This is followed by a fine-grained Post-training pipeline designed with increasing specificity: starting with Financial SFT, progressing to Finance Reasoning RL and Finance Agentic RL, and culminating in General RL aligned with real-world business scenarios. Empirical results demonstrate that QianfanHuijin achieves superior performance across various authoritative financial benchmarks. Furthermore, ablation studies confirm that the targeted Reasoning RL and Agentic RL stages yield significant gains in their respective capabilities. These findings validate our motivation and suggest that this fine-grained, progressive post-training methodology is poised to become a mainstream paradigm for various industrial-enhanced LLMs.

</details>


### [34] [World model inspired sarcasm reasoning with large language model agents](https://arxiv.org/abs/2512.24329)
*Keito Inoshita,Shinnosuke Mizuno*

Main category: cs.CL

TL;DR: 本文提出了一种基于世界模型启发的讽刺理解方法WM-SAR，通过将字面意义、上下文、规范期望和意图分解为多个LLM智能体，并结合不一致性得分与意图得分，利用逻辑回归进行可解释的讽刺检测，在多个基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有讽刺理解方法多依赖单一黑箱模型，难以结构化解释讽刺背后的认知因素；同时缺乏显式建模语义评价与规范期望或意图之间不匹配的框架。

Method: 将讽刺理解重构为世界模型启发的推理过程，提出WM-SAR框架：使用多个专门的LLM智能体分别建模字面意义、上下文、规范期望和意图；计算字面评价与规范期望之间的确定性不一致性得分，并结合意图得分，通过轻量级逻辑回归模型预测讽刺概率。

Result: 在多个代表性讽刺检测基准上，WM-SAR一致优于现有的深度学习和LLM方法；消融实验与案例分析表明，语义不一致性与意图推理的结合对有效检测讽刺至关重要。

Conclusion: WM-SAR通过融合LLM的推理能力与可解释的数值决策结构，在提升讽刺检测性能的同时增强了模型的可解释性，验证了显式建模认知成分的有效性。

Abstract: Sarcasm understanding is a challenging problem in natural language processing, as it requires capturing the discrepancy between the surface meaning of an utterance and the speaker's intentions as well as the surrounding social context. Although recent advances in deep learning and Large Language Models (LLMs) have substantially improved performance, most existing approaches still rely on black-box predictions of a single model, making it difficult to structurally explain the cognitive factors underlying sarcasm. Moreover, while sarcasm often emerges as a mismatch between semantic evaluation and normative expectations or intentions, frameworks that explicitly decompose and model these components remain limited. In this work, we reformulate sarcasm understanding as a world model inspired reasoning process and propose World Model inspired SArcasm Reasoning (WM-SAR), which decomposes literal meaning, context, normative expectation, and intention into specialized LLM-based agents. The discrepancy between literal evaluation and normative expectation is explicitly quantified as a deterministic inconsistency score, and together with an intention score, these signals are integrated by a lightweight Logistic Regression model to infer the final sarcasm probability. This design leverages the reasoning capability of LLMs while maintaining an interpretable numerical decision structure. Experiments on representative sarcasm detection benchmarks show that WM-SAR consistently outperforms existing deep learning and LLM-based methods. Ablation studies and case analyses further demonstrate that integrating semantic inconsistency and intention reasoning is essential for effective sarcasm detection, achieving both strong performance and high interpretability.

</details>


### [35] [Skim-Aware Contrastive Learning for Efficient Document Representation](https://arxiv.org/abs/2512.24373)
*Waheed Ahmed Abro,Zied Bouraoui*

Main category: cs.CL

TL;DR: 提出一种基于自监督对比学习的长文档表示方法，通过模拟人类略读策略，在法律和生物医学文本上显著提升准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在处理长文档（如法律、医学文本）时存在资源消耗大、上下文建模不充分或缺乏可解释性的问题；受人类略读策略启发，旨在构建更高效且语义丰富的长文档表示方法。

Method: 提出一种自监督对比学习框架：随机掩码文档中某一部分，并利用基于自然语言推理（NLI）的对比目标，使其与相关部分对齐、与无关部分分离，从而模拟人类整合信息的方式。

Result: 在法律和生物医学文本上的实验表明，该方法在准确性和计算效率方面均取得显著提升。

Conclusion: 所提方法有效结合了人类阅读策略与对比学习机制，为长文档表示提供了一种高效且语义更强的解决方案。

Abstract: Although transformer-based models have shown strong performance in word- and sentence-level tasks, effectively representing long documents, especially in fields like law and medicine, remains difficult. Sparse attention mechanisms can handle longer inputs, but are resource-intensive and often fail to capture full-document context. Hierarchical transformer models offer better efficiency but do not clearly explain how they relate different sections of a document. In contrast, humans often skim texts, focusing on important sections to understand the overall message. Drawing from this human strategy, we introduce a new self-supervised contrastive learning framework that enhances long document representation. Our method randomly masks a section of the document and uses a natural language inference (NLI)-based contrastive objective to align it with relevant parts while distancing it from unrelated ones. This mimics how humans synthesize information, resulting in representations that are both richer and more computationally efficient. Experiments on legal and biomedical texts confirm significant gains in both accuracy and efficiency.

</details>


### [36] [Comparing Approaches to Automatic Summarization in Less-Resourced Languages](https://arxiv.org/abs/2512.24410)
*Chester Palen-Michel,Constantine Lignos*

Main category: cs.CL

TL;DR: 该论文研究低资源语言的自动文本摘要，比较了包括零样本大语言模型（LLM）提示、微调mT5模型（结合数据增强与多语言迁移）以及基于LLM翻译-摘要-回译的流程等多种方法。实验使用五种评估指标，发现微调后的多语言mT5基线在多数指标上优于大多数方法（包括零样本LLM），且不同参数规模相近的LLM在低资源语言上的表现差异显著；此外，用LLM作为评估者在低资源语言上可能不够可靠。


<details>
  <summary>Details</summary>
Motivation: 高资源语言（如英语）的自动文本摘要已取得高性能，但低资源语言的相关研究较少。因此，有必要系统比较适用于低资源语言的多种摘要方法，以探索更有效和可靠的解决方案。

Method: 作者比较了多种摘要策略：1）对大小不同的大语言模型（LLM）进行零样本提示；2）对mT5等较小模型进行微调，并结合三种数据增强方法及多语言迁移；3）采用LLM翻译管道（源语言→英语→摘要→回译）。评估使用五种不同指标，并考察LLM作为评估者的可靠性。

Result: 实验结果显示：1）参数规模相近的LLM在低资源语言上的摘要性能存在明显差异；2）多语言微调的mT5基线在多数评估指标上优于其他方法，包括零样本LLM；3）使用LLM作为评估者在低资源语言上可能不够可靠。

Conclusion: 对于低资源语言的自动摘要任务，经过多语言微调的小型模型（如mT5）通常比零样本大语言模型更有效；同时，在低资源场景下应谨慎使用LLM作为自动评估工具。

Abstract: Automatic text summarization has achieved high performance in high-resourced languages like English, but comparatively less attention has been given to summarization in less-resourced languages. This work compares a variety of different approaches to summarization from zero-shot prompting of LLMs large and small to fine-tuning smaller models like mT5 with and without three data augmentation approaches and multilingual transfer. We also explore an LLM translation pipeline approach, translating from the source language to English, summarizing and translating back. Evaluating with five different metrics, we find that there is variation across LLMs in their performance across similar parameter sizes, that our multilingual fine-tuned mT5 baseline outperforms most other approaches including zero-shot LLM performance for most metrics, and that LLM as judge may be less reliable on less-resourced languages.

</details>


### [37] [Cleaning English Abstracts of Scientific Publications](https://arxiv.org/abs/2512.24459)
*Michael E. Rose,Nils A. Herrmann,Sebastian Erhardt*

Main category: cs.CL

TL;DR: 提出并开源了一个用于自动清理英文科研摘要中无关内容（如版权声明、作者注等）的语言模型，该模型在保持保守性的同时提高了文本嵌入的信息含量和相似性排序质量。


<details>
  <summary>Details</summary>
Motivation: 大量已发表的科研摘要包含与研究内容无关的冗余信息（如版权说明、章节标题、作者备注、元数据等），这些内容会干扰基于文本相似性或嵌入的下游分析。

Method: 开发并集成一个开源语言模型，用于自动识别并移除英文科学摘要中的非实质性内容。

Result: 该模型在清理摘要时表现出高保守性和精确性，有效改善了清理后摘要的相似性排序，并提升了标准长度嵌入的信息含量。

Conclusion: 所提出的语言模型能有效提升科研摘要的质量，适用于需要高质量文本输入的下游自然语言处理任务。

Abstract: Scientific abstracts are often used as proxies for the content and thematic focus of research publications. However, a significant share of published abstracts contains extraneous information-such as publisher copyright statements, section headings, author notes, registrations, and bibliometric or bibliographic metadata-that can distort downstream analyses, particularly those involving document similarity or textual embeddings. We introduce an open-source, easy-to-integrate language model designed to clean English-language scientific abstracts by automatically identifying and removing such clutter. We demonstrate that our model is both conservative and precise, alters similarity rankings of cleaned abstracts and improves information content of standard-length embeddings.

</details>


### [38] [IELTS Writing Revision Platform with Automated Essay Scoring and Adaptive Feedback](https://arxiv.org/abs/2512.24460)
*Titas Ramancauskas,Kotryna Ramancauske*

Main category: cs.CL

TL;DR: 本文设计并评估了一个面向雅思写作考试的智能修订平台，结合用户友好的界面、基于DistilBERT的自动评分系统和针对雅思评分标准的自适应反馈机制，通过多轮设计型研究（DBR）验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统雅思写作备考方法缺乏依据官方评分标准的个性化反馈，难以有效提升考生写作能力，因此需要一个能提供精准、自适应反馈的智能辅助平台。

Method: 采用设计型研究（DBR）方法，历经多个迭代周期：初期使用基于规则的自动评分系统，后期升级为基于DistilBERT transformer模型加回归头的自动作文评分（AES）系统，并在此基础上实现自适应反馈功能；平台架构将对话式引导与写作界面分离以降低认知负荷。

Result: 在DBR第4周期中，DistilBERT模型显著优于规则方法（MAE=0.66，R²为正）；第5周期的自适应反馈使考生平均分数提升0.060分（p=0.011，Cohen's d=0.504），但效果因修订策略而异；表面级修正比结构性干预更可靠。

Conclusion: 自动化反馈可作为人类教师指导的有效补充，尤其适用于提供保守、表层的修改建议；但在高分段作文评估方面仍存在挑战，未来需开展长期追踪研究并引入官方考官验证。

Abstract: This paper presents the design, development, and evaluation of a proposed revision platform assisting candidates for the International English Language Testing System (IELTS) writing exam. Traditional IELTS preparation methods lack personalised feedback, catered to the IELTS writing rubric. To address these shortcomings, the platform features an attractive user interface (UI), an Automated Essay Scoring system (AES), and targeted feedback tailored to candidates and the IELTS writing rubric. The platform architecture separates conversational guidance from a dedicated writing interface to reduce cognitive load and simulate exam conditions. Through iterative, Design-Based Research (DBR) cycles, the study progressed from rule-based to transformer-based with a regression head scoring, mounted with adaptive feedback.
  Early cycles (2-3) revealed fundamental limitations of rule-based approaches: mid-band compression, low accuracy, and negative $R^2$ values. DBR Cycle 4 implemented a DistilBERT transformer model with a regression head, yielding substantial improvements with MAE of 0.66 and positive $R^2$. This enabled Cycle 5's adaptive feedback implementation, which demonstrated statistically significant score improvements (mean +0.060 bands, p = 0.011, Cohen's d = 0.504), though effectiveness varied by revision strategy. Findings suggest automated feedback functions are most suited as a supplement to human instruction, with conservative surface-level corrections proving more reliable than aggressive structural interventions for IELTS preparation contexts. Challenges remain in assessing higher-band essays, and future work should incorporate longitudinal studies with real IELTS candidates and validation from official examiners.

</details>


### [39] [Paragraph Segmentation Revisited: Towards a Standard Task for Structuring Speech](https://arxiv.org/abs/2512.24517)
*Fabian Retkowski,Alexander Waibel*

Main category: cs.CL

TL;DR: 该论文将段落分割引入语音转录后处理流程，提出了首个面向语音领域的段落分割基准数据集（TEDPara 和 YTSegPara），设计了一种基于大语言模型的约束解码方法以保留原始文本结构，并开发了高效模型 MiniSeg，在段落乃至章节级别实现当前最优性能，从而确立段落分割为语音处理中的标准化实用任务。


<details>
  <summary>Details</summary>
Motivation: 自动语音转录通常以无结构的词流形式呈现，影响可读性与再利用。现有语音后处理流程缺乏段落结构化步骤，而文本分割领域也缺少自然、鲁棒的基准，尤其在语音场景下。

Method: 1）构建两个新基准：人工标注的 TEDPara 与合成标注的 YTSegPara；2）提出约束解码策略，使大语言模型可在不改变原始转录内容的前提下插入段落分隔符，支持句子对齐的评估；3）设计轻量模型 MiniSeg，并扩展为层次化结构以联合预测章节与段落。

Result: MiniSeg 在段落分割任务上达到 SOTA 性能，其层次化版本能以极低计算开销同时预测章节和段落边界；所提基准填补了语音领域结构化分割任务的空白。

Conclusion: 段落分割应作为语音转录后处理的标准环节，本文通过数据集、方法和模型三方面贡献，推动其成为语音处理中实用且可评估的任务。

Abstract: Automatic speech transcripts are often delivered as unstructured word streams that impede readability and repurposing. We recast paragraph segmentation as the missing structuring step and fill three gaps at the intersection of speech processing and text segmentation. First, we establish TEDPara (human-annotated TED talks) and YTSegPara (YouTube videos with synthetic labels) as the first benchmarks for the paragraph segmentation task. The benchmarks focus on the underexplored speech domain, where paragraph segmentation has traditionally not been part of post-processing, while also contributing to the wider text segmentation field, which still lacks robust and naturalistic benchmarks. Second, we propose a constrained-decoding formulation that lets large language models insert paragraph breaks while preserving the original transcript, enabling faithful, sentence-aligned evaluation. Third, we show that a compact model (MiniSeg) attains state-of-the-art accuracy and, when extended hierarchically, jointly predicts chapters and paragraphs with minimal computational cost. Together, our resources and methods establish paragraph segmentation as a standardized, practical task in speech processing.

</details>


### [40] [Safe in the Future, Dangerous in the Past: Dissecting Temporal and Linguistic Vulnerabilities in LLMs](https://arxiv.org/abs/2512.24556)
*Muhammad Abdullahi Said,Muhammad Sammani Sani*

Main category: cs.CL

TL;DR: 该研究通过HausaSafety数据集对GPT-5.1、Gemini 3 Pro和Claude 4.5 Opus进行多语言安全对齐审计，发现模型在豪萨语中的安全性并非简单退化，而是受语言与时间框架交互影响，表现出“复杂干扰”现象，并揭示了显著的“时间不对称性”，提出需采用“不变对齐”范式以保障跨语言和时间的安全稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在关键基础设施中广泛应用，但其安全对齐能力是否能从英语零样本迁移至其他语言仍被默认成立，这一假设在低资源语言（如豪萨语）中可能造成严重风险。作者旨在系统评估主流模型在非英语语境下的安全表现，特别是针对西非地区特有的威胁场景，以揭示多语言安全对齐的真实机制与潜在漏洞。

Method: 构建名为HausaSafety的新型对抗性数据集，涵盖西非真实威胁场景（如Yahoo-Yahoo诈骗、Dane枪支制造）；采用2（语言：英语 vs. 豪萨语）×4（时间框架）因子设计，对三个前沿模型（GPT-5.1、Gemini 3 Pro、Claude 4.5 Opus）进行共计1,440次评估；分析语言与时间变量交互对安全输出的影响。

Result: 1）发现“复杂干扰”机制：安全性能由语言与时间框架共同决定，而非单纯因低资源语言而下降；  
2）观察到“反向语言效应”：Claude 4.5 Opus在豪萨语中安全率（45.0%）反而高于英语（36.7%），归因于不确定性引发的拒绝策略；  
3）揭示“时间不对称性”：过去时态绕过安全防御（安全率仅15.6%），而将来时态触发过度保守拒绝（安全率达57.2%）；  
4）最安全与最脆弱配置间存在9.2倍差距，表明安全性是情境依赖的动态状态；  
5）模型依赖表面启发式而非深层语义理解，导致“安全口袋”现象，使全球南方用户暴露于本地化风险。

Conclusion: 当前大语言模型的安全对齐机制缺乏跨语言和跨时间的鲁棒性，其表现高度依赖上下文线索，易形成局部安全漏洞。作者呼吁摒弃“安全可零样本迁移”的假设，转向“不变对齐”（Invariant Alignment）新范式，即在不同语言和时间表述下保持一致的安全行为，以真正保障全球用户的公平与安全。

Abstract: As Large Language Models (LLMs) integrate into critical global infrastructure, the assumption that safety alignment transfers zero-shot from English to other languages remains a dangerous blind spot. This study presents a systematic audit of three state of the art models (GPT-5.1, Gemini 3 Pro, and Claude 4.5 Opus) using HausaSafety, a novel adversarial dataset grounded in West African threat scenarios (e.g., Yahoo-Yahoo fraud, Dane gun manufacturing). Employing a 2 x 4 factorial design across 1,440 evaluations, we tested the non-linear interaction between language (English vs. Hausa) and temporal framing. Our results challenge the prevailing multilingual safety gap narrative. Instead of a simple degradation in low-resource settings, we identified a mechanism of Complex Interference where safety is determined by the intersection of variables. While models exhibited a Reverse Linguistic with Claude 4.5 Opus proving significantly safer in Hausa (45.0%) than in English (36.7%) due to uncertainty-driven refusal they suffered catastrophic failures in temporal reasoning. We report a profound Temporal Asymmetry, where past-tense framing bypassed defenses (15.6% safe) while future-tense scenarios triggered hyper-conservative refusals (57.2% safe). The magnitude of this volatility is illustrated by a 9.2x disparity between the safest and most vulnerable configurations, proving that safety is not a fixed property but a context-dependent state. We conclude that current models rely on superficial heuristics rather than robust semantic understanding, creating Safety Pockets that leave Global South users exposed to localized harms. We propose Invariant Alignment as a necessary paradigm shift to ensure safety stability across linguistic and temporal shifts.

</details>


### [41] [HaluNet: Multi-Granular Uncertainty Modeling for Efficient Hallucination Detection in LLM Question Answering](https://arxiv.org/abs/2512.24562)
*Chaodong Tong,Qi Zhang,Jiayang Gao,Lei Jiang,Yanbing Liu,Nannan Sun*

Main category: cs.CL

TL;DR: HaluNet is a lightweight, trainable framework that fuses token-level probability and semantic representation uncertainties to detect hallucinations in LLM-generated answers efficiently in a single pass.


<details>
  <summary>Details</summary>
Motivation: Existing hallucination detection methods often focus on a single type of internal uncertainty and neglect the complementary information between token-level probability uncertainty and semantic representation uncertainty, limiting their effectiveness and robustness.

Method: HaluNet integrates multi-granular token-level uncertainties by combining semantic embeddings with probabilistic confidence and distributional uncertainty through a multi-branch neural architecture that adaptively fuses knowledge and output uncertainty.

Result: Experiments on SQuAD, TriviaQA, and Natural Questions demonstrate that HaluNet achieves strong hallucination detection performance and high computational efficiency, both with and without access to context.

Conclusion: HaluNet effectively leverages complementary internal uncertainty signals for real-time, scalable hallucination detection in LLM-based QA systems, offering a practical solution without reliance on external resources.

Abstract: Large Language Models (LLMs) excel at question answering (QA) but often generate hallucinations, including factual errors or fabricated content. Detecting hallucinations from internal uncertainty signals is attractive due to its scalability and independence from external resources. Existing methods often aim to accurately capture a single type of uncertainty while overlooking the complementarity among different sources, particularly between token-level probability uncertainty and the uncertainty conveyed by internal semantic representations, which provide complementary views on model reliability. We present \textbf{HaluNet}, a lightweight and trainable neural framework that integrates multi granular token level uncertainties by combining semantic embeddings with probabilistic confidence and distributional uncertainty. Its multi branch architecture adaptively fuses what the model knows with the uncertainty expressed in its outputs, enabling efficient one pass hallucination detection. Experiments on SQuAD, TriviaQA, and Natural Questions show that HaluNet delivers strong detection performance and favorable computational efficiency, with or without access to context, highlighting its potential for real time hallucination detection in LLM based QA systems.

</details>


### [42] [Korean Canonical Legal Benchmark: Toward Knowledge-Independent Evaluation of LLMs' Legal Reasoning Capabilities](https://arxiv.org/abs/2512.24572)
*Hongseok Oh,Wonseok Hwang,Kyoung-Woon On*

Main category: cs.CL

TL;DR: 提出了KCL基准，用于评估语言模型在法律推理方面的能力，并将其与领域知识解耦；包含多项选择（KCL-MCQA）和开放式生成（KCL-Essay）两部分，评测了30多个模型，发现当前模型在开放式任务上仍有显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法难以区分语言模型的法律推理能力与其内化的领域知识，因此需要一个能将二者解耦的基准。

Method: 构建KCL基准，提供问题级别的支持判例，包含KCL-MCQA（283道多选题，1,103个对齐判例）和KCL-Essay（169道开放生成题，550个判例及2,739条评分标准），并对30多个模型进行系统评估。

Result: 评估显示模型在KCL-Essay任务上表现明显不足，且专门优化推理能力的模型优于通用模型。

Conclusion: KCL有效支持对法律推理能力的独立评估，揭示了当前模型在复杂法律推理任务中的局限性，并公开了全部资源以促进后续研究。

Abstract: We introduce the Korean Canonical Legal Benchmark (KCL), a benchmark designed to assess language models' legal reasoning capabilities independently of domain-specific knowledge. KCL provides question-level supporting precedents, enabling a more faithful disentanglement of reasoning ability from parameterized knowledge. KCL consists of two components: (1) KCL-MCQA, multiple-choice problems of 283 questions with 1,103 aligned precedents, and (2) KCL-Essay, open-ended generation problems of 169 questions with 550 aligned precedents and 2,739 instance-level rubrics for automated evaluation. Our systematic evaluation of 30+ models shows large remaining gaps, particularly in KCL-Essay, and that reasoning-specialized models consistently outperform their general-purpose counterparts. We release all resources, including the benchmark dataset and evaluation code, at https://github.com/lbox-kr/kcl.

</details>


### [43] [Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time](https://arxiv.org/abs/2512.24574)
*Zhenyu Zhang,Xiaoxia Wu,Zhongzhu Zhou,Qingyang Wu,Yineng Zhang,Pragaash Ponnusamy,Harikaran Subbaraj,Jue Wang,Shuaiwen Leon Song,Ben Athiwaratkun*

Main category: cs.CL

TL;DR: 本文提出 CREST，一种无需训练的推理时方法，通过识别并干预大语言模型中与低效推理行为（如过度思考或思考不足）相关的特定注意力头，动态抑制无效推理路径，在提升准确率的同时显著减少生成 token 数量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂任务中依赖长链式思维（CoT）推理，但此类推理常因生成过多 token 而导致高延迟，或因推理过程不稳定（表现为“思考不足”或“过度思考”）而影响性能。作者旨在理解推理轨迹的内部结构，并探索无需重新训练即可优化推理效率与稳定性的方法。

Method: 作者发现模型中存在与特定认知行为（如验证、回溯）相关的专用注意力头。基于此，提出 CREST 方法，包含两个阶段：(1) 离线校准阶段，识别认知相关注意力头并计算其对应的 steering 向量；(2) 推理阶段，对隐藏表示沿这些向量方向进行旋转，以抑制低效推理行为。

Result: 在多个推理基准和不同模型上，CREST 最多提升准确率 17.5%，同时减少 37.6% 的 token 使用量，实现了更高精度与更低计算开销的统一。

Conclusion: CREST 提供了一种简单有效的训练-free 推理优化方案，通过定向干预认知相关的注意力机制，显著提升了大语言模型推理的效率与可靠性。

Abstract: Large Language Models (LLMs) often rely on long chain-of-thought (CoT) reasoning to solve complex tasks. While effective, these trajectories are frequently inefficient, leading to high latency from excessive token generation, or unstable reasoning that alternates between underthinking (shallow, inconsistent steps) and overthinking (repetitive, verbose reasoning). In this work, we study the structure of reasoning trajectories and uncover specialized attention heads that correlate with distinct cognitive behaviors such as verification and backtracking. By lightly intervening on these heads at inference time, we can steer the model away from inefficient modes. Building on this insight, we propose CREST, a training-free method for Cognitive REasoning Steering at Test-time. CREST has two components: (1) an offline calibration step that identifies cognitive heads and derives head-specific steering vectors, and (2) an inference-time procedure that rotates hidden representations to suppress components along those vectors. CREST adaptively suppresses unproductive reasoning behaviors, yielding both higher accuracy and lower computational cost. Across diverse reasoning benchmarks and models, CREST improves accuracy by up to 17.5% while reducing token usage by 37.6%, offering a simple and effective pathway to faster, more reliable LLM reasoning.

</details>


### [44] [Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models](https://arxiv.org/abs/2512.24618)
*Junru Lu,Jiarui Qin,Lingfeng Qiao,Yinghui Li,Xinyi Dai,Bo Ke,Jianfeng He,Ruizhi Qiao,Di Yin,Xing Sun,Yunsheng Wu,Yinsong Liu,Shuangyin Liu,Mingkong Tang,Haodong Lin,Jiayi Kuang,Fanxu Meng,Xiaojuan Tang,Yunjia Xi,Junjie Huang,Haotong Yang,Zhenyi Shen,Yangning Li,Qianwen Zhang,Yifei Yu,Siyu An,Junnan Dong,Qiufeng Wang,Jie Wang,Keyu Chen,Wei Wen,Taian Guo,Zhifeng Shen,Daohai Yu,Jiahao Li,Ke Li,Zongyi Li,Xiaoyu Tan*

Main category: cs.CL

TL;DR: Youtu-LLM 是一个从头训练的 1.96B 参数轻量级语言模型，通过紧凑架构、常识–STEM–智能体三阶段课程学习和可扩展的智能体中训策略，在保持低内存占用的同时，在通用和智能体任务上均达到同规模模型的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有小型语言模型多依赖蒸馏，缺乏原生推理与规划能力；作者旨在构建一个从零开始训练、兼具高计算效率与内在智能体能力的轻量模型，以支持长上下文、复杂推理及工具调用等任务。

Method: 采用基于密集多潜在注意力（MLA）的紧凑架构与 STEM 导向词表，支持 128k 上下文；利用约 11T token 的语料，实施“常识→STEM→智能体”渐进式预训练课程；并在中训阶段通过多样化合成轨迹数据强化模型在数学、编程和工具使用中的规划与反思能力。

Result: Youtu-LLM 在通用基准上媲美更大模型，在智能体专用任务上显著超越现有 SOTA，成为首个在 sub-2B 规模下实现强原生智能体能力的语言模型。

Conclusion: 轻量级语言模型无需依赖蒸馏即可通过系统性训练获得强大的原生智能体能力，Youtu-LLM 验证了高效架构与课程设计对小模型认知能力提升的关键作用。

Abstract: We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled "Commonsense-STEM-Agent" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.

</details>


### [45] [Do Large Language Models Know What They Are Capable Of?](https://arxiv.org/abs/2512.24661)
*Casey O. Barkan,Sid Black,Oliver Sourbut*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）是否能准确预测自身在特定任务上的成功概率，以及这种预测能力是否随多步任务推进而改善。研究发现，尽管所有测试的LLMs都表现出过度自信，但多数仍具备优于随机的判别能力；部分模型在引入失败的上下文经验后能降低过度自信并改善决策，但并非全部。模型的决策在其主观成功概率下大致理性，但由于估计过于乐观，导致实际决策不佳。结果表明当前LLM智能体受限于对自身能力的认知不足，并讨论了这对AI滥用与对齐风险的影响。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型对其自身任务表现的自我认知能力，尤其是在多步任务和高失败成本场景下能否通过上下文经验改进决策，以评估其作为智能体的可靠性及潜在风险。

Method: 通过实验评估多个大语言模型在单步与多步任务中对其成功概率的预测准确性，分析其判别能力、过度自信程度的变化，并测试引入失败上下文经验后模型是否能调整信心水平以优化决策。

Result: 所有被测LLM均表现出过度自信，但多数具有优于随机的判别力；较新或更大的模型通常未显示更强判别力（Claude系列除外）；在多步任务中，若干前沿LLM的过度自信随任务推进而加剧，推理型LLM表现不优于非推理型；部分LLM在经历失败上下文后能降低过度自信并显著改善决策，但并非全部；所有模型的决策在其主观成功概率下近似理性，但因估计过于乐观而导致实际决策不佳。

Conclusion: 当前大语言模型智能体受限于对自身能力的认知不足，其过度自信导致次优决策，这对其在高风险场景中的部署构成挑战，并可能加剧AI滥用与对齐风险。

Abstract: We investigate whether large language models (LLMs) can predict whether they will succeed on a given task and whether their predictions improve as they progress through multi-step tasks. We also investigate whether LLMs can learn from in-context experiences to make better decisions about whether to pursue a task in scenarios where failure is costly. All LLMs we tested are overconfident, but most predict their success with better-than-random discriminatory power. We find that newer and larger LLMs generally do not have greater discriminatory power, though Claude models do show such a trend. On multi-step agentic tasks, the overconfidence of several frontier LLMs worsens as they progress through the tasks, and reasoning LLMs perform comparably to or worse than non-reasoning LLMs. With in-context experiences of failure, some but not all LLMs reduce their overconfidence leading to significantly improved decision making, while others do not. Interestingly, all LLMs' decisions are approximately rational given their estimated probabilities of success, yet their overly-optimistic estimates result in poor decision making. These results suggest that current LLM agents are hindered by their lack of awareness of their own capabilities. We discuss the implications of LLMs' awareness of their capabilities for AI misuse and misalignment risks.

</details>


### [46] [R-Debater: Retrieval-Augmented Debate Generation through Argumentative Memory](https://arxiv.org/abs/2512.24684)
*Maoyuan Li,Zhongsheng Wang,Haoyuan Li,Jiamou Liu*

Main category: cs.CL

TL;DR: R-Debater is an agentic debate framework that leverages argumentative memory and retrieval-based evidence to generate consistent, multi-turn debates with improved coherence, stance alignment, and factual grounding compared to standard LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing language models often struggle with maintaining stance consistency, using relevant evidence, and generating logically coherent responses across multiple debate turns. The authors aim to address these limitations by integrating rhetorical principles and memory-based argument reuse into a structured debate agent.

Method: R-Debater combines a debate knowledge base for retrieving case-like evidence and prior debate moves with a role-based agent that generates coherent, stance-consistent utterances. It uses retrieval-augmented generation grounded in argumentative memory and is evaluated on next-utterance generation and adversarial multi-turn simulations.

Result: On the ORCHID benchmark with 32 held-out debates across seven domains, R-Debater outperforms strong LLM baselines in both single-turn (measured by InspireScore) and multi-turn settings (evaluated by Debatrix). Human evaluation by 20 experienced debaters confirms superior consistency and evidence usage.

Conclusion: Integrating retrieval-based grounding with structured, memory-informed planning enables more faithful, coherent, and stance-aligned multi-turn debates, demonstrating the value of rhetorical and memory-based modeling in agentic debate systems.

Abstract: We present R-Debater, an agentic framework for generating multi-turn debates built on argumentative memory. Grounded in rhetoric and memory studies, the system views debate as a process of recalling and adapting prior arguments to maintain stance consistency, respond to opponents, and support claims with evidence. Specifically, R-Debater integrates a debate knowledge base for retrieving case-like evidence and prior debate moves with a role-based agent that composes coherent utterances across turns. We evaluate on standardized ORCHID debates, constructing a 1,000-item retrieval corpus and a held-out set of 32 debates across seven domains. Two tasks are evaluated: next-utterance generation, assessed by InspireScore (subjective, logical, and factual), and adversarial multi-turn simulations, judged by Debatrix (argument, source, language, and overall). Compared with strong LLM baselines, R-Debater achieves higher single-turn and multi-turn scores. Human evaluation with 20 experienced debaters further confirms its consistency and evidence use, showing that combining retrieval grounding with structured planning yields more faithful, stance-aligned, and coherent debates across turns.

</details>


### [47] [MUSIC: MUlti-Step Instruction Contrast for Multi-Turn Reward Models](https://arxiv.org/abs/2512.24693)
*Wenzhe Li,Shujian Zhang,Wenxuan Zhou,John Lambert,Chi Jin,Andrew Hard,Rajiv Mathews,Lun Wang*

Main category: cs.CL

TL;DR: 本文提出MUSIC（Multi-Step Instruction Contrast），一种无监督的数据增强方法，通过合成在多个对话轮次中存在差异的对比对话对，来提升多轮奖励模型（RM）的性能。实验表明，基于MUSIC增强数据训练的RM在多轮对话评估上与先进专有LLM评判者的一致性更高，且不损害其在单轮基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有偏好数据集通常仅基于对话的最后一轮构建响应对比，无法充分捕捉多轮对话中的复杂性和细微差别，导致多轮自动评估能力滞后于多轮训练技术的发展。

Method: 提出MUSIC方法，一种无监督的数据增强策略，通过合成跨越多个对话轮次的对比对话对，以丰富训练信号；并在Skywork偏好数据集上使用该方法训练基于Gemma-2-9B-Instruct的多轮奖励模型。

Result: MUSIC增强的多轮RM在多轮对话评估任务中显著优于基线方法，与高级专有LLM评判者的判断一致性更高，同时在标准单轮RM基准上保持了原有性能。

Conclusion: 构建有效的多轮奖励模型需要涵盖多轮交互差异的对比信号，而MUSIC提供了一种无需人工标注即可生成此类信号的有效途径，显著提升了多轮对话自动评估的质量。

Abstract: Evaluating the quality of multi-turn conversations is crucial for developing capable Large Language Models (LLMs), yet remains a significant challenge, often requiring costly human evaluation. Multi-turn reward models (RMs) offer a scalable alternative and can provide valuable signals for guiding LLM training. While recent work has advanced multi-turn \textit{training} techniques, effective automated \textit{evaluation} specifically for multi-turn interactions lags behind. We observe that standard preference datasets, typically contrasting responses based only on the final conversational turn, provide insufficient signal to capture the nuances of multi-turn interactions. Instead, we find that incorporating contrasts spanning \textit{multiple} turns is critical for building robust multi-turn RMs. Motivated by this finding, we propose \textbf{MU}lti-\textbf{S}tep \textbf{I}nstruction \textbf{C}ontrast (MUSIC), an unsupervised data augmentation strategy that synthesizes contrastive conversation pairs exhibiting differences across multiple turns. Leveraging MUSIC on the Skywork preference dataset, we train a multi-turn RM based on the Gemma-2-9B-Instruct model. Empirical results demonstrate that our MUSIC-augmented RM outperforms baseline methods, achieving higher alignment with judgments from advanced proprietary LLM judges on multi-turn conversations, crucially, without compromising performance on standard single-turn RM benchmarks.

</details>


### [48] [BIOME-Bench: A Benchmark for Biomolecular Interaction Inference and Multi-Omics Pathway Mechanism Elucidation from Scientific Literature](https://arxiv.org/abs/2512.24733)
*Sibo Wei,Peng Chen,Lifeng Dong,Yin Luo,Lei Wang,Peng Zhang,Wenpeng Lu,Jianbin Guo,Hongjun Yang,Dajun Zeng*

Main category: cs.CL

TL;DR: 该论文提出了 BIOME-Bench，一个用于评估大语言模型（LLMs）在多组学通路机制解析中能力的标准化基准，并发现当前模型在生物分子相互作用推理和通路级机制解释方面仍存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有的通路富集（PE）方法受限于通路资源的结构性缺陷（如更新滞后、功能冗余、对分子状态不敏感），而尽管已有研究尝试用大语言模型改进PE解释，但缺乏标准化的端到端多组学通路机制解析基准，导致评估局限于小规模人工数据集或个案研究，阻碍了可复现的进展。

Method: 作者构建了 BIOME-Bench 基准，采用严格的四阶段工作流程，用于评估大语言模型在多组学分析中的两项核心能力：生物分子相互作用推理（Biomolecular Interaction Inference）和端到端多组学通路机制解析（Multi-Omics Pathway Mechanism Elucidation）。同时开发了针对这两项任务的评估协议，并在多个当前主流强模型上进行了全面实验。

Result: 实验结果表明，现有大语言模型在多组学分析中仍存在显著缺陷，难以可靠区分细粒度的生物分子关系类型，也无法生成忠实且鲁棒的通路级机制解释。

Conclusion: 当前的大语言模型尚不足以胜任高精度的多组学通路机制解析任务，亟需更专门的基准（如 BIOME-Bench）推动模型在该领域的可复现、系统性改进。

Abstract: Multi-omics studies often rely on pathway enrichment to interpret heterogeneous molecular changes, but pathway enrichment (PE)-based workflows inherit structural limitations of pathway resources, including curation lag, functional redundancy, and limited sensitivity to molecular states and interventions. Although recent work has explored using large language models (LLMs) to improve PE-based interpretation, the lack of a standardized benchmark for end-to-end multi-omics pathway mechanism elucidation has largely confined evaluation to small, manually curated datasets or ad hoc case studies, hindering reproducible progress. To address this issue, we introduce BIOME-Bench, constructed via a rigorous four-stage workflow, to evaluate two core capabilities of LLMs in multi-omics analysis: Biomolecular Interaction Inference and end-to-end Multi-Omics Pathway Mechanism Elucidation. We develop evaluation protocols for both tasks and conduct comprehensive experiments across multiple strong contemporary models. Experimental results demonstrate that existing models still exhibit substantial deficiencies in multi-omics analysis, struggling to reliably distinguish fine-grained biomolecular relation types and to generate faithful, robust pathway-level mechanistic explanations.

</details>


### [49] [Compute-Accuracy Pareto Frontiers for Open-Source Reasoning Large Language Models](https://arxiv.org/abs/2512.24776)
*Ákos Prucs,Márton Csutora,Mátyás Antal,Márk Marosi*

Main category: cs.CL

TL;DR: 该论文对开源大语言模型在数学与推理任务上的性能与推理计算成本进行权衡分析，发现混合专家（MoE）架构在性能与效率之间表现优异，并揭示了推理计算存在收益递减的饱和点。


<details>
  <summary>Details</summary>
Motivation: 当前研究多关注大语言模型在复杂推理任务中的准确性，却忽视了生成长推理链所带来的高昂计算开销；而在工业应用中，模型选择需兼顾准确率与推理成本。

Method: 作者对新旧开源大语言模型进行测试时计算感知的评估，在多个数学与推理密集型基准上绘制其帕累托前沿，并分析随时间演化的帕累托效率趋势及推理计算的边际收益。

Result: MoE架构在性能与效率之间展现出优越的平衡；随着推理计算量增加，模型准确率提升逐渐趋于饱和，表明模型存在无法通过延长推理克服的内在能力限制。

Conclusion: 推理阶段的计算资源投入存在收益递减现象，MoE架构是兼顾效率与性能的有效方案，未来模型改进应关注内在能力而非单纯延长推理链。

Abstract: Large Language Models (LLMs) are demonstrating rapid improvements on complex reasoning benchmarks, particularly when allowed to utilize intermediate reasoning steps before converging on a final solution. However, current literature often overlooks the significant computational burden associated with generating long reasoning sequences. For industrial applications, model selection depends not only on raw accuracy but also on resource constraints and inference costs. In this work, we conduct a test-time-compute aware evaluation of both contemporary and older open-source LLMs, mapping their Pareto frontiers across math- and reasoning-intensive benchmarks. Our findings identify the Mixture of Experts (MoE) architecture as a strong candidate to balance performance and efficiency in our evaluation setting. Furthermore, we trace the trajectory of Pareto efficiency over time to derive an emergent trend regarding accuracy gain per unit of compute. Finally, we demonstrate that there is a saturation point for inference-time compute. Beyond a certain threshold, accuracy gains diminish, indicating that while extended reasoning capabilities are beneficial, they cannot overcome intrinsic model limitations regarding specific complexities.

</details>


### [50] [Practising responsibility: Ethics in NLP as a hands-on course](https://arxiv.org/abs/2512.24825)
*Malvina Nissim,Viviana Patti,Beatrice Savoldi*

Main category: cs.CL

TL;DR: 本文介绍了在自然语言处理（NLP）教育中融入伦理内容的课程设计与教学方法，强调通过主动学习、互动环节和“以教促学”策略培养学生的批判性思维，并分享了四年间在多所院校、不同教育层次和跨学科背景下的实践经验与可复用教学成果。


<details>
  <summary>Details</summary>
Motivation: 随着NLP系统日益普及，将伦理考量纳入NLP教育变得至关重要，但课程开发面临领域快速演进及超越传统技术训练以培养批判性思维的双重挑战。

Method: 采用以主动学习为核心的教育方法，包括互动课堂、实践任务和“学习即教学”（learning by teaching）策略，并在四年间于不同机构、教育层级和跨学科环境中迭代优化课程。

Result: 课程成功适应多种教学场景，产生了大量可复用的教学资源和由学生创作的面向多元受众的教育产品。

Conclusion: 作者希望通过分享其课程设计与实施经验，为希望将社会影响议题融入NLP教学的教育者提供参考与启发。

Abstract: As Natural Language Processing (NLP) systems become more pervasive, integrating ethical considerations into NLP education has become essential. However, this presents inherent challenges in curriculum development: the field's rapid evolution from both academia and industry, and the need to foster critical thinking beyond traditional technical training. We introduce our course on Ethical Aspects in NLP and our pedagogical approach, grounded in active learning through interactive sessions, hands-on activities, and "learning by teaching" methods. Over four years, the course has been refined and adapted across different institutions, educational levels, and interdisciplinary backgrounds; it has also yielded many reusable products, both in the form of teaching materials and in the form of actual educational products aimed at diverse audiences, made by the students themselves. By sharing our approach and experience, we hope to provide inspiration for educators seeking to incorporate social impact considerations into their curricula.

</details>


### [51] [Triangulation as an Acceptance Rule for Multilingual Mechanistic Interpretability](https://arxiv.org/abs/2512.24842)
*Yanan Long*

Main category: cs.CL

TL;DR: 该论文提出了一种名为“三角测量”（triangulation）的新标准，用于验证多语言语言模型中机制性解释的因果有效性，要求候选电路在保持语义不变但表层形式变化的跨语言环境中同时满足必要性、充分性和不变性。


<details>
  <summary>Details</summary>
Motivation: 多语言语言模型虽然整体性能强，但在不同语言、文字和文化背景下行为不可预测。现有机制性解释缺乏对跨环境稳定性的检验，容易误判仅在单一环境下有效的虚假电路。

Method: 作者形式化了“指称族”（reference families）作为语义保持但表层扰动的变体，并引入“三角测量”作为接受规则，要求候选子图满足：1）必要性（消融电路会损害目标行为），2）充分性（修复激活可迁移行为），3）不变性（上述效应在指称族中方向稳定且幅度足够）。通过自动电路发现生成候选子图，并用三角测量进行接受或拒绝。该方法基于因果抽象理论，将三角测量视为对一系列交换干预下近似变换得分的评估。

Result: 实验表明，三角测量能有效过滤掉那些在单语言环境中看似有效但在跨语言环境中失效的虚假电路，在多个模型族、语言对和任务上验证了其作为可证伪标准的有效性。

Conclusion: 三角测量为多语言模型的机制性解释提供了一个严格的因果与跨环境不变性标准，有助于提升可解释性研究的可靠性与泛化能力。

Abstract: Multilingual language models achieve strong aggregate performance yet often behave unpredictably across languages, scripts, and cultures. We argue that mechanistic explanations for such models should satisfy a \emph{causal} standard: claims must survive causal interventions and must \emph{cross-reference} across environments that perturb surface form while preserving meaning. We formalize \emph{reference families} as predicate-preserving variants and introduce \emph{triangulation}, an acceptance rule requiring necessity (ablating the circuit degrades the target behavior), sufficiency (patching activations transfers the behavior), and invariance (both effects remain directionally stable and of sufficient magnitude across the reference family). To supply candidate subgraphs, we adopt automatic circuit discovery and \emph{accept or reject} those candidates by triangulation. We ground triangulation in causal abstraction by casting it as an approximate transformation score over a distribution of interchange interventions, connect it to the pragmatic interpretability agenda, and present a comparative experimental protocol across multiple model families, language pairs, and tasks. Triangulation provides a falsifiable standard for mechanistic claims that filters spurious circuits passing single-environment tests but failing cross-lingual invariance.

</details>


### [52] [Big AI is accelerating the metacrisis: What can we do?](https://arxiv.org/abs/2512.24863)
*Steven Bird*

Main category: cs.CL

TL;DR: 当前生态、意义与语言危机正汇聚成一场元危机，大型AI加剧了这一趋势；语言工程师在其中扮演关键角色，却延续着对人类不利的可扩展性叙事，并为权贵提供技术支持；亟需探索以人类繁荣和地球生命为核心的替代性NLP发展路径。


<details>
  <summary>Details</summary>
Motivation: 作者指出世界正面临生态、意义和语言三重危机的交汇，而大型AI及其背后的语言工程实践正在加速这些危机，因此迫切需要重新思考自然语言处理（NLP）的发展方向，使其服务于人类福祉与地球生态。

Method: 本文采用批判性分析方法，审视当前AI与语言工程的价值取向、技术叙事及其社会影响，并呼吁通过集体智慧设计以人为本、尊重生命系统的替代方案。

Result: 揭示了当前主流NLP研究与开发中隐含的价值盲区及其对多重全球危机的推波助澜作用，提出转向“肯定生命”（life-affirming）范式的必要性。

Conclusion: NLP领域必须摆脱价值中立的迷思，拒绝服务于少数权贵的技术路径，转而构建以人类繁荣和地球可持续性为核心的新范式。

Abstract: The world is in the grip of ecological, meaning, and language crises which are converging into a metacrisis. Big AI is accelerating them all. Language engineers are playing a central role, persisting with a scalability story that is failing humanity, supplying critical talent to plutocrats and kleptocrats, and creating new technologies as if the whole endeavour was value-free. We urgently need to explore alternatives, applying our collective intelligence to design a life-affirming future for NLP that is centered on human flourishing on a living planet.

</details>


### [53] [Encyclo-K: Evaluating LLMs with Dynamically Composed Knowledge Statements](https://arxiv.org/abs/2512.24867)
*Yiming Liang,Yizhi Li,Yantao Du,Ge Zhang,Jiayi Zhou,Yuchen Wu,Yinzhu Piao,Denghui Cao,Tong Sun,Ziniu Li,Li Du,Bo Lei,Jiaheng Liu,Chenghua Lin,Zhaoxiang Zhang,Wenhao Huang,Jiajun Zhang*

Main category: cs.CL

TL;DR: Encyclo-K 是一个基于知识陈述的新型 LLM 基准，通过从教科书中提取独立知识陈述并在测试时动态组合生成问题，有效解决了现有基准在数据污染、单知识点评估和高成本标注方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 基准主要在问题层面构建，存在易受数据污染、仅评估单一知识点以及依赖昂贵领域专家标注三大缺陷，亟需一种更鲁棒、全面且低成本的评估范式。

Method: 提出 Encyclo-K：以权威教科书中的独立知识陈述为基本单元进行标注（仅需验证格式合规性），在测试时通过随机采样动态组合 8–10 个陈述生成综合性问题，实现大规模、低成本、抗污染的动态评估。

Result: 在 50 多个 LLM 上的实验表明，Encyclo-K 具有强区分度和挑战性：最强模型 GPT-5.1 准确率仅为 62.07%，推理模型和聊天模型分别呈现 16.04%–62.07% 和 9.71%–50.40% 的清晰性能梯度，验证了其动态评估与多陈述综合理解的有效性。

Conclusion: Encyclo-K 通过以知识陈述为单位并动态生成问题，成功克服了传统基准的三大缺陷，为 LLM 在细粒度学科知识上的综合理解能力提供了一个可扩展、可靠且低成本的动态评估框架。

Abstract: Benchmarks play a crucial role in tracking the rapid advancement of large language models (LLMs) and identifying their capability boundaries. However, existing benchmarks predominantly curate questions at the question level, suffering from three fundamental limitations: vulnerability to data contamination, restriction to single-knowledge-point assessment, and reliance on costly domain expert annotation. We propose Encyclo-K, a statement-based benchmark that rethinks benchmark construction from the ground up. Our key insight is that knowledge statements, not questions, can serve as the unit of curation, and questions can then be constructed from them. We extract standalone knowledge statements from authoritative textbooks and dynamically compose them into evaluation questions through random sampling at test time. This design directly addresses all three limitations: the combinatorial space is too vast to memorize, and model rankings remain stable across dynamically generated question sets, enabling reliable periodic dataset refresh; each question aggregates 8-10 statements for comprehensive multi-knowledge assessment; annotators only verify formatting compliance without requiring domain expertise, substantially reducing annotation costs. Experiments on over 50 LLMs demonstrate that Encyclo-K poses substantial challenges with strong discriminative power. Even the top-performing OpenAI-GPT-5.1 achieves only 62.07% accuracy, and model performance displays a clear gradient distribution--reasoning models span from 16.04% to 62.07%, while chat models range from 9.71% to 50.40%. These results validate the challenges introduced by dynamic evaluation and multi-statement comprehensive understanding. These findings establish Encyclo-K as a scalable framework for dynamic evaluation of LLMs' comprehensive understanding over multiple fine-grained disciplinary knowledge statements.

</details>


### [54] [mHC: Manifold-Constrained Hyper-Connections](https://arxiv.org/abs/2512.24880)
*Zhenda Xie,Yixuan Wei,Huanqi Cao,Chenggang Zhao,Chengqi Deng,Jiashi Li,Damai Dai,Huazuo Gao,Jiang Chang,Liang Zhao,Shangyan Zhou,Zhean Xu,Zhengyan Zhang,Wangding Zeng,Shengding Hu,Yuqing Wang,Jingyang Yuan,Lean Wang,Wenfeng Liang*

Main category: cs.CL

TL;DR: 提出了一种名为流形约束超连接（mHC）的新框架，在保留超连接（HC）性能优势的同时，通过将残差连接空间投影到特定流形上恢复其恒等映射性质，并优化基础设施以提升训练稳定性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 超连接（HC）虽提升了模型性能，但破坏了残差连接固有的恒等映射特性，导致训练不稳定、可扩展性受限以及显著的内存访问开销。

Method: 提出流形约束超连接（mHC）框架，将HC的残差连接空间投影到特定流形以恢复恒等映射性质，并结合严格的基础设施优化以确保效率。

Result: 实验表明mHC在大规模训练中有效，实现了切实的性能提升和更优的可扩展性。

Conclusion: mHC作为HC的一种灵活且实用的扩展，有助于深化对拓扑架构设计的理解，并为基础模型的演进提供新方向。

Abstract: Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.

</details>


### [55] [BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts](https://arxiv.org/abs/2512.24885)
*Hengli Li,Zhaoxin Yu,Qi Shen,Chenxi Li,Mengmeng Wang,Tinglang Wu,Yipeng Kang,Yuxuan Wang,Song-Chun Zhu,Zixia Jia,Zilong Zheng*

Main category: cs.CL

TL;DR: BEDA is a framework that integrates belief estimation with dialogue generation through probabilistic constraints, enabling agents to perform strategic dialogue acts (Adversarial and Alignment) more effectively. It outperforms strong baselines across three tasks: Conditional Keeper Burglar (adversarial), Mutual Friends (cooperative), and CaSiNo (negotiation).


<details>
  <summary>Details</summary>
Motivation: Prior work in strategic dialogue often estimates beliefs accurately but fails to systematically use those beliefs during utterance generation. The authors aim to bridge this gap by formalizing core dialogue acts and linking belief estimation directly to constrained generation.

Method: The authors propose BEDA, a framework comprising a world set, a belief estimator, and a conditional generator. They formalize two strategic dialogue acts—Adversarial and Alignment—and operationalize them via probabilistic constraints that guide what an agent may generate based on inferred beliefs.

Result: BEDA consistently outperforms strong baselines: it improves success rate by ≥5.0 points on CKBG (up to 20.6 with GPT-4.1-nano), achieves +9.3 average improvement on Mutual Friends, and secures optimal deals on CaSiNo.

Conclusion: Casting belief estimation as probabilistic constraints offers a simple yet general mechanism for reliable strategic dialogue, effectively bridging belief estimation and action-oriented generation.

Abstract: Strategic dialogue requires agents to execute distinct dialogue acts, for which belief estimation is essential. While prior work often estimates beliefs accurately, it lacks a principled mechanism to use those beliefs during generation. We bridge this gap by first formalizing two core acts Adversarial and Alignment, and by operationalizing them via probabilistic constraints on what an agent may generate. We instantiate this idea in BEDA, a framework that consists of the world set, the belief estimator for belief estimation, and the conditional generator that selects acts and realizes utterances consistent with the inferred beliefs. Across three settings, Conditional Keeper Burglar (CKBG, adversarial), Mutual Friends (MF, cooperative), and CaSiNo (negotiation), BEDA consistently outperforms strong baselines: on CKBG it improves success rate by at least 5.0 points across backbones and by 20.6 points with GPT-4.1-nano; on Mutual Friends it achieves an average improvement of 9.3 points; and on CaSiNo it achieves the optimal deal relative to all baselines. These results indicate that casting belief estimation as constraints provides a simple, general mechanism for reliable strategic dialogue.

</details>


### [56] [Adaptive Dependency-aware Prompt Optimization Framework for Multi-Step LLM Pipeline](https://arxiv.org/abs/2512.24933)
*Minjun Zhao,Xinyu Zhang,Shuai Zhang,Deyang Li,Ruifeng Shi*

Main category: cs.CL

TL;DR: ADOPT is a novel framework for optimizing prompts in multi-step LLM pipelines by modeling inter-step dependencies and using Shapley-based adaptive optimization, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Multi-step LLM pipelines rely on carefully crafted prompts at each step, but jointly optimizing these prompts is challenging due to missing step-level supervision and complex inter-step dependencies, leading to unstable or suboptimal results with current end-to-end methods.

Method: ADOPT introduces an Adaptive Dependency-aware Prompt Optimization framework that models the dependency of each LLM step on the final outcome to estimate precise textual gradients. It decouples gradient estimation from updates, reducing the problem to single-prompt optimizations, and uses a Shapley-value-based mechanism to adaptively allocate optimization effort.

Result: Experiments on real-world datasets and varied pipeline architectures demonstrate that ADOPT consistently outperforms state-of-the-art prompt optimization baselines in both effectiveness and robustness.

Conclusion: ADOPT provides a principled and effective solution for prompt optimization in multi-step LLM pipelines by explicitly accounting for step dependencies and enabling stable, adaptive optimization.

Abstract: Multi-step LLM pipelines invoke large language models multiple times in a structured sequence and can effectively solve complex tasks, but their performance heavily depends on the prompts used at each step. Jointly optimizing these prompts is difficult due to missing step-level supervision and inter-step dependencies. Existing end-to-end prompt optimization methods struggle under these conditions and often yield suboptimal or unstable updates. We propose ADOPT, an Adaptive Dependency-aware Prompt Optimization framework for multi-step LLM pipelines. ADOPT explicitly models the dependency between each LLM step and the final task outcome, enabling precise text-gradient estimation analogous to computing analytical derivatives. It decouples textual gradient estimation from gradient updates, reducing multi-prompt optimization to flexible single-prompt optimization steps, and employs a Shapley-based mechanism to adaptively allocate optimization resources. Experiments on real-world datasets and diverse pipeline structures show that ADOPT is effective and robust, consistently outperforming state-of-the-art prompt optimization baselines.

</details>


### [57] [Classifying long legal documents using short random chunks](https://arxiv.org/abs/2512.24997)
*Luis Adrián Cabrera-Diego*

Main category: cs.CL

TL;DR: 提出了一种基于DeBERTa V3和LSTM的法律文档分类器，通过随机选取48个短文本块（每块最多128个token）进行输入，并利用Temporal构建了可靠的部署流水线；模型加权F得分为0.898，CPU上每100个文件的中位处理时间为498秒。


<details>
  <summary>Details</summary>
Motivation: 法律文档通常包含专业术语且篇幅较长，直接将全文输入基于Transformer的模型进行分类存在不可行、成本高或速度慢的问题。

Method: 采用DeBERTa V3与LSTM结合的架构，输入为从文档中随机抽取的48个短文本块（每个最多128个token），并使用Temporal实现耐用、可靠的部署流水线。

Result: 最佳模型在测试中取得0.898的加权F-score，部署于CPU的流水线处理100个文件的中位时间为498秒。

Conclusion: 所提方法有效缓解了长法律文档分类中的计算瓶颈，在保持较高分类性能的同时实现了稳定可扩展的部署流程。

Abstract: Classifying legal documents is a challenge, besides their specialized vocabulary, sometimes they can be very long. This means that feeding full documents to a Transformers-based models for classification might be impossible, expensive or slow. Thus, we present a legal document classifier based on DeBERTa V3 and a LSTM, that uses as input a collection of 48 randomly-selected short chunks (max 128 tokens). Besides, we present its deployment pipeline using Temporal, a durable execution solution, which allow us to have a reliable and robust processing workflow. The best model had a weighted F-score of 0.898, while the pipeline running on CPU had a processing median time of 498 seconds per 100 files.

</details>


### [58] [MAMA-Memeia! Multi-Aspect Multi-Agent Collaboration for Depressive Symptoms Identification in Memes](https://arxiv.org/abs/2512.25015)
*Siddhant Agarwal,Adya Dhuler,Polly Ruhnke,Melvin Speisman,Md Shad Akhtar,Shweta Yadav*

Main category: cs.CL

TL;DR: 该论文提出了一种名为 MAMAMemeia 的多智能体多角度讨论框架，用于通过 meme 检测社交媒体用户的抑郁症状，并引入了包含大语言模型生成与人工标注解释的数据集 RESTOREx。MAMAMemeia 在 macro-F1 上比当前最优方法提升了 7.55%，成为新的基准。


<details>
  <summary>Details</summary>
Motivation: 随着 meme 被越来越多地用于表达抑郁情绪，亟需有效方法识别其中所体现的抑郁症状。现有方法在准确性和可解释性方面存在不足，因此作者旨在构建一个基于临床心理学理论、具备高解释性的新框架。

Method: 作者提出了 MAMAMemeia 框架，该框架基于认知分析疗法（Cognitive Analytic Therapy, CAT）能力模型，采用多智能体协作机制，从多个角度对 meme 进行分析；同时构建了名为 RESTOREx 的资源库，包含由大语言模型生成并经人工标注的解释，用于支持抑郁症状检测。

Result: MAMAMemeia 在 macro-F1 指标上相较当前最优方法提升了 7.55%，并在与超过 30 种方法的对比中确立为新基准。

Conclusion: 结合临床心理学理论与多智能体架构的 MAMAMemeia 框架显著提升了 meme 中抑郁症状识别的性能，RESTOREx 数据集为该任务提供了重要支持，推动了社交媒体心理健康分析的发展。

Abstract: Over the past years, memes have evolved from being exclusively a medium of humorous exchanges to one that allows users to express a range of emotions freely and easily. With the ever-growing utilization of memes in expressing depressive sentiments, we conduct a study on identifying depressive symptoms exhibited by memes shared by users of online social media platforms. We introduce RESTOREx as a vital resource for detecting depressive symptoms in memes on social media through the Large Language Model (LLM) generated and human-annotated explanations. We introduce MAMAMemeia, a collaborative multi-agent multi-aspect discussion framework grounded in the clinical psychology method of Cognitive Analytic Therapy (CAT) Competencies. MAMAMemeia improves upon the current state-of-the-art by 7.55% in macro-F1 and is established as the new benchmark compared to over 30 methods.

</details>


### [59] [Modeling Language as a Sequence of Thoughts](https://arxiv.org/abs/2512.25026)
*Nasim Borazjanizadeh,James McClelland*

Main category: cs.CL

TL;DR: 本文提出Thought Gestalt (TG)模型，一种在词元和句子级别“思想”状态两个抽象层次上建模语言的循环Transformer。TG通过交叉注意力机制利用先前句子的表征记忆来生成当前句子的词元，并使用单一的下一个词元交叉熵目标进行训练。实验表明，TG在数据和参数效率上优于GPT-2等基线模型，并能有效缓解关系方向性错误（如反转诅咒）。


<details>
  <summary>Details</summary>
Motivation: 当前的Transformer语言模型主要依赖表层的共现统计信息，无法形成对实体和事件的全局一致的潜在表征，这导致了其在关系方向性（如反转诅咒）、上下文错误和数据效率低下等方面存在缺陷。受认知科学中人类理解语言时会将输入流转换为紧凑、持久的事件式表征这一观点的启发，作者旨在构建一个能同时在词元和更高层次语义单元上建模的模型。

Method: 作者提出了Thought Gestalt (TG)模型，这是一种循环Transformer。该模型在两个抽象层次上运作：词元（token）和句子级别的“思想”（thought）状态。TG一次生成一个句子的词元，同时通过交叉注意力机制访问一个存储了先前句子表征的记忆库。关键创新在于，词元和句子表征由同一套模型参数生成，并通过单一的下一个词元交叉熵损失进行端到端训练。通过保留写入记忆的句子表征的计算图，未来词元损失的梯度可以通过交叉注意力反向传播，从而优化生成早期句子向量的参数。

Result: 在扩展性实验中，TG模型在匹配的GPT-2及其他基线模型上持续展现出更高的效率。缩放拟合结果表明，GPT-2需要多出约5-8%的数据和33-42%的参数才能达到与TG相当的损失水平。此外，TG在父子关系反转诅咒探针任务上显著减少了关系方向性泛化错误。

Conclusion: Thought Gestalt模型通过引入句子级别的“思想”状态并将其与词元级建模统一在一个端到端的框架内，有效地提升了语言模型的数据和参数效率，并增强了其在关系推理等需要全局一致性的任务上的鲁棒性。这验证了借鉴人类认知中多层次表征机制对于改进语言模型的有效性。

Abstract: Transformer language models can generate strikingly natural text by modeling language as a sequence of tokens. Yet, by relying primarily on surface-level co-occurrence statistics, they fail to form globally consistent latent representations of entities and events, lack of which contributes to brittleness in relational direction (e.g., reversal curse), contextualization errors, and data inefficiency. On the other hand, cognitive science shows that human comprehension involves converting the input linguistic stream into compact, event-like representations that persist in memory while verbatim form is short-lived. Motivated by this view, we introduce Thought Gestalt (TG) model, a recurrent Transformer that models language at two levels of abstraction - tokens and sentence-level "thought" states. TG generates the tokens of one sentence at a time while cross-attending to a memory of prior sentence representations. In TG, token and sentence representations are generated using the same set of model parameters and trained with a single objective, the next-token cross-entropy: by retaining the computation graph of sentence representations written to memory, gradients from future token losses flow backward through cross-attention to optimize the parameters generating earlier sentence vectors. In scaling experiments, TG consistently improves efficiency over matched GPT-2 runs, among other baselines, with scaling fits indicating GPT-2 requires ~5-8% more data and ~33-42% more parameters to match TG's loss. TG also reduces errors on relational direction generalization on a father-son reversal curse probe.

</details>


### [60] [AdaGReS:Adaptive Greedy Context Selection via Redundancy-Aware Scoring for Token-Budgeted RAG](https://arxiv.org/abs/2512.25052)
*Chao Peng,Bin Wang,Zhilei Long,Jinfang Sheng*

Main category: cs.CL

TL;DR: AdaGReS is a redundancy-aware context selection framework for RAG that jointly optimizes relevance and diversity under a token budget, using an adaptive trade-off parameter and greedy selection with theoretical near-optimality guarantees.


<details>
  <summary>Details</summary>
Motivation: Standard top-k retrieval in RAG often yields redundant or near-duplicate context chunks, which waste token budgets and harm generation quality; there is a need for a method that balances relevance and redundancy without manual tuning.

Method: AdaGReS formulates a set-level objective combining query-chunk relevance and intra-set redundancy penalties, performs greedy selection under a token-budget constraint using marginal gains, and introduces a closed-form, instance-adaptive calibration of the relevance-redundancy trade-off parameter. The method is supported by a theoretical analysis showing epsilon-approximate submodularity under practical conditions.

Result: Experiments on Natural Questions and a biomedical drug corpus show AdaGReS consistently reduces redundancy, improves context quality, and enhances end-to-end answer accuracy and robustness across different settings.

Conclusion: AdaGReS effectively addresses redundancy in RAG context selection by adaptively balancing relevance and diversity under token constraints, offering both empirical gains and theoretical guarantees.

Abstract: Retrieval-augmented generation (RAG) is highly sensitive to the quality of selected context, yet standard top-k retrieval often returns redundant or near-duplicate chunks that waste token budget and degrade downstream generation. We present AdaGReS, a redundancy-aware context selection framework for token-budgeted RAG that optimizes a set-level objective combining query-chunk relevance and intra-set redundancy penalties. AdaGReS performs greedy selection under a token-budget constraint using marginal gains derived from the objective, and introduces a closed-form, instance-adaptive calibration of the relevance-redundancy trade-off parameter to eliminate manual tuning and adapt to candidate-pool statistics and budget limits. We further provide a theoretical analysis showing that the proposed objective exhibits epsilon-approximate submodularity under practical embedding similarity conditions, yielding near-optimality guarantees for greedy selection. Experiments on open-domain question answering (Natural Questions) and a high-redundancy biomedical (drug) corpus demonstrate consistent improvements in redundancy control and context quality, translating to better end-to-end answer quality and robustness across settings.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [61] [Hojabr: Towards a Theory of Everything for AI and Data Analytics](https://arxiv.org/abs/2512.23925)
*Amir Shaikhha*

Main category: cs.DB

TL;DR: Hojabr 是一种统一的声明式中间语言，融合关系代数、张量代数与约束推理，以支持跨数据库、图计算和机器学习系统的统一优化与互操作。


<details>
  <summary>Details</summary>
Motivation: 当前数据分析流水线中关系查询、图处理和张量计算各自为政，系统碎片化导致重复优化、互操作性差以及逻辑抽象与物理执行策略割裂。

Method: 提出 Hojabr 语言，基于高阶代数框架统一表达连接、聚合、张量缩并和递归计算，并将物理执行选择（如连接算法、稀疏/稠密表示）建模为约束特化决策；同时支持与现有声明式语言的双向翻译。

Result: Hojabr 能显式表达语义、结构和代数属性，并在整个编译栈中支持扩展，从而在数据库系统、机器学习框架和编译器基础设施之间实现优化技术的系统化复用与推理。

Conclusion: Hojabr 通过统一的代数框架和约束驱动的物理执行机制，有效弥合了不同计算范式之间的鸿沟，为跨领域系统优化提供了新路径。

Abstract: Modern data analytics pipelines increasingly combine relational queries, graph processing, and tensor computation within a single application, but existing systems remain fragmented across paradigms, execution models, and research communities. This fragmentation results in repeated optimization efforts, limited interoperability, and strict separation between logical abstractions and physical execution strategies.
  We propose Hojabr as a unified declarative intermediate language to address this problem. Hojabr integrates relational algebra, tensor algebra, and constraint-based reasoning within a single higher-order algebraic framework, in which joins, aggregations, tensor contractions, and recursive computations are expressed uniformly. Physical choices, such as join algorithms, execution models, and sparse versus dense tensor representations, are handled as constraint-specialization decisions rather than as separate formalisms. Hojabr supports bidirectional translation with existing declarative languages, enabling programs to be both lowered into Hojabr for analysis and optimization and lifted back into their original declarative form. By making semantic, structural, and algebraic properties explicit, and by supporting extensibility across the compilation stack, Hojabr enables systematic reasoning and reuse of optimization techniques across database systems, machine learning frameworks, and compiler infrastructures.

</details>


### [62] [High-dimensional Regret Minimization](https://arxiv.org/abs/2512.24078)
*Junyu Liao,Ashwin Lall,Mitsunori Ogihara,Raymond Wong*

Main category: cs.DB

TL;DR: FHDR is a novel interactive query framework that drastically reduces both computation time (<0.01s) and user interactions (<30 rounds) for high-dimensional multi-criteria decision making, outperforming existing methods by orders of magnitude.


<details>
  <summary>Details</summary>
Motivation: Existing interactive query algorithms do not scale well to high-dimensional datasets (e.g., housing or financial markets with hundreds of attributes), often requiring excessive user interactions (over 1000 rounds) or failing computationally.

Method: The authors propose FHDR (Fast High-Dimensional Reduction), a new framework designed to efficiently learn user preferences through limited interaction rounds while maintaining low computational overhead in high-dimensional spaces.

Result: Experiments show FHDR achieves at least one order of magnitude improvement in execution time and several orders of magnitude reduction in required interactions compared to state-of-the-art algorithms.

Conclusion: FHDR establishes a new state of the art for scalable interactive regret minimization in high-dimensional multi-criteria decision-making scenarios, effectively addressing the limitations of prior approaches.

Abstract: Multi-criteria decision making in large databases is very important in real world applications. Recently, an interactive query has been studied extensively in the database literature with the advantage of both the top-k query (with limited output size) and the skyline query (which does not require users to explicitly specify their preference function). This approach iteratively asks the user to select the one preferred within a set of options. Based on rounds of feedback, the query learns the implicit preference and returns the most favorable as a recommendation.
  However, many modern applications in areas like housing or financial product markets feature datasets with hundreds of attributes. Existing interactive algorithms either fail to scale or require excessive user interactions (often exceeding 1000 rounds). Motivated by this, we propose FHDR (Fast High-Dimensional Reduction), a novel framework that takes less than 0.01s with fewer than 30 rounds of interaction. It is considered a breakthrough in the field of interactive queries since most, if not all, existing studies are not scalable to high-dimensional datasets.
  Extensive experiments demonstrate that FHDR outperforms the best-known algorithms by at least an order of magnitude in execution time and up to several orders of magnitude in terms of the number of interactions required, establishing a new state of the art for scalable interactive regret minimization.

</details>


### [63] [LMG Index: A Robust Learned Index for Multi-Dimensional Performance Balance](https://arxiv.org/abs/2512.24824)
*Yuzhen Chen,Bin Yao*

Main category: cs.DB

TL;DR: 本文提出LMIndex框架及其变体LMG，通过高效的顶层结构和误差阈值训练算法，在查询、更新、稳定性及空间占用等多个维度上显著优于现有索引方法。


<details>
  <summary>Details</summary>
Motivation: 现有学习型索引通常仅优化查询延迟或空间使用等有限目标，忽视了更新效率与稳定性等实际评估维度，且多依赖于对数据分布或工作负载的假设，缺乏在未知或动态场景下的理论保障，限制了其通用性。

Method: 提出LMIndex框架，采用理论上查询/更新复杂度为O(1)（键类型固定时）的顶层结构和实践中接近O(1)的最优误差阈值训练算法；并在此基础上设计LMG变体，引入新颖的间隙分配策略以提升动态工作负载下的更新性能与稳定性。

Result: 实验表明，LMG在批量加载（快8.25倍）、点查询（快1.49倍）、范围查询（比B+树快4.02倍）、读写混合更新（快1.5倍）、稳定性（变异系数低82.59倍）和空间占用（小1.38倍）等方面均达到领先或具有竞争力的性能。

Conclusion: LMG有效打破了现有学习型索引在多维性能指标间的固有折衷，提供了一个平衡且通用的学习索引框架。

Abstract: Index structures are fundamental for efficient query processing on large-scale datasets. Learned indexes model the indexing process as a prediction problem to overcome the inherent trade-offs of traditional indexes. However, most existing learned indexes optimize only for limited objectives like query latency or space usage, neglecting other practical evaluation dimensions such as update efficiency and stability. Moreover, many learned indexes rely on assumptions about data distributions or workloads, lacking theoretical guarantees when facing unknown or evolving scenarios, which limits their generality in real-world systems.
  In this paper, we propose LMIndex, a robust framework for learned indexing that leverages a efficient query/update top-layer structure (theoretically $O(1)$ when the key type is fixed) and a efficient optimal error threshold training algorithm (approach $O(1)$ in practice). Building upon this, we develop LMG (LMIndex with gaps), a variant employing a novel gap allocation strategy to enhance update performance and maintain stability under dynamic workloads. Extensive evaluations show that LMG achieves competitive or leading performance, including bulk loading (up to 8.25$\times$ faster), point queries (up to 1.49$\times$ faster), range queries (up to 4.02$\times$ faster than B+Tree), update (up to 1.5$\times$ faster on read-write workloads), stability (up to 82.59$\times$ lower coefficient of variation), and space usage (up to 1.38$\times$ smaller). These results demonstrate that LMG effectively breaks the multi-dimensional performance trade-offs inherent in state-of-the-art approaches, offering a balanced and versatile framework.

</details>
