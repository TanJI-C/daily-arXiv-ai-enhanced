<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 4]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [AHA: Scalable Alternative History Analysis for Operational Timeseries Applications](https://arxiv.org/abs/2601.04432)
*Harshavardhan Kamarthi,Harshil Shah,Henry Milner,Sayan Sinha,Yan Li,B. Aditya Prakash,Vyas Sekar*

Main category: cs.DB

TL;DR: 该论文提出了一种名为AHA（Alternative History Analytics）的系统，用于高效且精确地支持对高维运维时序数据进行“替代历史分析”（如回溯性异常检测、告警配置实验等）。相比传统方法（如数据仓库、采样、大数据系统），AHA在保证100%分析准确率的同时，可将总拥有成本（计算+存储）降低高达85倍。


<details>
  <summary>Details</summary>
Motivation: 运维系统常需对高维时序数据（如用户QoE指标及其元数据）进行回溯性分析（例如评估不同异常检测算法或告警策略），但现有数据处理方案要么成本高昂，要么无法保证重放准确性。因此，亟需一种兼顾成本效益与保真度的新系统。

Method: AHA系统的设计基于三大关键洞察：1）底层统计量具有可分解性；2）属性值组合所定义的子群体在活跃数量上具有稀疏性；3）现代分析数据库中的聚合操作具备效率结构。利用这些特性，AHA优化了存储与计算，以实现低成本高保真的替代历史分析。

Result: 在多个真实世界数据集及某大型视频分析公司的生产管道案例研究中，AHA在广泛的下游任务中实现了100%的准确性，并将总拥有成本（TCO）降低了高达85倍。

Conclusion: AHA通过结合工作负载特性和现代数据库优势，成功解决了高维运维数据替代历史分析中的成本与准确性难题，为运维数据分析提供了一种高效可靠的解决方案。

Abstract: Many operational systems collect high-dimensional timeseries data about users/systems on key performance metrics. For instance, ISPs, content distribution networks, and video delivery services collect quality of experience metrics for user sessions associated with metadata (e.g., location, device, ISP). Over such historical data, operators and data analysts often need to run retrospective analysis; e.g., analyze anomaly detection algorithms, experiment with different configurations for alerts, evaluate new algorithms, and so on. We refer to this class of workloads as alternative history analysis for operational datasets. We show that in such settings, traditional data processing solutions (e.g., data warehouses, sampling, sketching, big-data systems) either pose high operational costs or do not guarantee accurate replay. We design and implement a system, called AHA (Alternative History Analytics), that overcomes both challenges to provide cost efficiency and fidelity for high-dimensional data. The design of AHA is based on analytical and empirical insights about such workloads: 1) the decomposability of underlying statistics; 2) sparsity in terms of active number of subpopulations over attribute-value combinations; and 3) efficiency structure of aggregation operations in modern analytics databases. Using multiple real-world datasets and as well as case-studies on production pipelines at a large video analytics company, we show that AHA provides 100% accuracy for a broad range of downstream tasks and up to 85x lower total cost of ownership (i.e., compute + storage) compared to conventional methods.

</details>


### [2] [Structural Indexing of Relational Databases for the Evaluation of Free-Connex Acyclic Conjunctive Queries](https://arxiv.org/abs/2601.04757)
*Cristian Riveros,Benjamin Scheidt,Nicole Schweikardt*

Main category: cs.DB

TL;DR: 本文提出了一种基于数据库内部结构对称性的新型索引结构，通过构建辅助数据库 $D_{col}$，可在预处理时间与 $D_{col}$ 大小成线性关系的前提下，对任意自由连接无环合取查询（fc-ACQ）实现常数延迟枚举或计数；$D_{col}$ 的大小由关系着色细化算法决定，在许多典型结构（如正则图、二叉树）下远小于原始数据库。


<details>
  <summary>Details</summary>
Motivation: 现有基于值或顺序的索引方法（如B+树）无法有效利用数据库元组间的结构性对称性，限制了fc-ACQ查询的评估效率。作者旨在开发一种能利用此类内部结构对称性的索引机制，以在理论上和实践中提升查询性能，尤其是在 $D_{col}$ 远小于原始数据库 $D$ 的情形下。

Method: 引入一个基于Scheidt与Schweikardt（2025）提出的“关系着色细化”（relational color refinement）技术构建的辅助数据库 $D_{col}$ 作为索引核心。该方法将数据库中的元组根据其结构对称性进行着色，并将着色结果压缩为 $D_{col}$。预处理阶段在线性于 $|D_{col}|$ 的时间内完成，之后可对任意fc-ACQ实现常数延迟的答案枚举或计数。

Result: 对于任意fc-ACQ，可在 $O(|D_{col}|)$ 预处理时间后实现常数延迟枚举或计数；$|D_{col}|$ 在最坏情况下为 $O(|D|)$，但在具有丰富结构对称性的数据库（如正则图、二叉树）中可降至常数或对数级别，从而显著优于传统索引方法的复杂度下限。

Conclusion: 该工作首次建立了利用数据库内部结构性对称性进行索引的理论基础，证明了通过关系着色细化构建的 $D_{col}$ 可支持所有fc-ACQ的高效评估，且在多种实际数据分布下具有优于线性复杂度的潜力。

Abstract: We present an index structure to boost the evaluation of free-connex acyclic conjunctive queries (fc-ACQs) over relational databases. The main ingredient of the index associated with a given database $D$ is an auxiliary database $D_{col}$. Our main result states that for any fc-ACQ $Q$ over $D$, we can count the number of answers of $Q$ or enumerate them with constant delay after a preprocessing phase that takes time linear in the size of $D_{col}$.
  Unlike previous indexing methods based on values or order (e.g., B+ trees), our index is based on structural symmetries among tuples in a database, and the size of $D_{col}$ is related to the number of colors assigned to $D$ by Scheidt and Schweikardt's "relational color refinement" (2025). In the particular case of graphs, this coincides with the minimal size of an equitable partition of the graph. For example, the size of $D_{col}$ is logarithmic in the case of binary trees and constant for regular graphs. Even in the worst-case that $D$ has no structural symmetries among tuples at all, the size of $D_{col}$ is still linear in the size of $D$.
  Given that the size of $D_{col}$ is bounded by the size of $D$ and can be much smaller (even constant for some families of databases), our index is the first foundational result on indexing internal structural symmetries of a database to evaluate all fc-ACQs with performance potentially strictly smaller than the database size.

</details>


### [3] [LGTD: Local-Global Trend Decomposition for Season-Length-Free Time Series Analysis](https://arxiv.org/abs/2601.04820)
*Chotanansub Sophaken,Thanadej Rattanakornphan,Piyanon Charoenpoonpanich,Thanapol Phungtua-eng,Chainarong Amornbunchornvej*

Main category: cs.DB

TL;DR: LGTD is a season-length-free time series decomposition method that models trend and emergent seasonal structure through global and adaptive local trends, enabling robust performance on irregular or drifting periodic patterns without manual parameter tuning.


<details>
  <summary>Details</summary>
Motivation: Existing seasonal-trend decomposition methods often require user-specified or estimated season lengths and assume stable periodicity, which limits their robustness and applicability in heterogeneous real-world time series with drifting, intermittent, or multi-scale recurring patterns.

Method: LGTD decomposes a time series into a smooth global trend, adaptive local trends (via an error-driven module called AutoTrend that segments the detrended signal into piecewise-linear regimes), and a residual. Seasonality emerges implicitly from repeated local trend patterns rather than being explicitly modeled with a fixed period.

Result: Experiments on synthetic benchmarks show LGTD achieves robust and balanced decomposition performance across scenarios with fixed, transitive, and variable season lengths—particularly outperforming period-based methods when periodicity is unstable or weak. The algorithm scales linearly with time series length under mild conditions.

Conclusion: LGTD provides a practical, low-touch decomposition framework that eliminates the need for season-length specification and effectively handles complex, real-world time series with irregular or evolving seasonal structures.

Abstract: Time series decomposition into trend, seasonal structure, and residual components is a core primitive for downstream analytics such as anomaly detection, change-point detection, and forecasting. However, most existing seasonal-trend decomposition methods rely on user-specified or estimated season lengths and implicitly assume stable periodic structure. These assumptions limit robustness and deployability in large, heterogeneous collections where recurring patterns may drift, appear intermittently, or exist at multiple time scales.
  We propose LGTD (Local-Global Trend Decomposition), a season-length-free decomposition framework that represents a time series as the sum of a smooth global trend, adaptive local trends whose recurrence induces implicit (emergent) seasonal structure, and a residual component. Rather than explicitly modeling seasonality through a fixed or estimated period, LGTD treats seasonal structure as an emergent property arising from repeated local trend regimes. Concretely, LGTD first estimates a global trend capturing long-term evolution, then applies AutoTrend, an adaptive error-driven local linear trend inference module, to segment the detrended signal into short-lived piecewise-linear regimes. Residuals are obtained after removing both global and local trends.
  By eliminating manual season-length specification, LGTD supports automated, low-touch deployment across time series with irregular, drifting, or weakly periodic structure. We analyze computational complexity and show that LGTD scales linearly with series length under mild conditions. Experiments on synthetic benchmarks demonstrate robust and balanced decomposition performance across fixed, transitive, and variable season-length settings, especially where period-based methods degrade.

</details>


### [4] [Rule Rewriting Revisited: A Fresh Look at Static Filtering for Datalog and ASP](https://arxiv.org/abs/2601.05108)
*Philipp Hanisch,Markus Krötzsch*

Main category: cs.DB

TL;DR: 该论文重新审视并推广了1986年由Kifer和Lozinskii提出的Datalog静态过滤方法，将其扩展至回答集编程（ASP），并提出可处理的近似算法以应对一般情形下双指数复杂度的问题，在典型场景中可显著提升规则系统性能。


<details>
  <summary>Details</summary>
Motivation: 尽管静态过滤作为一种数据无关的Datalog优化方法早在1986年就被提出，但近年来在研究和系统开发中被忽视，其特例甚至被独立重新发现。因此，有必要回顾原始方法，并用现代术语和更通用的过滤谓词加以更新，同时探索其在ASP中的适用性。

Method: 作者使用更新后的术语和更具表达力的过滤谓词重新表述原始静态过滤方法，并将其推广到回答集编程（ASP）；针对其高计算复杂度（一般为双指数，有界元数下单指数），提出可高效计算的近似算法。

Result: 所提出的扩展方法在表达能力上严格优于经典方法，但复杂度更高；而所设计的近似算法在典型实际数据上仍能生成显著优化的逻辑程序，性能提升可达一个数量级。

Conclusion: 静态过滤方法值得在现代逻辑编程系统中重新关注，通过合理的近似策略可在保持实用性的同时获得显著性能收益。

Abstract: Static filtering is a data-independent optimisation method for Datalog, which generalises algebraic query rewriting techniques from relational databases. In spite of its early discovery by Kifer and Lozinskii in 1986, the method has been overlooked in recent research and system development, and special cases are being rediscovered independently. We therefore recall the original approach, using updated terminology and more general filter predicates that capture features of modern systems, and we show how to extend its applicability to answer set programming (ASP). The outcome is strictly more general but also more complex than the classical approach: double exponential in general and single exponential even for predicates of bounded arity. As a solution, we propose tractable approximations of the algorithm that can still yield much improved logic programs in typical cases, e.g., it can improve the performance of rule systems over real-world data in the order of magnitude.

</details>
