<div id=toc></div>

# Table of Contents

- ["query optimization"](#"query optimization") [Total: 1]
- [Ziniu Wu](#Ziniu Wu) [Total: 3]
- [cs.DB](#cs.DB) [Total: 5]
- [learned "cost model"](#learned "cost model") [Total: 5]


<div id='"query optimization"'></div>

# "query optimization" [[Back]](#toc)

### [1] [From Legacy EDW to Hybrid Cloud: Modernizing ETL/ELT for Risk, Finance, and Regulatory Reporting](https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Ravi-Kumar-Vallemoni/publication/399689967_From_Legacy_EDW_to_Hybrid_Cloud_Modernizing_ETLELT_for_Risk_Finance_and_Regulatory_Reporting/links/6966d3e0abecff2489eae8bb/From-Legacy-EDW-to-Hybrid-Cloud-Modernizing-ETL-ELT-for-Risk-Finance-and-Regulatory-Reporting.pdf&hl=zh-CN&sa=X&d=8328488653903060191&ei=JKhradSyL6yK6rQPm4H2iQs&scisig=AHkA5jQllf-f9-RzBdDWoTcwmdO7&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=0&folt=kw-top)
*RK Vallemoni*

Main category: "query optimization"

TL;DR: 本文提出了一种将传统企业数据仓库（EDW）从批处理ETL架构逐步迁移至混合云ELT平台的现代化蓝图，通过领域驱动的数据产品、增量式“绞杀者模式”迁移、显式数据契约和兼容性保障机制，在确保审计性与合规性的前提下显著降低延迟与成本。


<details>
  <summary>Details</summary>
Motivation: 传统EDW架构基于批处理ETL、紧耦合模式和集中治理，难以应对日益复杂的监管要求、近实时风险可视性需求及高昂基础设施成本；同时，新兴的混合云、对象存储、分布式查询引擎等技术为向ELT和去中心化数据架构转型提供了可能，但组织在迁移过程中面临向后兼容、审计限制和操作风险等挑战。

Method: 提出一种结合领域驱动数据产品、ELT下推转换、控制平面编排和显式数据契约的现代化框架，采用“绞杀者模式”进行增量迁移，强调向后兼容的模式设计、可对账性、回滚安全机制和受控的切换计划，并辅以迁移路线图、成本/性能指标和正式风险登记册。

Result: 实证表明，该方法可将报告延迟降低40%以上，计算支出减少高达35%，同时提升对监管场景的响应能力，且不损害数据完整性或系统韧性。

Conclusion: 通过结构化的混合云ELT现代化路径，组织可在维持严格治理与审计合规的前提下，显著提升报告效率、SLA达标率并降低总体拥有成本，验证了渐进式架构演进在高度监管行业中的可行性与优势。

Abstract: The use of Enterprise Data Warehouse (EDWs) has been experienced as the analytical backbone of risk management, financial reporting and regulatory reporting of the data in very regulated sectors like banking, insurance, and capital markets. They were based on batch-oriented Extract Transform Load (ETL) paradigms, tight coupled schema and monolithic governance models that are better suited to stability than agility. Nevertheless, the increasing regulatory complexity, impacts of the near-real time risk visibility requirements, and increasing cost of infrastructure have emanated inherent weaknesses of the legacy EDW architectures. At the same time, the emergence of hybrid cloud platforms, scalable object storage, distributed query engines, and workflow orchestration system has made it possible to make the paradigm shift toward Extract–Load–Transform (ELT), domain-driven data products, and decentralized ownership models. In spite of these developments, in numerous organizations, the pressure to modernize reporting pipes based on strong backward compatibility criteria, audit limitations and the operational risks of massive data migrations makes this a challenge. This paper gives a detailed blueprint of modernization in the process of moving the old EDW centric ETL architectures to the hybrid cloud ELT platforms to suit the risk, finance, and regulatory reporting. Its proposed solution integrates domain-driven data products and ELT pushdown transformations orchestrating control planes and explicit data contracts that is applied in an incremental fashion with a strangler pattern. The framework focuses on retrogressively compatible schemas, reconcilability determinacy, the rollback safety nets, and regulated cutover plans to provide continuous regulatory compliance. Using a wellorganized migration roadmap, cost and performance metrics and an official risk register, the paper will show how organizations can shorten report delivery cycles, enhance service-level agreement (SLA) compliance and minimize the overall cost of ownership without sacrificing auditability and strict governance. The findings have shown that hybrid cloud ELT systems may cut the latency in report by more than 40%, cut compute expenditure by up to 35, and become much more responsive to regulatory cases without infection of information integrity or resilience.

</details>


<div id='Ziniu Wu'></div>

# Ziniu Wu [[Back]](#toc)

### [2] [Markovian Pre-Trained Transformer for Next-Item Recommendation](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.08275&hl=zh-CN&sa=X&d=4748469596494327208&ei=3NlqaYuJI7ux6rQPmKbEyA4&scisig=AHkA5jSFQdKBtVneTXYrtYrgD9ma&oi=scholaralrt&hist=Pxo5FIAAAAAJ:11734074576040036222:AHkA5jT4qH64J8nZejvQlyvZo1xT&html=&pos=0&folt=rel)
*C Xu,G Li,J Wang,W Zhang*

Main category: Ziniu Wu

TL;DR: 提出了基于马尔可夫链预训练的通用推荐模型MPT，仅需微调轻量适配器即可在多个真实数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有先进序列推荐模型实际上主要依赖用户最近一次交互进行预测，历史行为仅用于辅助推断用户非序列化的身份特征，因此需要一种能有效聚焦最新交互并具备通用性和可迁移性的推荐模型。

Method: 在合成马尔可夫链上对Transformer模型进行完全预训练（Markovian Pre-trained Transformer, MPT），使其学习从上下文中估计状态转移概率并关注最后一个状态；随后在下游任务中仅微调一个轻量级适配器。

Result: 在来自三个平台的五个公开数据集上，MPT显著优于传统推荐预训练方法和近期语言模型预训练范式。

Conclusion: 马尔可夫预训练是一种高效且可扩展的通用推荐模型构建策略，其成功源于对序列推荐中“马尔可夫性”的本质洞察。

Abstract: We introduce the Markovian Pre-trained Transformer (MPT) for next-item recommendation, a transferable model fully pre-trained on synthetic Markov chains, yet capable of achieving state-of-the-art performance by fine-tuning a lightweight adaptor. This counterintuitive success stems from the observation of the `Markovian' nature: advanced sequential recommenders coincidentally rely on the latest interaction to make predictions, while the historical interactions serve mainly as auxiliary cues for inferring the user's general, non-sequential identity. This characteristic necessitates the capabilities of a universal recommendation model to effectively summarize the user sequence, with particular emphasis on the latest interaction. MPT inherently has the potential to be universal and transferable. On the one hand, when trained to predict the next state of Markov chains, it acquires the capabilities to estimate transition probabilities from the context (one adaptive manner for summarizing sequences) and attend to the last state to ensure accurate state transitions. On the other hand, unlike the heterogeneous interaction data, an unlimited amount of controllable Markov chains is available to boost the model capacity. We conduct extensive experiments on five public datasets from three distinct platforms to validate the superiority of Markovian pre-training over traditional recommendation pre-training and recent language pre-training paradigms.

</details>


### [3] [Serverless Elasticsearch: the Transition from Stateful to Stateless](https://scholar.google.com/scholar_url?url=https://www.cs.ubc.ca/~brendan/papers/serverless-elasticsearch.pdf&hl=zh-CN&sa=X&d=4663664700620309864&ei=3NlqaYuJI7ux6rQPmKbEyA4&scisig=AHkA5jQ42xzoAOOsQlkP2_egdEN7&oi=scholaralrt&hist=Pxo5FIAAAAAJ:11734074576040036222:AHkA5jT4qH64J8nZejvQlyvZo1xT&html=&pos=1&folt=rel)
*I Psaroudakis,P Salehi,J Bryan,FF Castaño,B Cully…*

Main category: Ziniu Wu

TL;DR: 本文提出了一种面向Elasticsearch的新型无服务器架构，通过将计算与存储解耦并将数据卸载至高可用对象存储，简化了原有的多层数据架构为仅索引和搜索两层，在保持相同API和读写语义的同时显著提升了索引吞吐量并支持近乎无限的数据规模。


<details>
  <summary>Details</summary>
Motivation: 传统Elasticsearch采用有状态的共享无架构，计算与存储紧密耦合，导致需要维护多个硬件配置层级（如hot、warm、cold、frozen）以平衡成本、性能和可用性，管理复杂且扩展受限。

Method: 设计一种无状态的无服务器架构，将索引文件、事务日志和集群元数据等数据卸载到高可用、低成本的对象存储中；引入自定义批提交格式封装索引数据以降低上传开销，并对事务日志进行批量上传；同时实现对象存储中的文件删除机制。

Result: 实验表明，在相同计算硬件条件下，该无服务器架构相比传统有状态Elasticsearch可实现高达2倍的索引吞吐量，并能线性扩展以匹配数据摄入负载；索引数据上传成本最多降低100倍，事务日志上传成本最多降低30倍。

Conclusion: 通过将计算与存储解耦并利用云对象存储，Elasticsearch可实现更简化的两层架构、更高的可扩展性和更低的存储成本，同时保持兼容性和强一致性语义。

Abstract: Elasticsearch (ES) is a distributed search and analytics database consisting of a cluster of nodes, each hosting a disjoint subset of the data. Until recently ES had a shared-nothing architecture that relied on local disks to store cluster data such as index files, transaction logs, and cluster state metadata. In this stateful architecture, compute is coupled with storage, and this leads to different tiers (eg, hot, warm, cold, and frozen) of hardware and configurations that the administrator chooses from to balance cost, performance, and high availability. In this paper, we show the technical design of a new serverless architecture, focusing on decoupling compute from storage, and offloading data to an affordable highly available object store, while supporting the same APIs and read-after-write semantics. Specifically, we show why and how this stateless architecture simplifies the data tiers to just two: indexing and search. This architecture supports indexing and searching practically limitless data while scaling each tier independently. Furthermore, we describe how we wrap index data into a custom batch commit format onto the object store to decrease upload costs by up to 100x, how we batch transaction log uploads to decrease upload costs by up to 30x, and how we delete files from the object store. We experimentally show that Serverless ES can get twice the indexing throughput of (stateful) ES on comparable compute hardware by using cloud object storage for durability instead of replication, and can scale linearly to match ingestion workloads.

</details>


### [4] [Automated Data Quality Frameworks for Healthcare Data Lakes](https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Narasimha-Chaitanya-Samineni/publication/399697229_Automated_Data_Quality_Frameworks_for_Healthcare_Data_Lakes/links/69657c35c906f117f2a45653/Automated-Data-Quality-Frameworks-for-Healthcare-Data-Lakes.pdf&hl=zh-CN&sa=X&d=5626713935160258952&ei=3NlqaYuJI7ux6rQPmKbEyA4&scisig=AHkA5jQYqaaNBdGUKxtpksZ-lWCB&oi=scholaralrt&hist=Pxo5FIAAAAAJ:11734074576040036222:AHkA5jT4qH64J8nZejvQlyvZo1xT&html=&pos=2&folt=rel)
*NC Samineni*

Main category: Ziniu Wu

TL;DR: 本文提出了一种面向医疗数据湖的自动化数据质量（ADQ）框架，融合规则与机器学习方法，支持异常检测与治理对齐的评分模型，以应对医疗数据在规模和异构性增长下的质量问题。


<details>
  <summary>Details</summary>
Motivation: 随着医疗数据湖中数据量和异构性的快速增长，传统人工数据质量检查难以有效识别模式漂移、缺失值、临床编码错误、摄入错误及时间相关不一致等问题，亟需可扩展的自动化解决方案。

Method: 提出一个综合的ADQ框架，结合基于规则与机器学习驱动的验证机制，引入异常检测技术，并构建与治理要求对齐的数据质量评分模型；采用元数据驱动设计，强调自动化、合规性和实时操作能力。

Result: 提供了该框架的架构蓝图、大规模数据质量指标表、验证指南及实施建议，适用于医疗机构、支付方和分析平台。

Conclusion: 所提出的ADQ框架能有效提升医疗数据湖的数据质量管理水平，满足数字健康生态系统对高质量、实时、合规数据的持续需求。

Abstract: Healthcare data lakes serve as unified repositories that integrate electronic health records (EHR), claims data, laboratory systems, imaging metadata, device-generated patient streams, and financial information into scalable analytical environments. As these data ecosystems expand in volume and heterogeneity, ensuring data quality becomes increasingly difficult. Manual data quality checks are insufficient for detecting schema drift, missing values, inaccurate clinical codes, ingestion errors, or time-dependent inconsistencies. Automated Data Quality (ADQ) frameworks introduce scalable mechanisms to evaluate, score, and enforce data quality rules across ingestion, transformation, and consumption layers. This research article proposes a comprehensive ADQ framework for healthcare data lakes, integrates rule-based and machine-learning-driven validation, introduces anomaly detection techniques, and formalizes a governance-aligned scoring model. The study contributes an architectural blueprint, large data quality metrics tables, validation guidelines, and implementation recommendations for health systems, payers, and analytics platforms. The framework emphasizes automation, metadata-driven design, regulatory alignment, and real-time operability to meet the evolving data needs of digital health ecosystems.[1][3][6][8][10]

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [5] [Multiverse: Transactional Memory with Dynamic Multiversioning](https://arxiv.org/abs/2601.09735)
*Gaetano Coccimiglio,Trevor Brown,Srivatsan Ravi*

Main category: cs.DB

TL;DR: Multiverse is a new software transactional memory (STM) system that integrates both versioned and unversioned transactions to deliver high performance for common workloads while efficiently supporting long-running reads through multiversioning.


<details>
  <summary>Details</summary>
Motivation: Existing STMs struggle with long-running reads over frequently updated data; multiversioning helps but often degrades performance for regular transactions. There is a need for an STM that supports both types of workloads efficiently without compromising performance.

Method: Multiverse introduces a hybrid STM design that concurrently supports versioned transactions (for long-running reads) and unversioned transactions (for standard workloads), combining the strengths of both approaches within a single system.

Result: Experiments show Multiverse matches or exceeds the performance of state-of-the-art unversioned STMs on standard workloads and dramatically outperforms existing STMs—by several orders of magnitude—in scenarios involving long-running reads with frequent updates.

Conclusion: Multiverse successfully bridges the gap between unversioned and multiversioned STMs, offering high performance across diverse workloads without sacrificing efficiency in either case.

Abstract: Software transactional memory (STM) allows programmers to easily implement concurrent data structures. STMs simplify atomicity. Recent STMs can achieve good performance for some workloads but they have some limitations. In particular, STMs typically cannot support long-running reads which access a large number of addresses that are frequently updated. Multiversioning is a common approach used to support this type of workload. However, multiversioning is often expensive and can reduce the performance of transactions where versioning is not necessary. In this work we present Multiverse, a new STM that combines the best of both unversioned TM and multiversioning. Multiverse features versioned and unversioned transactions which can execute concurrently. A main goal of Multiverse is to ensure that unversioned transactions achieve performance comparable to the state of the art unversioned STM while still supporting fast versioned transactions needed to enable long running reads. We implement Multiverse and compare it against several STMs. Our experiments demonstrate that Multiverse achieves comparable or better performance for common case workloads where there are no long running reads. For workloads with long running reads and frequent updates Multiverse significantly outperforms existing STMS. In several cases for these workloads the throughput of Multiverse is several orders of magnitude faster than other STMs.

</details>


### [6] [The "I" in FAIR: Translating from Interoperability in Principle to Interoperation in Practice](https://arxiv.org/abs/2601.10008)
*Evan Morris,Gaurav Vaidya,Phil Owen,Jason Reilly,Karamarie Fecho,Patrick Wang,Yaphet Kebede,E. Kathleen Carter,Chris Bizon*

Main category: cs.DB

TL;DR: 该论文提出了两个工具Babel和ORION，分别解决因标识符方案和数据模型不一致导致的FAIR数据资源在实践中难以互操作的问题，并构建了一个完全互操作的知识库集合。


<details>
  <summary>Details</summary>
Motivation: 尽管许多科学数据资源遵循FAIR原则并采用标准化词汇和元数据结构，但由于标识符方案和数据模型的差异，这些资源在实际中仍难以有效互操作。

Method: 开发了两个工具：Babel通过生成经过人工审核的标识符映射，构建等价标识符团簇并通过高性能API提供服务；ORION则将不同知识库摄入并转换为统一的、社区管理的数据模型。

Result: 成功利用Babel和ORION构建了一个完全互操作的知识库库，并公开提供下载与使用（https://robokop.renci.org）。

Conclusion: Babel和ORION有效弥合了FAIR原则下数据互操作性理论与实践之间的鸿沟，为实现真正可互操作的科学数据生态系统提供了可行方案。

Abstract: The FAIR (Findable, Accessible, Interoperable, and Reusable) data principles [1] promote the interoperability of scientific data by encouraging the use of persistent identifiers, standardized vocabularies, and formal metadata structures. Many resources are created using vocabularies that are FAIR-compliant and well-annotated, yet the collective ecosystem of these resources often fails to interoperate effectively in practice. This continued challenge is mainly due to variation in identifier schemas and data models used in these resources. We have created two tools to bridge the chasm between interoperability in principle and interoperation in practice. Babel solves the problem of multiple identifier schemes by producing a curated set of identifier mappings to create cliques of equivalent identifiers that are exposed through high-performance APIs. ORION solves the problems of multiple data models by ingesting knowledge bases and transforming them into a common, community-managed data model. Here, we describe Babel and ORION and demonstrate their ability to support data interoperation. A library of fully interoperable knowledge bases created through the application of Babel and ORION is available for download and use at https://robokop.renci.org.

</details>


### [7] [Redundancy-Driven Top-$k$ Functional Dependency Discovery](https://arxiv.org/abs/2601.10130)
*Xiaolong Wan,Xixian Han*

Main category: cs.DB

TL;DR: SDP is a novel algorithm for efficiently discovering the top-$k$ functional dependencies (FDs) in relational databases by ranking them based on redundancy count and leveraging pruning techniques to avoid exhaustive search.


<details>
  <summary>Details</summary>
Motivation: Existing FD discovery algorithms suffer from high computational cost and produce excessively large result sets, making them impractical for large-scale or high-dimensional data and hindering the identification of truly useful FDs.

Method: The authors propose SDP (Selective-Discovery-and-Prune), which ranks FDs by redundancy count—a measure of duplicated information—and uses a monotonic upper bound on redundancy to prune unpromising search branches. Three optimizations are introduced: attribute ordering by partition cardinality, use of a Partition Cardinality Matrix for tighter bounds, and a global scheduler to prioritize promising branches.

Result: Experiments on over 40 datasets demonstrate that SDP significantly outperforms exhaustive FD discovery methods in both speed and memory usage while effectively identifying the most relevant FDs.

Conclusion: SDP provides an efficient and scalable solution for selective FD discovery by focusing on top-$k$ dependencies ranked by redundancy, addressing both performance and usability challenges of traditional approaches.

Abstract: Functional dependencies (FDs) are basic constraints in relational databases and are used for many data management tasks. Most FD discovery algorithms find all valid dependencies, but this causes two problems. First, the computational cost is prohibitive: computational complexity grows quadratically with the number of tuples and exponentially with the number of attributes, making discovery slow on large-scale and high-dimensional data. Second, the result set can be huge, making it hard to identify useful dependencies. We propose SDP (Selective-Discovery-and-Prune), which discovers the top-$k$ FDs ranked by redundancy count. Redundancy count measures how much duplicated information an FD explains and connects directly to storage overhead and update anomalies. SDP uses an upper bound on redundancy to prune the search space. It is proved that this upper bound is monotone: adding attributes refines partitions and thus decreases the bound. Once the bound falls below the top-$k$ threshold, the entire branch can be skipped. We improve SDP with three optimizations: ordering attributes by partition cardinality, using pairwise statistics in a Partition Cardinality Matrix to tighten bounds, and a global scheduler to explore promising branches first. Experiments on over 40 datasets show that SDP is much faster and uses less memory than exhaustive methods.

</details>


### [8] [Improving Database Performance by Application-side Transaction Merging](https://arxiv.org/abs/2601.10596)
*Xueyuan Ren,Frank Li,Yang Wang*

Main category: cs.DB

TL;DR: 该论文提出了一种通过在应用端合并结构相似的SQL语句或事务来提升事务处理性能的新方法，并设计了名为TransactionMerger的中间件，在TPC-C和Spree应用中分别实现了最高2.65倍和3.52倍的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 传统事务处理系统在面对大量结构相似但独立提交的事务时，存在冗余操作和资源争用问题，限制了整体吞吐性能。作者旨在通过在应用侧识别并合并这类事务，减少冗余读取、利用SQL语义优化语句，并预计算跨事务的聚合效果，从而提升系统效率。

Method: 作者提出了TransactionMerger中间件，用于跨客户端收集并合并结构相似的事务；同时开发了静态分析工具，以在不违反事务隔离性的前提下识别可合并的机会。具体技术包括：1）基于SQL语义合并相似语句；2）消除冗余读操作；3）跨事务合并冲突语句并通过预计算其聚合效应实现优化。

Result: 在TPC-C基准测试中，事务合并使吞吐量提升最高达2.65倍；在真实电商应用Spree中，吞吐量提升高达3.52倍，验证了所提方法的有效性。

Conclusion: 通过在应用层对结构相似事务进行合并优化，可以在不修改数据库内核的前提下显著提升事务处理性能，且该方法适用于真实应用场景，具有良好的实用价值和可扩展性。

Abstract: This paper explores a new opportunity to improve the performance of transaction processing at the application side by merging structurely similar statements or transactions. Concretely, we re-write transactions to 1) merge similar statements using specific SQL semantics; 2) eliminate redundant reads; and 3) merge contending statements across transactions by pre-computing their aggregated effect. Following this idea, we present the design of TransactionMerger, a middleware to collect and merge transactions across different clients. We further present a static analysis tool to identify the merging opportunity without violating isolation as well as our experience of re-writing transactions in TPC-C and Spree, a popular real-world application. Our evaluation shows that such transaction merging can improve TPC-C throughput by up to 2.65X and Spree throughput by 3.52X.

</details>


### [9] [Translating database mathematical schemes into relational database software applications with MatBase](https://arxiv.org/abs/2601.10604)
*Christian Mancas,Diana Christina Mancas*

Main category: cs.DB

TL;DR: 本文提出了一种将初等数学数据模型转换为关系模型及非关系约束的伪代码算法，并证明其高效、可靠、完备且最优，同时以家谱树子宇宙为例进行了应用，并提供了SQL和VBA实现示例。


<details>
  <summary>Details</summary>
Motivation: 为支持智能数据库管理系统MatBase，需将数学数据模型自动转换为可实施的关系数据库结构及相应的非关系约束，以提升建模效率与系统可靠性。

Method: 设计并形式化一种伪代码算法，用于将Elementary Mathematical Data Model方案翻译为关系模式及配套的非关系约束集；通过理论分析证明算法的快速性、健壮性、完备性和最优性；并通过家谱树子宇宙案例进行验证。

Result: 成功实现了从数学模型到关系数据库的自动转换，验证了算法的有效性，并提供了SQL与VBA代码样例及开发指南，用于实施非关系约束。

Conclusion: 所提出的算法在理论和实践上均表现优异，能有效支撑MatBase系统中复杂语义约束的自动化实现，为智能数据库建模提供了可靠工具。

Abstract: We present a pseudocode algorithm for translating our (Elementary) Mathematical Data Model schemes into relational ones and associated sets of non-relational constraints, used by MatBase, our intelligent database management system prototype. We prove that this algorithm is very fast, solid, complete, and optimal. We apply it to a mathematical scheme modeling the genealogical trees subuniverse. We also provide examples of SQL and VBA code for enforcing some of its non-relational constraints, as well as guidelines to develop code for enforcing such constraints.

</details>


<div id='learned "cost model"'></div>

# learned "cost model" [[Back]](#toc)

### [10] [Pre-optimization-based deep reinforcement learning for autonomous economic and exergy management of stochastic H2/CO ratio syngas switching downstream …](https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0959652626000995&hl=zh-CN&sa=X&d=3631195359197376550&ei=JKhraYHnM-qsieoPgq2L-Ac&scisig=AHkA5jTA4IHzxteoB97L8R3wsJkO&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=2&folt=kw-top)
*Y Sun,Z Liu,Z Cui,W Wang,W Tian*

Main category: learned "cost model"

TL;DR: 本文提出了一种基于预优化的深度强化学习（DRL）控制框架，用于随机性合成气切换下游（SSSD）过程的自主经济与㶲（2E）管理，在降低㶲损和运行成本的同时，提升了产品控制的稳定性与智能适应能力。


<details>
  <summary>Details</summary>
Motivation: 由于全球变暖和能源需求上升，可持续合成气生产成为关键清洁能源载体。然而，下游过程中存在的非稳态、随机性及模型-装置失配等问题，使得传统控制策略和运行管理面临挑战，亟需智能化解决方案。

Method: 首先构建高保真SSSD动态模型以模拟系统随机行为与动态损失；随后将优化约束算法嵌入DRL框架，为每次随机重置生成优化的初始动作集；最后通过智能体与动态模型交互训练，实现高效探索与奖励最大化，从而获得最优控制策略。

Result: 所提方法相比PID和传统DRL分别减少3.8%和2.1%的㶲损，实现最低运行成本8542美元，并提供最稳定的产品控制；训练后的智能体还能高效自适应过程不确定性，提升探索效率与稳定性。

Conclusion: 该预优化DRL框架有效实现了SSSD过程的智能、可持续运行，在复杂动态环境下兼顾经济性与能效，为高随机性化工过程的自主控制提供了新范式。

Abstract: Driven by global warming and escalating energy demand, sustainable syngas production continues to advance as an essential cleaner energy carrier. The switching of the syngas H2/CO ratio can supply tailored feedstock for synthesizing diverse downstream chemicals. However, unsteady state, stochastic behavior, and plant-model mismatch characteristics of the downstream process complicate existing control strategies and operational management. This study addresses the critical challenge of intelligent operational management in stochastic syngas switching downstream (SSSD) processes characterized by strong stochasticity and dynamic losses. To this end, a pre-optimization-based deep reinforcement learning (DRL) control framework is proposed for autonomous economic and exergy (2E) management of SSSD process. First, a high-fidelity dynamic model of SSSD process is developed to simulate the stochastic behavior and dynamic losses of the system. Then, the optimization constraint algorithm is innovatively embedded into the DRL framework to obtain an optimized initial set of actions for each random reset. Finally, the agent is trained to interact with the dynamic model in pursuit of efficient exploration and reward, which produces an optimal control strategy. The proposed method reduces exergy destruction by 3.8 % and 2.1 % compared to proportional-integral-derivative (PID) and conventional DRL frameworks, while obtaining the minimum operating cost of $8542 and the most stable product control. The well-trained agent is also able to self-adapt to process uncertainties with higher exploration efficiency and stability, achieving intelligent and sustainable production for complex and dynamic processes.

</details>


### [11] [Optimizing Enterprise Cloud Infrastructure Using Predictive Analytics and Machine Learning Algorithms](https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Dharmasena-Sd/publication/399721332_Optimizing_Enterprise_Cloud_Infrastructure_Using_Predictive_Analytics_and_Machine_Learning_Algorithms/links/696676625cc49c35ce7ceac7/Optimizing-Enterprise-Cloud-Infrastructure-Using-Predictive-Analytics-and-Machine-Learning-Algorithms.pdf&hl=zh-CN&sa=X&d=6381765822969601704&ei=JKhraYHnM-qsieoPgq2L-Ac&scisig=AHkA5jRCM8AKWFXCqFL7E4LvFLPj&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=3&folt=kw-top)
*O Pandey*

Main category: learned "cost model"

TL;DR: 本文综述了如何通过预测分析与机器学习（包括监督学习、无监督学习和强化学习）实现云基础设施的主动治理，提出一个基于实时遥测数据的AI驱动架构，以在成本、性能与可用性之间取得平衡，并展望绿色计算与量子加速等未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统被动式多云与混合云基础设施管理已无法应对日益复杂的环境，亟需转向主动、自适应的治理模式以提升业务价值并降低运维开销。

Method: 综合运用监督学习（用于工作负载预测）、无监督学习（用于异常检测）和强化学习（用于自主扩缩容），构建一个理论化的云资源管理生命周期框架及AI驱动的实时自愈架构。

Result: 所提出的架构能有效利用实时遥测数据实现自我修复，同时识别并应对数据真实性、模型漂移和ML引擎计算开销等关键技术挑战。

Conclusion: 预测性优化是实现云基础设施向主动、自调整资产转型的核心机制，可最大化业务价值并最小化运营支出，为实现全面云自治提供战略路径。

Abstract: The escalating complexity of multi-cloud and hybrid enterprise environments has rendered traditional, reactive infrastructure management obsolete. This review article investigates the transformation of cloud governance through the integration of predictive analytics and machine learning (ML) algorithms. We evaluate how supervised learning for workload forecasting, unsupervised learning for anomaly detection, and reinforcement learning for autonomous scaling address the competing priorities of cost, performance, and availability. The study details a theoretical framework for the cloud resource management lifecycle and proposes an AI-driven architecture that utilizes real-time telemetry data to execute self-healing remediations. Furthermore, we address critical technical constraints, including data veracity, model drift, and the computational overhead of ML engines. By exploring future trajectories such as green computing optimization and quantum-accelerated resource allocation, this article provides a strategic roadmap for organizations aiming to achieve total cloud autonomy. Ultimately, we demonstrate that predictive optimization is the essential mechanism for transforming cloud infrastructure into a proactive, self-adjusting asset that delivers maximum business value with minimal operational expenditure.

</details>


### [12] [A robust and ensemble greenhouse model for enhancing yield of tomato crops](https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s41870-025-02854-w&hl=zh-CN&sa=X&d=5220920798550479413&ei=JKhraYHnM-qsieoPgq2L-Ac&scisig=AHkA5jR0SuCzuS6bw_a9p324SYhH&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=4&folt=kw-top)
*T George Princess,E Poovammal*

Main category: learned "cost model"

TL;DR: 本文提出一种结合软演员-评论家（SAC）与Q学习的集成强化学习方法，通过引入离散随机化与Dropout模块，有效提升温室控制中的样本效率与鲁棒性，从而提高作物产量与净利润。


<details>
  <summary>Details</summary>
Motivation: 随着全球人口增长，自主温室对保障粮食供给至关重要，但现有温室优化策略在样本效率和鲁棒性方面存在不足，尤其是在真实环境中模拟耗时、训练数据有限的情况下，亟需更高效的优化方法。

Method: 作者将软演员-评论家（SAC）与Q学习算法融合构建集成模型，并引入离散随机化和Dropout机制以优化最差情况下的低效样本；该问题被建模为Mismatch马尔可夫决策优化问题。

Result: 实验表明，所提方法在样本效率和鲁棒性方面优于现有方法，并成功提升了温室作物产量与净收益。

Conclusion: 结合SAC与Q学习的集成强化学习框架能有效解决自主温室控制中的样本效率与鲁棒性挑战，为智能农业提供了一种高效可行的优化方案。

Abstract: The usage of autonomous greenhouses has become essential in meeting the food demands of the world’s expanding population. Finding the right optimization strategy to sustain growth, yield, and profit is one of the major issues with greenhouse production. Addressing this issue effectively requires a combination of advanced technologies, data-driven insights, and innovative management practices. The automated optimizations, which are often implemented using reinforcement learning algorithms, encounter issues with sample efficiency and robustness due to the time-consuming nature of the real-world simulation. Therefore, the goal of this research is to solve these issues by combining the soft actor critic (SAC) and Q learning algorithms to create an ensemble method. To properly optimize the worst-case inefficient samples, a discrete randomization and dropout module is included. The problem of sample efficiency and resilience is treated as a Mismatch Markov Decision Optimisation problem. The suggested model outperforms the current methods in handling the robustness and sample efficiency issues, according to an experimental evaluation. Additionally, this improvement increased production and maximized net profit.

</details>


### [13] [A Sustainable Computing Approach to Understand Unstructured Paths for a Resource-Constrained System](https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s00354-026-00317-9&hl=zh-CN&sa=X&d=178699044167383675&ei=JKhraYHnM-qsieoPgq2L-Ac&scisig=AHkA5jRESHO5-jTeD1Dtklbi03aw&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=6&folt=kw-top)
*L Wang,J Wu,S Wang,H Wei*

Main category: learned "cost model"

TL;DR: 本文提出了一种无需训练和相机标定的可持续方法，利用低成本单目相机在资源受限的野外环境中理解非结构化路径、进行三维重建并规划可行走路线。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于关注检测或分割等特定任务，忽视了在计算、内存和能源资源极度受限的野外环境中对能耗、成本与复杂度的可持续性需求。

Method: 通过单目相机检测边缘线，提取具有相对完整性的轮廓候选曲线；基于几何方向连续性和覆盖度对候选曲线进行筛选，获得近似轮廓作为可行走区域；进一步分析不同可行走轮廓间的相对几何约束，实现非结构化路径的三维理解、重建与路径规划。

Result: 实验从能耗、成本和交互比等多个维度评估表明，该方法不仅能有效理解非结构化路径，而且具备低功耗、低成本和轻量化的可持续优势，更适合资源受限的野外系统。

Conclusion: 所提方法无需先验训练和相机内参标定，在保证功能的同时显著提升了在资源受限场景下的可持续性，为野外非结构化环境感知提供了一种实用解决方案。

Abstract: Unstructured scene understanding in real-world environments remains difficult, such as how to understand unstructured paths in field environments using resource-constrained systems with extremely limited computational, memory, and energy resources. Traditional methods are too focused on the specific goal of detection or segmentation, and seriously neglect the sustainability of energy consumption, cost and complexity. In this study, we present a sustainable method to understand unstructured paths and reconstruct them in 3D space and plan walkable routes through a low-cost monocular camera. Edges lines are detected. Contour candidate curves are extracted by the relative integrity of lines. Based on geometric orientation continuity and coverage, candidates are refined to get approximate contours, which are regarded as walkable regions. By analyzing the relative geometric constraints between different walkable contours, unstructured paths can be understood and reconstructed in 3D space and walkable routes can be planned. This method requires no prior training, and no calibration of the internal parameters of monocular camera, which has the sustainability of low power consumption and low cost. We provide a comprehensive evaluation of the method in terms of multiple dimensions such as energy consumption, cost and interaction ratio. The results show that the method not only can understand unstructured paths, but also its sustainability of low power, low cost, and lightweight are more suitable for a resource-constrained system in field environments.

</details>


### [14] [Data-driven sustainability assessment of advanced manufacturing processes](https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s00170-025-17185-0&hl=zh-CN&sa=X&d=15689676441249919366&ei=JKhraYHnM-qsieoPgq2L-Ac&scisig=AHkA5jRSGtIFLyNpW3NhfR3L7dRd&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=7&folt=kw-top)
*MU Farooq,S Deng*

Main category: learned "cost model"

TL;DR: 本文综述了用于量化单元制造过程可持续性性能的数据驱动与模型驱动方法，重点分析了生命周期评估（LCA）与自下而上的参数化建模方法，并提出一个整合多准则决策与人工智能算法的资源感知型可持续性评估框架。


<details>
  <summary>Details</summary>
Motivation: 单元制造过程常面临高生产率与低环境影响、低成本及高质量之间的多目标冲突，现有研究多聚焦特定案例，缺乏系统性整合，亟需统一的可持续性评估方法。

Method: 综述并比较了三类单元制造过程（增材、减材、成形）中应用的可持续性评估方法，包括生命周期评估（LCA）、基于过程参数的自下而上建模，以及与多准则决策和人工智能算法的融合策略。

Result: 发现LCA虽能有效量化环境影响，但依赖耗时且需专用工具的清单编制；而参数化建模更适合复杂系统并支持灵活的情景分析。结合数据驱动方法可提升决策支持能力。

Conclusion: 应构建一个整合参数化建模、多准则决策与AI技术的综合性数据驱动框架，以实现对制造可持续性的资源感知型评估。

Abstract: Unit manufacturing processes often involve conflicting objectives: achieving high throughput and productivity while minimizing environmental impacts, reducing costs, and ensuring product quality. In this context, computational methods are instrumental in deriving quantitative indicators from process data and supporting data-driven decision-making. Computational, including optimization-based, methods are commonly employed to quantify and compare the performance of alternative processes. However, most existing studies focus on specific use cases, resulting in fragmented insights. Therefore, this paper surveys state-of-the-art data-and model-driven methods used to quantify sustainability performance in unit manufacturing processes. Specifically, we review sustainability evaluation methods applied to three major categories of unit manufacturing processes: additive, subtractive, and formative. We highlight that life cycle assessment (LCA) is effective for quantitative environmental system assessment; however, compiling reliable life cycle inventories is time-consuming and often requires proprietary software tools. In contrast, bottom-up parametric modeling approaches, which describe system behavior based on individual process parameters, have proved effective in evaluating complex systems and enabling flexible what-if analyses. The paper further investigates the integration of these modeling methods with data-driven analytical approaches such as multi-criteria decision-making and artificial intelligence–based algorithms for extracting actionable information. Based on this review, we propose a holistic data-driven framework for resource-informed assessment of manufacturing sustainability.

</details>
