<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 6]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [From Metadata to Meaning: A Semantic Units Knowledge Graph for the Biodiversity Exploratories](https://arxiv.org/abs/2601.00002)
*Tarek Al Mustafa*

Main category: cs.DB

TL;DR: 该论文提出“语义单元”（Semantic Units, SUs）作为知识图谱中具有语义意义的命名子图，以提升生态与生物多样性研究者对知识图谱的认知互操作性，并通过在德国Biodiversity Exploratories项目元数据构建的知识图谱上首次实现SUs，探索其对查询的影响；同时展示了利用大语言模型和嵌入模型从文献标题/摘要中提取结构化元数据并增强FAIR特性的方法。


<details>
  <summary>Details</summary>
Motivation: 知识图谱（KGs）虽在生态与生物多样性研究中具有整合数据、支持推理和机器互操作的潜力，但其SPARQL查询语言对多数用户不友好，且KG的技术建模需求常与终端用户的语义需求脱节，导致大量图谱内容对用户缺乏语义价值。

Method: 作者基于德国Biodiversity Exploratories项目的出版物与数据集元数据构建了一个知识图谱，并在其上首次实现“语义单元”（SUs）——即具有语义显著性的命名子图；此外，采用大语言模型（LLMs）从标题和摘要中抽取结构化元数据类别，并利用嵌入模型为元数据注入潜在语义信息，以支持FAIR原则。

Result: 成功实现了语义单元在真实知识图谱中的构建与应用，初步验证了SUs可提升用户与KG之间的认知互操作性，并展示了LLM与嵌入模型在自动化生成结构化、FAIR兼容元数据方面的有效性。

Conclusion: 语义单元为弥合知识图谱技术需求与用户语义需求之间的鸿沟提供了一种可行路径，结合大语言模型和嵌入技术可有效支持生态学领域知识图谱的构建与使用，推动研究数据的FAIR化。

Abstract: Knowledge Graphs (KGs) bear great potential for ecology and biodiversity researchers in their ability to support synthesis and integration efforts, meta-analyses, reasoning tasks, and overall machine interoperability of research data. However, this potential is yet to be realized as KGs are notoriously difficult to interact with via their query language SPARQL for many user groups alike. Additionally, a further hindrance for user-KG interaction is the fundamental disconnect between user requirements and requirements KGs have to fulfill regarding machine-interoperability, reasoning tasks, querying, and further technical requirements. Thus, many statements in a KG are of no semantic significance for end users. In this work, we investigate a potential remedy for this challenge: Semantic Units (SUs) are semantically significant, named subgraphs in a KG with the goal to enhance cognitive interoperability for users, and to provide responses to common KG modelling challenges. We model and construct a KG from publication and dataset metadata of the Biodiversity Exploratories (BE), a research platform for functional biodiversity research across research plots in Germany to contribute to biodiversity research from the perspective of computer science. We contribute further by delivering the first implementation of semantic units on a knowledge graph and investigate how SUs impact KG querying. Finally, we present two implementations of tasks that show how large language models (LLMs) can be used to extract structured metadata categories from publication and dataset titles and abstracts, and how embedding models can be used to enrich metadata with latent information, in an effort to support the creation of structured and FAIR (findable, accessible, interoperable, and reusable) metadata.

</details>


### [2] [Database Theory in Action: Yannakakis' Algorithm](https://arxiv.org/abs/2601.00098)
*Paraschos Koutris,Stijn Vansummeren,Qichen Wang,Yisu Remy Wang,Xiangyao Yu*

Main category: cs.DB

TL;DR: 本文简要综述了近期在提升Yannakakis算法实用性方面的进展，包括效率和实现简易性，并指出了若干未来研究方向。


<details>
  <summary>Details</summary>
Motivation: Yannakakis的经典算法虽在理论上对无环连接是最优的，但因实际性能不佳而未被广泛采用。

Method: 综述近期改进Yannakakis算法实用性的技术进展。

Result: 总结了提高该算法效率与实现简易性的关键方法。

Conclusion: 尽管已有显著进展，Yannakakis算法的广泛应用仍需进一步研究，文中提出了多个潜在研究方向。

Abstract: Yannakakis' seminal algorithm is optimal for acyclic joins, yet it has not been widely adopted due to its poor performance in practice. This paper briefly surveys recent advancements in making Yannakakis' algorithm more practical, in terms of both efficiency and ease of implementation, and points out several avenues for future research.

</details>


### [3] [Avoiding Thread Stalls and Switches in Key-Value Stores: New Latch-Free Techniques and More](https://arxiv.org/abs/2601.00208)
*David Lomet,Rui Wang*

Main category: cs.DB

TL;DR: 该论文提出了一种名为“notices”的新型无锁机制，通过结合增量记录更新显著减少线程切换或停顿带来的性能损耗，尤其适用于B树索引维护等场景。


<details>
  <summary>Details</summary>
Motivation: 传统基于锁（latch-based）的方法在高并发下因资源争用导致线程阻塞，而现有无锁（latch-free）技术虽可避免阻塞，却常因执行大量无效工作而收益有限。因此，亟需一种既能避免线程切换/停顿，又能减少浪费性计算的高效并发控制机制。

Method: 作者提出“notices”这一新型无锁方法，结合delta record updating（增量记录更新）机制，在B树索引维护等操作中实现高效并发控制，从而避免线程阻塞并最小化无效工作。

Result: 所提方法能有效解决B树索引维护中的并发问题，在高负载下显著降低因线程切换或停顿造成的性能开销，并减少无锁机制中常见的浪费性计算。

Conclusion: Notices作为一种新颖的无锁技术，能够高效支持高并发键值存储系统中的索引维护操作，在避免线程阻塞的同时大幅减少无效工作，为提升系统整体性能提供了可行路径。

Abstract: A significant impediment to high performance in key-value stores is the high cost of thread switching or stalls. While there are many sources for this, a major one is the contention for resources. And this cost increases with load as conflicting operations more frequently try to access data concurrently. Traditional latch-based approaches usually handle these situations by blocking one or more contending threads. Latch-free techniques can avoid this behavior. But the payoff may be limited if latch-free techniques require executing wasted work. In this paper, we show how latch-free techniques exploit delta record updating and can significantly reduce wasted work by using notices, a new latch-free approach. This paper explains how notices work and can solve B-tree index maintenance problems, while avoiding thread switches or stalls. Other opportunities for avoiding thread switches or stalls are also discussed.

</details>


### [4] [Combining Time-Series and Graph Data: A Survey of Existing Systems and Approaches](https://arxiv.org/abs/2601.00304)
*Mouna Ammar,Marvin Hofer,Erhard Rahm*

Main category: cs.DB

TL;DR: 本文综述了图数据与时序数据融合的现有方法与系统，将其划分为四类架构，并分析了各类系统在跨模型集成度、成熟度和开放性等方面的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前图数据与时序数据的联合建模需求日益增长，但缺乏对现有融合系统及其设计权衡的系统性梳理，亟需一个全面的分类与评估框架以指导选择与开发。

Method: 对现有图与时序数据融合系统进行系统性调研，依据架构特征将其划分为四类，并从跨模型集成程度、系统成熟度、开放性等维度进行对比分析。

Result: 明确了四类主流架构及其在统一支持图与时序数据方面的实现特点与适用场景，揭示了不同系统在关键需求满足上的差异与取舍。

Conclusion: 该综述为研究者和实践者提供了理解、评估和选择图与时序数据融合系统的基础，有助于根据具体需求权衡不同方案的优劣。

Abstract: We provide a comprehensive overview of current approaches and systems for combining graphs and time series data. We categorize existing systems into four architectural categories and analyze how these systems meet different requirements and exhibit distinct implementation characteristics to support both data types in a unified manner. Our overview aims to help readers understand and evaluate current options and trade-offs, such as the degree of cross-model integration, maturity, and openness.

</details>


### [5] [KELP: Robust Online Log Parsing Through Evolutionary Grouping Trees](https://arxiv.org/abs/2601.00633)
*Satyam Singh,Sai Niranjan Ramachandran*

Main category: cs.DB

TL;DR: KELP is a novel real-time log parser based on an Evolutionary Grouping Tree that continuously adapts to schema drifts via online clustering, outperforming static heuristic parsers on a new, more realistic benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing online log parsers use static template models that break silently under minor schema changes in dynamic production environments, causing missed alerts and operational overhead.

Method: KELP employs a new data structure called the Evolutionary Grouping Tree, which dynamically clusters incoming logs by evolving its structure—splitting, merging, and re-evaluating nodes based on real-time frequency distributions.

Result: KELP achieves high parsing accuracy on a newly introduced benchmark that captures realistic structural ambiguity in production logs, significantly outperforming traditional heuristic parsers while maintaining high throughput.

Conclusion: KELP effectively addresses the brittleness of current log parsers through continuous adaptation, offering a robust solution for real-time log analysis in dynamic environments, supported by a more representative evaluation benchmark.

Abstract: Real-time log analysis is the cornerstone of observability for modern infrastructure. However, existing online parsers are architecturally unsuited for the dynamism of production environments. Built on fundamentally static template models, they are dangerously brittle: minor schema drifts silently break parsing pipelines, leading to lost alerts and operational toil. We propose \textbf{KELP} (\textbf{K}elp \textbf{E}volutionary \textbf{L}og \textbf{P}arser), a high-throughput parser built on a novel data structure: the Evolutionary Grouping Tree. Unlike heuristic approaches that rely on fixed rules, KELP treats template discovery as a continuous online clustering process. As logs arrive, the tree structure evolves, nodes split, merge, and re-evaluate roots based on changing frequency distributions. Validating this adaptability requires a dataset that models realistic production complexity, yet we identify that standard benchmarks rely on static, regex-based ground truths that fail to reflect this. To enable rigorous evaluation, we introduce a new benchmark designed to reflect the structural ambiguity of modern production systems. Our evaluation demonstrates that KELP maintains high accuracy on this rigorous dataset where traditional heuristic methods fail, without compromising throughput. Our code and dataset can be found at codeberg.org/stonebucklabs/kelp

</details>


### [6] [DeXOR: Enabling XOR in Decimal Space for Streaming Lossless Compression of Floating-point Data](https://arxiv.org/abs/2601.00695)
*Chuanyi Lv,Huan Li,Dingyu Yang,Zhongle Xie,Lu Chen,Christian S. Jensen*

Main category: cs.DB

TL;DR: DeXOR is a novel compression framework for streaming floating-point numbers that leverages decimal XOR to encode common prefixes and suffixes, achieving higher compression ratios and faster decompression than existing methods while remaining robust under extreme conditions.


<details>
  <summary>Details</summary>
Motivation: Streaming floating-point data is increasingly common, yet existing compression techniques struggle with high-precision values or lack of smoothness between consecutive numbers. There is a need for a method that effectively exploits redundancy while handling binary-decimal conversion errors and extreme data conditions.

Method: DeXOR introduces a decimal XOR procedure to encode longest common prefixes and suffixes in decimal space. It uses scaled truncation with error-tolerant rounding, optimized bit management strategies for decimal XOR, and a robust exception handler for floating-point exponents to ensure accurate, low-cost decompression and stability.

Result: Evaluated on 22 datasets, DeXOR achieves a 15% higher compression ratio and 20% faster decompression speed compared to state-of-the-art methods, with competitive compression speed, scalability, and robustness in extreme scenarios where other schemes fail.

Conclusion: DeXOR effectively addresses the challenges of compressing streaming floating-point data by combining decimal-space redundancy elimination with error-resilient decompression mechanisms, outperforming existing approaches in both efficiency and robustness.

Abstract: With streaming floating-point numbers being increasingly prevalent, effective and efficient compression of such data is critical. Compression schemes must be able to exploit the similarity, or smoothness, of consecutive numbers and must be able to contend with extreme conditions, such as high-precision values or the absence of smoothness. We present DeXOR, a novel framework that enables decimal XOR procedure to encode decimal-space longest common prefixes and suffixes, achieving optimal prefix reuse and effective redundancy elimination. To ensure accurate and low-cost decompression even with binary-decimal conversion errors, DeXOR incorporates 1) scaled truncation with error-tolerant rounding and 2) different bit management strategies optimized for decimal XOR. Additionally, a robust exception handler enhances stability by managing floating-point exponents, maintaining high compression ratios under extreme conditions. In evaluations across 22 datasets, DeXOR surpasses state-of-the-art schemes, achieving a 15% higher compression ratio and a 20% faster decompression speed while maintaining a competitive compression speed. DeXOR also offers scalability under varying conditions and exhibits robustness in extreme scenarios where other schemes fail.

</details>
