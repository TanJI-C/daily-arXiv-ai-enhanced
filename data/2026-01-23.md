<div id=toc></div>

# Table of Contents

- ["query optimization"](#"query optimization") [Total: 8]
- [learned "cost model"](#learned "cost model") [Total: 8]
- [cs.DB](#cs.DB) [Total: 2]
- [Ziniu Wu](#Ziniu Wu) [Total: 3]


<div id='"query optimization"'></div>

# "query optimization" [[Back]](#toc)

### [1] [Trading determinism for speed: An algebra for large graph analytics using nondeterminism](https://scholar.google.com/scholar_url?url=https://dbdbd2025.uantwerpen.be/wp-content/uploads/2025/12/dbdbd25_18.pdf&hl=zh-CN&sa=X&d=6595967988522896330&ei=JrFxabWCJ7uM6rQP2dOq-Ag&scisig=AHkA5jS8fTpRySjts6TCSRC6peu9&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=0&folt=kw-top)
*AE Bince,R Brijder*

Main category: "query optimization"

TL;DR: 该论文探讨了 wfor-MATLANG 语言中因非确定性 witness 算子所带来的查询优化机会。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用语言构造中的计算自由度，探索在 wfor-MATLANG 中实现查询优化的可能性。

Method: 通过分析 wfor-MATLANG 语言中非确定性 witness 算子的特性，识别并形式化其带来的查询优化机会。

Result: 证明了 wfor-MATLANG 确实存在可被利用的查询优化机会，这源于其非确定性 witness 算子所提供的计算灵活性。

Conclusion: wfor-MATLANG 的非确定性 witness 算子为查询优化提供了理论基础和实践潜力，拓展了矩阵查询语言的优化空间。

Abstract: … language constructs potentially provides opportunities for query optimization by leveraging the freedom in the computation. We show that, indeed, wfor-MATLANG, due to its nondeterministic witness operator, has query optimization opportunities …

</details>


### [2] [A Hybrid Data Structure and Algorithmic Approach for Effi-cient Memory Management and Query Processing in High-Performance Software Systems](https://scholar.google.com/scholar_url?url=https://journal.apjikom.or.id/index.php/PAF/article/download/20/23&hl=zh-CN&sa=X&d=1453314790160932200&ei=JrFxabWCJ7uM6rQP2dOq-Ag&scisig=AHkA5jRgNcP7a5GOjuOoKZPdZdmj&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=1&folt=kw-top)
*Z Zulfikar,FA Prasetya,MA Putri*

Main category: "query optimization"

TL;DR: 本文提出了一种结合多种传统数据结构优势的混合数据结构，通过缓存感知算法、动态内存分配和中间查询结果压缩，在高性能计算环境中同时优化内存使用和查询速度。


<details>
  <summary>Details</summary>
Motivation: 在高性能计算（HPC）环境中，传统数据结构（如B树和哈希表）通常在内存效率与查询性能之间难以兼顾，导致在内存受限系统中表现不佳。因此，亟需一种能同时优化两者的新方法。

Method: 提出一种混合数据结构，融合缓存感知算法、动态内存分配机制以及中间查询结果的压缩技术，并通过多种工作负载下的基准测试，与B树和哈希表等标准结构进行对比评估。

Result: 实验结果显示，该混合结构最多可减少30%的内存开销，同时查询处理速度比传统方法快达1.5倍，并在点查询和范围查询等多种查询类型下均表现出稳健性能。

Conclusion: 该混合数据结构为高性能计算系统提供了一种兼顾内存效率与查询速度的有效解决方案，未来可拓展至分布式系统和新兴计算范式以增强其可扩展性与适应性。

Abstract: In high-performance computing (HPC) environments, the need to balance memory efficiency and query performance is crucial for ensuring optimal system performance. Traditional data structures, such as B-trees and hash tables, often prioritize either memory usage or query speed, leading to suboptimal performance in memory-constrained systems. This paper proposes a hybrid data structure that combines the strengths of multiple traditional data structures to optimize both memory usage and query processing speed. The proposed hybrid structure integrates cache-conscious algorithms, dynamic memory allocation, and compression techniques for intermediate query results. The approach is evaluated through extensive benchmarking tests comparing it to standard data structures like B-trees and hash tables under various workloads. Results show that the hybrid data structure reduces memory overhead by up to 30% while maintaining query processing speeds up to 1.5 times faster than conventional methods. Furthermore, the hybrid structure demonstrates robust performance across different types of queries, including both point and range queries, ensuring versatility and efficiency. The findings indicate that this hybrid approach provides a promising solution for HPC systems, where both memory efficiency and query speed are essential. Future research can explore extending the hybrid structure to distributed systems and emerging technologies, further improving its scalability and adaptability to new computational paradigms.

</details>


### [3] [Performance Tuning of ETL Pipelines in Healthcare Big Data Platforms](https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Arina-Finnegan-2/publication/399857980_Performance_Tuning_of_ETL_Pipelines_in_Healthcare_Big_Data_Platforms/links/696ba62dc906f117f2a67af9/Performance-Tuning-of-ETL-Pipelines-in-Healthcare-Big-Data-Platforms.pdf&hl=zh-CN&sa=X&d=16301326055948312418&ei=JrFxabWCJ7uM6rQP2dOq-Ag&scisig=AHkA5jRJgUgU72v3KQ37i3wwGyXl&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=2&folt=kw-top)
*P Harry,C Jonathan*

Main category: "query optimization"

TL;DR: 该研究通过系统性多策略优化ETL流程，在医疗大数据平台上实现了最高68.7%的作业执行时间减少，显著提升数据集成效率。


<details>
  <summary>Details</summary>
Motivation: 医疗数据（如电子健康记录、可穿戴设备和基因组测序）呈指数级增长，其异构性对数据集成构成重大挑战，亟需高效ETL流程以支持临床与运营分析。

Method: 采用混合方法研究设计，基于大规模模拟医疗数据集，对增量数据抽取、分布式内存处理（Spark）、查询优化和策略性数据分区等优化技术进行实证测试与基准评估。

Result: 整体作业执行时间最多减少68.7%，其中内存计算（Spark）和增量加载策略贡献最为显著。

Conclusion: 本研究提供了一个经验证的ETL性能优化框架，可帮助数据工程师提升医疗数据集成管道的效率、可扩展性与成本效益。

Abstract: The exponential growth of healthcare data from Electronic Health Records (EHRs), wearables, and genomic sequencing presents a formidable challenge for data integration. Efficient Extract, Transform, Load (ETL) pipelines are critical for transforming this heterogeneous data into actionable insights. This study hypothesizes that a systematic, multi-strategy approach to ETL performance tuning can significantly reduce processing time and resource consumption in healthcare big data platforms. We employed a mixed-methods research design, utilizing a largescale, simulated healthcare dataset to empirically test and benchmark a suite of optimization techniques against a baseline ETL workflow. These techniques included incremental data extraction, distributed in-memory processing, query optimization, and strategic data partitioning. The results demonstrated a cumulative performance improvement of up to 68.7% in total job execution time. The most significant gains were achieved through the implementation of in-memory Spark processing and incremental load strategies. This research provides a validated framework for data engineers and architects to enhance the efficiency, scalability, and cost-effectiveness of healthcare data integration pipelines, thereby accelerating clinical and operational analytics.

</details>


### [4] [Breaking the Spiral: A Utility-Driven Optimization Framework for Balanced Information Retrieval in the LLM Era](https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3788865&hl=zh-CN&sa=X&d=8931733612144997941&ei=JrFxabWCJ7uM6rQP2dOq-Ag&scisig=AHkA5jTXJkiRGy0IV0VKX7_1TVKI&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=3&folt=kw-top)
*X Chen,B He,H Lin,X Han,T Wang,B Cao,L Sun…*

Main category: "query optimization"

TL;DR: 该研究揭示了大语言模型（LLM）生成文本在检索系统中反复引入所引发的“沉默螺旋”效应，即人类生成内容可见性下降、信息多样性减少和检索性能退化，并提出了一种名为效用驱动多目标优化（UMO）的框架来有效缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM和RAG系统的广泛应用，LLM生成文本对检索系统的长期影响尚不明确，尤其可能引发“沉默螺旋”现象，导致信息生态同质化。因此，亟需理解并缓解此类负面影响。

Method: 作者构建了一个模拟管道，迭代地将LLM生成文本注入检索系统以观察其累积效应；为缓解该问题，提出UMO框架，包含两个阶段：1）使用NSGA-II算法优化多目标偏好权重；2）将这些权重直接融入检索向量空间，无需重新训练模型。

Result: 实验表明，随着LLM生成文本比例增加，检索系统更倾向于检索此类内容，导致人类生成内容曝光减少、多样性下降、错误传播及检索性能显著降低；而UMO框架能有效维持检索效果、提升人类内容召回比例、抑制LLM内容过度主导并保持信息多样性。

Conclusion: LLM生成文本的反复引入会引发“沉默螺旋”，损害检索系统的多样性和有效性；所提出的UMO框架通过多目标优化与向量空间调整，可在不重训模型的前提下有效缓解该问题，为构建健康的信息检索生态提供新思路。

Abstract: The widespread adoption of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems is reshaping the landscape of information retrieval. However, the long-term effects of LLM-generated texts on retrieval systems remain underexplored, creating challenges for mitigating their impact. The effects are examined in this study, with a particular focus on the “Spiral of Silence” phenomenon, which refers to the marginalization of diverse information as certain types of content dominate, leading to a homogenized information ecosystem. To investigate this, a simulation pipeline is constructed to model the iterative introduction of LLM-generated texts into retrieval systems. Experimental results across multiple iterations reveal that as the presence of LLM-generated texts within the system grows, retrieval systems exhibit a stronger tendency to retrieve these texts. This trend, in turn, reduces the visibility of human-generated content, diminishes diversity, propagates errors, and results in a notable decline in retrieval performance. To address these challenges, we propose a Utility-Driven Multi-Objective Optimization (UMO) framework to effectively mitigate the “Spiral of Silence”. This framework employs a two-phase approach: an optimization phase, leveraging the NSGA-II algorithm to derive optimal preference weights for multiple objectives, and a memorization phase, which directly integrates these weights into the retrieval vector space without requiring additional model retraining. Experimental results demonstrate that this framework maintains stable retrieval effectiveness, improves the retrieval proportion of human-generated content, reduces the excessive influence of LLM-generated texts, and preserves information diversity, effectively mitigating the “Spiral of Silence”.

</details>


### [5] [Graph Algorithms for Everyone, Everywhere](https://scholar.google.com/scholar_url?url=https://dbdbd2025.uantwerpen.be/wp-content/uploads/2025/12/dbdbd25_14.pdf&hl=zh-CN&sa=X&d=4577647052999606463&ei=JrFxabWCJ7uM6rQP2dOq-Ag&scisig=AHkA5jRcGni1-quQQ00ZKLw0VYZl&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=5&folt=kw-top)
*D de Graaf,R Brijder,N Yakovets*

Main category: "query optimization"

TL;DR: GraphAlg 是一种面向图算法的领域特定语言，可编译为关系代数，并已集成到数据库系统中以支持图算法。


<details>
  <summary>Details</summary>
Motivation: 在数据库系统中高效支持图算法仍具挑战，现有方法往往缺乏表达力或性能不佳，因此需要一种既能保持关系代数优势又能方便表达图算法的语言。

Method: 提出 GraphAlg 语言，将其编译为关系代数操作，并将其集成到数据库系统中，利用现有查询优化和执行机制。

Result: 实验表明 GraphAlg 能有效表达多种图算法，并在数据库环境中实现良好的性能，验证了其作为数据库内图算法支持方案的可行性。

Conclusion: GraphAlg 为在关系数据库中高效执行图算法提供了一种实用且可扩展的方法，弥合了图计算与关系数据处理之间的鸿沟。

Abstract: We recently introduced GraphAlg [1], a domain-specific language for graph algorithms that compiles to relational algebra. We demonstrated that GraphAlg is a viable solution to algorithm support in databases by integrating it into the …

</details>


### [6] [Relational Processing of Tensor Programs](https://scholar.google.com/scholar_url?url=https://dbdbd2025.uantwerpen.be/wp-content/uploads/2025/12/dbdbd25_30.pdf&hl=zh-CN&sa=X&d=13091813374176810026&ei=JrFxabWCJ7uM6rQP2dOq-Ag&scisig=AHkA5jT41KbeZITW4e2ALD-0rnFd&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=6&folt=kw-top)
*TM Serrano*

Main category: "query optimization"

TL;DR: 该论文指出，尽管现有张量编译器和关系型查询引擎在处理稀疏张量方面各有优势，但它们在优化通用张量核（tensor kernels）方面存在局限：前者专注于特定核的系统性优化，后者则受限于单一或受限的半环（semiring）假设，无法表达复杂的张量核组合。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的数值数据和关系数据通常以稀疏形式存储，而现有的张量编译器（如TACO、Finch等）主要优化特定的张量核，未充分利用关系型查询引擎固有的稀疏处理能力；同时，基于关系视角的数值计算方法多局限于线性代数和特定半环，难以表达通用张量核的组合。

Method: 论文分析了当前张量编译器与关系型查询引擎在处理稀疏张量时的方法差异，指出二者均未有效结合对方的优势：张量编译器缺乏对任意张量表达式的优化能力，而关系型方法受限于半环假设和表达能力（如FAQ框架仅支持单一点态函数）。

Result: 揭示了现有系统在处理通用张量核时的不足：既无法像关系引擎那样天然支持稀疏计算，又不能像张量编译器那样高效优化复杂核；同时指出FAQ等框架因表达能力受限，无法满足基本张量核组合的需求。

Conclusion: 需要一种融合关系型稀疏处理优势与张量编译器优化能力的新方法，以支持更通用、高效的张量核计算，突破当前系统在表达能力和优化范围上的限制。

Abstract: Diverse data science applications [1, 2, 3, 4, 5, 6, 7, 8] require the ability to operate over numerical arrays that have more than two dimensions, commonly known as tensors. Motivated by this necessity, in recent years there has been an emergence of tensor compilers such as TACO [9], Finch [18], Halide [11] and ExTensor [12], as well as declarative frameworks like Galley [13].
In addition, real-world numerical data is sparse: tensors with predominantly zero entries. Similarly, relational data is stored sparsely: only relevant tuples are materialized. For this reason, relational query engines arise as a natural candidate to process tensor workloads, since they are inherently sparse-oriented, performing computation only over existing tuples. The systems mentioned above extend array programming (eg, NumPy [14] and Py-Torch [15]) to sparse data, supporting arbitrary user-defined pointwise functions and aggregates. With the exception of Galley, these compilers focus on systematically optimizing the computation tensor kernels, not arbitrary tensor expressions. Tensor kernels are common tensor expressions that arise in different contexts and work as building blocks for tensor-based algorithms. Current systems [9, 18, 11, 12, 13] do not attempt to adapt relational processing optimization techniques to the computation of the tensor kernels. On the other hand, many of the solutions that have tackled numerical computations from a relational perspective focus on processing linear algebra computations, and assume that matrix entries and operations belong to an arbitrary semiring [9, 16, 17, 18, 19, 20, 21, 22, 27, 23, 25, 26, 24, 16, 28], and system-oriented linear algebra optimizations [27, 23, 25, 26, 24, 16, 28] assume that entries and functions belong to the usual, but specific, semiring (R,·,+). If we want to go beyond a single semiring, the FAQ [29] framework is an option, but the expressions are restricted to one pointwise funtction, which is insufficient to express even the most basic tensor kernels compositions.

</details>


### [7] [Client-Server Architecture in Distributed Databases](https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/M-Wayahdi/publication/399828907_Client-Server_Architecture_in_Distributed_Databases/links/696a7286c906f117f2a5f742/Client-Server-Architecture-in-Distributed-Databases.pdf&hl=zh-CN&sa=X&d=268279325412125768&ei=JrFxabWCJ7uM6rQP2dOq-Ag&scisig=AHkA5jQzrFiy6u_cHuLC4uc4RT9c&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=7&folt=kw-top)
*MR Wayahdi*

Main category: "query optimization"

TL;DR: 本文探讨了客户端-服务器架构在分布式数据库管理系统（DDBMS）中的演进，从传统的两层模型向多层架构的转变。


<details>
  <summary>Details</summary>
Motivation: 随着现代计算对可扩展性、灵活性和性能的需求不断提升，传统两层客户端-服务器架构在分布式数据库环境中的局限性日益凸显，亟需向更复杂的多层架构演进。

Method: 通过分析传统两层架构的不足，结合多层架构的设计原则，本文系统性地阐述了从两层到多层客户端-服务器架构的转型过程，并可能涉及架构组件划分、通信机制及中间件技术等。

Result: 展示了多层架构在提升系统可维护性、可扩展性和负载均衡方面的优势，并可能通过案例或性能指标验证其在DDBMS中的有效性。

Conclusion: 多层客户端-服务器架构是应对现代分布式数据库系统复杂需求的有效解决方案，代表了该领域的重要发展方向。

Abstract: Client-server architecture serves as the backbone of modern computing, particularly within Distributed Database Management Systems (DDBMS). This paper discusses the transformation of client-server architecture from traditional two-tier models to multi-tier …

</details>


### [8] [Hybrid Database Architecture for Retail Big Data Analytics: PostgreSQL vs MongoDB Performance Analysis](https://scholar.google.com/scholar_url?url=https://ejournal.radenintan.ac.id/index.php/IJECS/article/download/28108/10282&hl=zh-CN&sa=X&d=3095383083580231776&ei=JrFxabWCJ7uM6rQP2dOq-Ag&scisig=AHkA5jTaRkqVNKFQ1q4n3kKdoZLE&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=8&folt=kw-top)
*TFI Noor,E Nugraha,J Maknun,I Kustiawan,E Kurşun*

Main category: "query optimization"

TL;DR: 在零售大数据分析场景中，MongoDB 8.0 查询速度比 PostgreSQL 16 快 28–31%，但 PostgreSQL 在资源效率（CPU、内存、存储 I/O）上高出 13–17%，且存储占用仅为 MongoDB 的 1/6。两者在分析性能与基础设施效率之间存在根本权衡。


<details>
  <summary>Details</summary>
Motivation: 零售业面临海量数据处理挑战，需在分析性能与资源效率之间取得平衡；当前缺乏对主流数据库（如 PostgreSQL 与 MongoDB）在真实零售大数据环境下系统性性能对比的研究。

Method: 采用实验性定量方法，基于大规模真实零售销售与库存数据集，对 PostgreSQL 16 和 MongoDB 8.0 在典型分析型工作负载下的查询性能、资源消耗（CPU、RAM、存储 I/O）及存储效率进行基准测试。

Result: MongoDB 在读密集型分析查询（尤其是大规模聚合操作）中执行速度比 PostgreSQL 快 28–31%；而 PostgreSQL 在资源使用效率上优于 MongoDB 13–17%，且存储需求仅为 MongoDB 的 1/6。

Conclusion: PostgreSQL 与 MongoDB 在零售大数据分析中各具优势：前者强调资源与存储效率，后者侧重查询速度。研究揭示了分析性能与基础设施效率之间的核心权衡，为构建混合数据架构、优化零售大数据系统提供实证依据和决策支持。

Abstract: The rapid growth of retail big data has intensified the challenge of selecting a database architecture that can balance analytical performance and resource efficiency, particularly in data-intensive retail environments. This study aims to conduct a comparative performance analysis between PostgreSQL 16 and MongoDB 8.0 in the context of implementing big data analytics in the retail industry. An experimental quantitative approach is used, utilizing a large-scale, real-world retail sales and inventory dataset to benchmark PostgreSQL 16 and MongoDB 8.0 across a range of representative analytical workloads. Results show MongoDB is 28-31% faster in query processing, but PostgreSQL is 13-17% more efficient in resource usage (CPU, RAM, Storage I/O) and requires 6x less storage. These results indicate that MongoDB consistently achieves faster execution times for read-intensive analytical queries, especially in large-scale aggregation operations. Conversely, PostgreSQL exhibits superior storage efficiency and lower computational resource consumption due to its normalized relational architecture. These findings reveal a fundamental trade-off between analytical speed and infrastructure efficiency in retail big data systems. This research contributes to the development of hybrid data architecture strategies for big data analytics in the retail industry, supporting performance optimization and informed decision-making in data-rich environments

</details>


<div id='learned "cost model"'></div>

# learned "cost model" [[Back]](#toc)

### [9] [Benchmarking Ensemble Learning Approaches for Coronary Artery Disease Classification](https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s44196-025-01147-1&hl=zh-CN&sa=X&d=13200064857938630763&ei=JrFxaZ6PLLui6rQPrda9qQc&scisig=AHkA5jTSXoYiepoaevwZg0wQCL9_&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=0&folt=kw-top)
*S Upadhyay,AK Sagar,NR Roy*

Main category: learned "cost model"

TL;DR: 本文综述了集成学习在冠心病（CAD）预测中的应用，指出其相较于传统模型（如SVM、ANN、KNN等）在准确率和AUC方面表现更优（达92.5%准确率和0.89 AUC），并探讨了特征选择、临床整合及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统冠心病诊断方法在预测准确性方面存在局限，而CAD作为全球主要死因之一，亟需更高效、精准的早期诊断手段。因此，探索机器学习特别是集成学习方法在提升CAD预测性能方面的潜力具有重要临床意义。

Method: 文章系统回顾并比较了多种用于CAD预测的机器学习模型（如随机森林、SVM、ANN、KNN）与集成学习方法（如bagging、boosting、stacking），同时分析了PCA和RFE等特征选择技术对模型性能的提升作用。

Result: 所提出的集成方法在CAD预测中达到92.5%的准确率和0.89的AUC，比传统模型高出5–7%；同时揭示了集成模型在计算成本、可解释性和数据不平衡等方面的挑战。

Conclusion: 集成学习在提高CAD早期诊断准确性方面具有显著优势，但其在真实临床环境中的部署仍面临若干挑战；未来研究应聚焦于提升模型效率、可解释性及对不平衡医疗数据的鲁棒性，以促进其在医疗系统中的实际应用。

Abstract: Coronary artery disease (CAD) is one of the leading causes of death worldwide, and early diagnosis plays a crucial role in reducing its impact. Traditional diagnostic methods have limitations in their ability to predict CAD accurately, which has led to the adoption of machine learning (ML) models for enhancing prediction accuracy. Among these, ensemble learning methods have shown considerable promise by combining multiple classifiers to boost the overall performance and address the complex nature of CAD prediction. Ensemble techniques such as bagging, boosting, and stacking improve model robustness and generalization. In this review, we explore various contemporary state-of-the-art algorithms used for CAD prediction, such as Random Forest, Support Vector Machines (SVM), Artificial Neural Networks (ANNs), and K-Nearest Neighbors (KNN). We compare the performance of these models with ensemble learning methods, highlighting their strengths and weaknesses in clinical settings. Additionally, the paper discusses the importance of feature selection techniques such as Principal Component Analysis (PCA) and Recursive Feature Elimination (RFE), which enhance model accuracy by eliminating irrelevant attributes. The proposed ensemble method achieved 92.5% accuracy and 0.89 AUC, outperforming traditional models by 5–7%. We also emphasize the challenges associated with ensemble models, including computational cost, model interpretability, and data imbalance, which often occurs in medical datasets. Furthermore, the review provides insights into the integration of ensemble learning models into healthcare systems and their potential for improving early CAD diagnosis and patient outcomes. Finally, the paper suggests directions for future research to improve the efficiency and applicability of ensemble methods for CAD classification, particularly in real-world clinical environments.

</details>


### [10] [Tolerance allocation of complex systems based on supervised machine learning and adaptive sampling](https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s00170-025-17336-3&hl=zh-CN&sa=X&d=5580070672578278958&ei=JrFxaZ6PLLui6rQPrda9qQc&scisig=AHkA5jQGoBYwEUzZ-H2yiHQW_1Aw&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=2&folt=kw-top)
*JY Dantan,W Zouhri*

Main category: learned "cost model"

TL;DR: 本文提出了一种结合自适应采样、重要性采样和C4.5决策树算法的新框架，用于复杂系统的公差分配，显著提升了数据集平衡性和公差识别效果。


<details>
  <summary>Details</summary>
Motivation: 传统公差分配方法在复杂系统中难以高效探索解空间并准确识别符合要求的配置，因此需要一种更智能、自适应的方法来提升精度与效率。

Method: 该框架融合了三种技术：自适应采样根据前序迭代动态调整采样策略；重要性采样聚焦于高概率产生合规解的区域；C4.5决策树从标注数据中学习规则，识别合规配置模式，辅助优选候选解。

Result: 在Janssen机构上的实验表明，该框架显著改善了数据集的类别平衡性，并提高了公差识别的准确性。

Conclusion: 所提出的集成方法有效增强了复杂系统公差分配中的解空间探索能力与决策精度，为工程设计提供了更可靠的支持。

Abstract: This paper presents a novel framework for the tolerance allocation of complex systems. The framework integrates adaptive sampling, importance sampling, and the C4.5 decision tree algorithm to enhance exploration of the solution space and improve accuracy. Adaptive sampling dynamically adjusts the sampling strategy based on previous iterations, while importance sampling focuses on regions with a higher probability of yielding conforming solutions. The C4.5 algorithm generates decision trees from labelled data, identifying patterns and rules for conforming configurations, which assists in selecting the most effective candidate solutions for tolerance allocation. When tested on the Janssen mechanism, the framework demonstrated significant improvements in dataset balance and tolerance identification.

</details>


### [11] [Development, validation, and comparative evaluation of an immersive virtual reality for MRI patient preparation training in radiologic technology education](https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S2949678026000024&hl=zh-CN&sa=X&d=5985929901681987337&ei=JrFxaZ6PLLui6rQPrda9qQc&scisig=AHkA5jQytl2vM9wHrffG9JxSa40w&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=3&folt=kw-top)
*J Cheewakul,SK Marshall,S Chaichulee*

Main category: learned "cost model"

TL;DR: 本研究开发了一种沉浸式虚拟现实（VR）教学模块，用于模拟磁共振成像（MRI）前的患者准备流程，并通过随机对照试验比较其与传统教学材料在即时学习效果上的差异。


<details>
  <summary>Details</summary>
Motivation: 传统放射学教育依赖讲授式教学，受限于设备短缺、患者安全顾虑和师资不足，难以提供充分的实践机会。因此，亟需探索更有效的教学方法以提升学习效果。

Method: 采用随机两组前后测设计，将28名参与者分为VR组和传统材料组，通过前测与后测评估两种教学方式的即时学习成效。

Result: VR教学模块在提升学习者知识掌握和操作理解方面优于传统教学材料，显示出显著的即时学习增益。

Conclusion: 沉浸式VR教学可作为传统放射学教育的有效补充，有助于克服实践训练中的资源与安全限制，提升教学效果。

Abstract: Traditional radiologic technology education primarily relies on lecture-based methods and provides limited hands-on opportunities due to equipment shortages, patient-safety concerns, and limited instructor availability. In this study, we developed an immersive virtual reality (VR) educational module simulating patient preparation for magnetic resonance imaging (MRI). We compared the immediate learning impact of the VR module with that of conventional materials using a randomized two-arm pre-test/post-test design. Twenty-eight …

</details>


### [12] [Adaptive Adversary Regression in Algorithmic Trading](https://scholar.google.com/scholar_url?url=https://bsmachinelearning.com/assets/reports/randomness_algo_trading.pdf&hl=zh-CN&sa=X&d=17534813383792256675&ei=JrFxaZ6PLLui6rQPrda9qQc&scisig=AHkA5jTBKqzAgXQjCc5ExJXweGGg&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=4&folt=kw-top)
*RMDLH Csenteri,MRNR Preslav*

Main category: learned "cost model"

TL;DR: 本文提出一种基于自适应随机执行策略的防御框架，以应对确定性算法交易易被对手预测和利用的问题。通过在10只高流动性ETF上实证评估，Ornstein-Uhlenbeck（OU）随机化机制显著降低对手可预测性（AUC从0.78降至0.57），减少14.8个基点的执行成本，并提升投资组合夏普比率19%。该方法在5次迭代内实现对抗安全，年化收益估计达3700万美元。


<details>
  <summary>Details</summary>
Motivation: 确定性算法交易策略虽统计效率高，但易被对手通过推断订单流进行预测和利用，从而导致不利选择和执行成本上升。因此，亟需引入具备对抗鲁棒性的动态随机执行机制。

Method: 作者构建了一个自适应随机执行策略框架，评估三种随机化机制：均匀随机、Ornstein-Uhlenbeck（OU）均值回归过程和粉红噪声（1/f噪声）。使用静态二分类器和基于回归的价格预测模型作为对手方，在1,538个交易日对10只高流动性ETF进行实证测试。同时引入自适应升级机制，衡量不同策略达到对抗安全所需的迭代次数。

Result: OU策略将对手可预测性降低27%（AUC: 0.78→0.57），实现14.8个基点的执行成本改善（其中10.6个基点来自不利选择减少），投资组合夏普比率提升19%（0.94→1.12）。在自适应升级框架下，OU仅需5次迭代即可达成对抗安全，优于其他方法（15–20次）。在日均1亿美元交易规模下，年化收益估计为3700万美元。

Conclusion: 对抗性鲁棒性不能依赖静态防御，而必须通过动态、博弈论驱动的自适应机制实现。OU过程在兼顾执行效率与安全性方面表现最优，为机构级算法交易提供了实用且高效的解决方案。

Abstract: Deterministic algorithmic execution strategies, while statistically efficient, are vulnerable to exploitation by adversaries who can infer and anticipate order flows. We present a framework for defending against such vulnerabilities through adaptive stochastic execution policies. We evaluate three randomization mechanisms—uniform randomization, Ornstein-Uhlenbeck (OU) meanreverting processes, and pink (1/f) noise—using both static binary classifiers and regression-based price prediction adversaries. Evaluated on 10 highly-liquid ETFs over 1,538 trading days, the OU policy reduces adversary predictability by 27%(AUC: 0.78→ 0.57) and achieves 14.8 basis points implementation shortfall improvement, primarily through 10.6 bps adverse selection reduction. Portfolio-level Sharpe ratio improves 19%(0.94→ 1.12). Our adaptive escalation framework demonstrates that OU achieves adversarial safety in 5 iterations versus 15-20 iterations for alternatives. At institutional scale ($100 M daily volume), the framework delivers estimated $37 M annual benefit. This work establishes that adversarial robustness requires dynamic, game-theoretic adaptation rather than static defense. The full research codebase is available at our GitHub repository.

</details>


### [13] [Neural Architecture Search and Automatic Code Optimization: Techniques, trends, and challenges (With Full Text)](https://scholar.google.com/scholar_url?url=https://repository.cud.ac.ae/bitstreams/5a7a749c-dea6-463b-ab01-6b576ff05076/download&hl=zh-CN&sa=X&d=5977990374977516459&ei=JrFxaZ6PLLui6rQPrda9qQc&scisig=AHkA5jQQP0R8u1-aXdnCqfVZuT4J&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=5&folt=kw-top)
*I Bachiri,S Niar,R Baghdadi,H Ouarnoughi*

Main category: learned "cost model"

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep Learning models have experienced exponential growth in complexity and resource demands in recent years. Accelerating these models for efficient execution on resource-constrained devices has become more crucial than ever. Two notable techniques used to achieve this goal are Hardware-Aware Neural Architecture Search (HW-NAS) and Automatic Code Optimization (ACO). HW-NAS automatically designs accurate yet hardware-friendly neural networks, while ACO involves searching for the best code optimizations to apply on …

</details>


### [14] [Sustainable Computing: Informatics and Systems](https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Vandana_Dwarka/publication/398915242_Towards_energy-efficient_scientific_computing_Reversible_numerical_linear_algebra_kernels_in_floating-point_arithmetic/links/695ed32027359023a0151d0b/Towards-energy-efficient-scientific-computing-Reversible-numerical-linear-algebra-kernels-in-floating-point-arithmetic.pdf&hl=zh-CN&sa=X&d=16597676325910825152&ei=JrFxaZ6PLLui6rQPrda9qQc&scisig=AHkA5jSUb956Pmz3Dtb88FJ4Yn6F&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=7&folt=kw-top)
*V Dwarka*

Main category: learned "cost model"

TL;DR: 该论文探讨了高分辨率偏微分方程（PDE）模型与高维学习系统中固定能耗成本的问题，特别关注拉普拉斯图学习和扩散模型等场景，并基于门电路成本模型进行分析。


<details>
  <summary>Details</summary>
Motivation: 在高维学习系统和高分辨率PDE建模中，计算能耗成为关键瓶颈。作者旨在量化并优化这些系统中的固定能量开销，特别是在图学习和扩散模型等机器学习任务中。

Method: 采用基于门电路成本的计算模型，结合拉普拉斯系统和扩散模型的结构特性，分析其在高维或高分辨率设置下的能量消耗规律。

Result: 揭示了在特定机器学习架构下，固定能耗成本与模型维度或分辨率之间的关系，并为能效优化提供了理论依据。

Conclusion: 通过引入门成本模型，论文为理解高维学习系统和PDE驱动模型中的能量效率提供了新视角，有助于未来低功耗AI系统的设计。

Abstract: … high-resolution PDE models and high-dimensional learning systems, the fixed energy costs of … –driven machine–learning settings (eg Laplacian systems in graph learning, diffusion models, … The underlying gate cost model uses the …

</details>


### [15] [Alt-Consulting: What comes after the end of strategy consulting as we knew it](https://scholar.google.com/scholar_url?url=https://books.google.com/books%3Fhl%3Dzh-CN%26lr%3D%26id%3DrxiyEQAAQBAJ%26oi%3Dfnd%26pg%3DPT5%26dq%3Dlearned%2B%2B%2522cost%2Bmodel%2522%26ots%3D4e1LzqatDZ%26sig%3DIGYEG4sfXhlFJhYikwypN37AzHI&hl=zh-CN&sa=X&d=10377169058402481897&ei=JrFxaZ6PLLui6rQPrda9qQc&scisig=AHkA5jTlPFqgbsAZ4nuOmHA3pdhb&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=8&folt=kw-top)
*UA Bhatt*

Main category: learned "cost model"

TL;DR: 《Alt-Consulting》一书指出，传统战略咨询模式正因生成式AI、精简型咨询公司崛起及客户对速度与价值的更高要求而瓦解，并提出五种新兴行业原型及应对策略。


<details>
  <summary>Details</summary>
Motivation: 传统战略咨询依赖高成本、长时间项目和层级结构，已难以满足当前市场对效率、透明度和性价比的需求；同时，生成式AI等技术正在颠覆分析师团队的工作方式，促使行业必须转型。

Method: 作者结合克莱顿·克里斯坦森的颠覆性创新理论、法律行业被技术冲击的历史经验、真实转型案例以及对咨询行业的深入观察，系统分析行业变革动因并提出新范式。

Result: 识别出传统咨询金字塔模型崩溃的五大驱动因素，并提出五种正在重塑行业的新型咨询原型；同时为不同角色（企业高管、合伙人、年轻顾问）提供适应新环境的具体行动指南。

Conclusion: 传统咨询模式虽尚未完全崩溃，但结构性裂痕已显；唯有主动拥抱变革、重构服务模式与价值主张，从业者才能在“另类咨询”（Alt-Consulting）时代保持相关性并引领未来。

Abstract: The consulting industry is at a breaking point. For decades, strategy consulting thrived on a simple formula: prestigious firms, expensive teams, and months-long projects. But the ground is shifting. Generative AI is automating the work of entire analyst teams. Lean boutiques are delivering partner-level thinking at a fraction of traditional costs. Clients are demanding speed, transparency, and value, not just polished decks. What comes after the end of strategy consulting as we knew it? In Alt-Consulting, seasoned strategist Utsav Bhatt maps the future of the profession with clarity and conviction. Inside, you'll discover: Why the traditional consulting pyramid is collapsing and what's replacing it Five new archetypes reshaping the industry What law firms learned when technology disrupted them first How to buy consulting differently: faster, cheaper, better What consultants at every level must do to stay relevant Drawing on Clayton Christensen's theory of disruptive innovation, real transformation cases, and timeless wisdom, Alt-Consulting is both a strategic playbook and a call to reimagine professional services. The old model isn't broken yet. But the cracks are loud. Whether you're a Chief Strategy Officer tired of overpriced projects, a consulting firm partner sensing the shift, or a young consultant wondering about your career in ten years, this book shows you how to lead, not lag. The disruption is already here. The question is: are you ready?

</details>


### [16] [Geological theories and engineering technologies for deep, ultra-deep, and deep-sea oil and gas exploration and development: Progress, challenges, and future …](https://scholar.google.com/scholar_url?url=https://www.jstage.jst.go.jp/article/arr/6/1/6_394/_pdf&hl=zh-CN&sa=X&d=10555039997461631356&ei=JrFxaZ6PLLui6rQPrda9qQc&scisig=AHkA5jTHv3836chX_jeZAutI9m3S&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=9&folt=kw-top)
*D Jiang,H Li*

Main category: learned "cost model"

TL;DR: 本文系统综述了深层、超深层和深水油气勘探开发在地质理论与工程技术方面的最新进展，分析了当前面临的关键挑战，并提出了未来研究的战略方向。


<details>
  <summary>Details</summary>
Motivation: 在全球能源体系转型与低碳化进程持续推进的背景下，深层、超深层及深水油气资源日益成为保障未来能源安全与推动技术创新的关键领域。然而，复杂地质条件与极端工程环境下的多场耦合效应、储层强非均质性、工程风险以及经济与碳减排双重约束等问题，制约了该领域的可持续发展，亟需系统梳理理论与技术进展并明确未来突破路径。

Method: 采用系统综述方法，整合近年来在油气成藏动力学、储层结构演化、流体运移机制、多尺度构造分析等石油地质理论方面的研究成果，以及钻完井、水力压裂、深水钻探和智能生产等工程技术的突破，进而识别关键科学问题与技术瓶颈。

Result: 研究表明，地质理论的深化为复杂条件下油气藏的预测与精准开发提供了新基础，而工程技术的进步支撑了高温、高压、高应力及强非均质环境下的安全高效作业。但多场耦合、储层非均质性加剧、极端工况风险及经济-碳排双重约束仍是主要障碍。

Conclusion: 未来应加强地质理论体系的集成完善，并推动智能化与绿色化工程技术的协同创新，以夯实深层、超深层及深水油气勘探开发的科学基础，支撑能源安全与低碳转型协同发展。

Abstract: Amid the ongoing transformation of the global energy system and the sustained advancement of the lowcarbon transition, deep, ultra-deep, and deep-sea oil and gas resources are increasingly becoming key domains supporting future energy security and technological innovation. In recent years, significant progress has been achieved in petroleum geological theories, particularly in hydrocarbon accumulation dynamics, reservoir structural evolution, fluid migration mechanisms, and multi-scale structural analysis, providing new scientific foundations for the prediction, evaluation, and precise development of oil and gas under complex geological conditions. Meanwhile, continuous breakthroughs in drilling and completion, hydraulic fracturing, deep-sea drilling, and intelligent production technologies have enabled safe and efficient operations under high-temperature, high-pressure, high-stress, and strongly heterogeneous environments. However, challenges such as multi-field coupling effects in deep formations, intensified reservoir heterogeneity, engineering risks under extreme conditions, and the dual constraints of economic feasibility and carbon reduction remain critical barriers to sustainable development in this field. This paper systematically reviews recent advances in geological theories and engineering technologies for deep, ultradeep, and deep-sea oil and gas exploration and development, analyzes the key challenges currently faced, and proposes strategic directions for future research, including the integrated enhancement of geological theory systems and the synergistic innovation of intelligent and green engineering technologies. The study aims to provide theoretical support and practical insights for advancing the scientific foundation and technological upgrading of oil and gas exploration and development.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [17] [NL4ST: A Natural Language Query Tool for Spatio-Temporal Databases](https://arxiv.org/abs/2601.15758)
*Xieyang Wang,Mengyi Liu,Weijia Yi,Jianqiu Xu,Raymond Chi-Wing Wong*

Main category: cs.DB

TL;DR: NL4ST 是一个支持自然语言查询（NLQ）的交互式工具，旨在帮助非专业用户对时空数据库进行高效查询，其采用三层架构，并在多个真实与合成数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 随着移动计算设备和定位技术的发展，时空数据激增，但传统查询方式（如范围查询、最近邻查询等）需要用户具备领域知识和查询语言技能，对非专家用户构成障碍。因此，亟需一种支持自然语言查询的系统以降低使用门槛。

Method: NL4ST 采用三层架构：(i) 知识库与语料库用于知识准备；(ii) 自然语言理解模块用于实体链接；(iii) 查询计划生成模块用于生成物理执行计划。系统通过交互式界面将自然语言输入转化为可执行的时空查询计划。

Result: 在四个真实与合成数据集上的演示表明，NL4ST 能有效生成准确的时空物理查询计划，验证了其可行性和实用性。

Conclusion: NL4ST 成功弥合了非专家用户与时空数据库查询之间的鸿沟，为自然语言驱动的时空数据访问提供了实用解决方案，并已上线提供在线演示。

Abstract: The advancement of mobile computing devices and positioning technologies has led to an explosive growth of spatio-temporal data managed in databases. Representative queries over such data include range queries, nearest neighbor queries, and join queries. However, formulating those queries usually requires domain-specific expertise and familiarity with executable query languages, which would be a challenging task for non-expert users. It leads to a great demand for well-supported natural language queries (NLQs) in spatio-temporal databases. To bridge the gap between non-experts and query plans in databases, we present NL4ST, an interactive tool that allows users to query spatio-temporal databases in natural language. NL4ST features a three-layer architecture: (i) knowledge base and corpus for knowledge preparation, (ii) natural language understanding for entity linking, and (iii) generating physical plans. Our demonstration will showcase how NL4ST provides effective spatio-temporal physical plans, verified by using four real and synthetic datasets. We make NL4ST online and provide the demo video at https://youtu.be/-J1R7R5WoqQ.

</details>


### [18] [EAIFD: A Fast and Scalable Algorithm for Incremental Functional Dependency Discovery](https://arxiv.org/abs/2601.16025)
*Yajuan Xu,Xixian Han,Xiaolong Wan*

Main category: cs.DB

TL;DR: 本文提出EAIFD算法，通过维护差集的偏超图并将增量函数依赖（FD）发现问题转化为超图上的最小击中集枚举，结合多属性哈希表（MHT）和两阶段验证策略，显著提升性能并降低内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有静态FD发现算法在增量更新场景下效率低下，需完全重运行；而现有增量算法则面临严重的性能与内存瓶颈。因此，亟需一种高效、可扩展的增量FD发现方法。

Method: EAIFD算法将增量FD发现建模为超图上的最小击中集枚举问题。其核心包括：(1) 设计多属性哈希表（MHT）存储有效FD的高频键值映射，其内存占用与数据集大小无关；(2) 采用两阶段验证策略，先利用MHT大幅缩减候选集验证空间，再批量加载数据块对剩余候选进行验证，避免重复I/O。

Result: 在真实数据集上的实验表明，EAIFD相比现有算法，运行时间最高提速一个数量级，内存使用量减少两个数量级以上。

Conclusion: EAIFD是一种高效且可扩展的增量FD发现解决方案，通过创新的数据结构和验证机制，有效克服了现有方法的性能与内存瓶颈。

Abstract: Functional dependencies (FDs) are fundamental integrity constraints in relational databases, but discovering them under incremental updates remains challenging. While static algorithms are inefficient due to full re-execution, incremental algorithms suffer from severe performance and memory bottlenecks. To address these challenges, this paper proposes EAIFD, a novel algorithm for incremental FD discovery. EAIFD maintains the partial hypergraph of difference sets and reframes the incremental FD discovery problem into minimal hitting set enumeration on hypergraph, avoiding full re-runs. EAIFD introduces two key innovations. First, a multi-attribute hash table ($MHT$) is devised for high-frequency key-value mappings of valid FDs, whose memory consumption is proven to be independent of the dataset size. Second, two-step validation strategy is developed to efficiently validate the enumerated candidates, which leverages $MHT$ to effectively reduce the validation space and then selectively loads data blocks for batch validation of remaining candidates, effectively avoiding repeated I/O operations. Experimental results on real-world datasets demonstrate the significant advantages of EAIFD. Compared to existing algorithms, EAIFD achieves up to an order-of-magnitude speedup in runtime while reducing memory usage by over two orders-of-magnitude, establishing it as a highly efficient and scalable solution for incremental FD discovery.

</details>


<div id='Ziniu Wu'></div>

# Ziniu Wu [[Back]](#toc)

### [19] [Optimus: Deployable Query Optimization via Novel SQL Rewrites](https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DZVzYv7Qois&hl=zh-CN&sa=X&d=8376193066075901600&ei=nwRxaby5HPStieoP-8T7kA0&scisig=AHkA5jS9_FfixoqCddZw8Pcingt6&oi=scholaralrt&hist=Pxo5FIAAAAAJ:11734074576040036222:AHkA5jT4qH64J8nZejvQlyvZo1xT&html=&pos=0&folt=rel)
*R Lone*

Main category: Ziniu Wu

TL;DR: Optimus improves query optimization by expanding the action space with novel execution plan rewrites, achieving a 1.16x speedup over vanilla PostgreSQL on the extended JOB benchmark without engine modifications.


<details>
  <summary>Details</summary>
Motivation: Traditional learned query optimizers are limited to a fixed set of configurations, restricting the achievable plan space; Optimus addresses this by dynamically discovering and applying novel rewrites.

Method: Optimus employs graph-based inductive matrix completion and a multilayer perceptron to select among mined execution plan rewrites, aiming to minimize query latency while ensuring deployability through guard-checked compilation and no engine changes.

Result: On the extended JOB benchmark, Optimus achieves a 1.16x speedup compared to vanilla PostgreSQL using only its novel rewrites.

Conclusion: By expanding the optimizer’s action space with learnable, deployable rewrites, Optimus demonstrates that performance gains are possible without modifying the underlying database engine.

Abstract: Learned database query optimizers typically optimize over a set of configurations, which limits the attainable plan space. Optimus expands the action space itself by mining novel execution plan rewrites and learns to select among these actions online. Optimus utilizes graph-based inductive matrix completion and a multilayer perceptron with the objective of minimizing latency. Crucially, the system is deployable by design: it requires no engine modifications and its rules include guard-checked compilation. On the extended JOB benchmark, Optimus yields a speedup of 1.16x over vanilla PostgreSQL solely using novel rewrites.

</details>


### [20] [Missing Value Imputation in Tabular Data Lakes Unleashed: A Hybrid Approach: F. Luo et al.](https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s00778-025-00957-1&hl=zh-CN&sa=X&d=4606946733610701853&ei=H3ByaebiO_OlieoPkILrwAk&scisig=AHkA5jSP3th-QnoRoXa_mamsssfF&oi=scholaralrt&hist=Pxo5FIAAAAAJ:11734074576040036222:AHkA5jT4qH64J8nZejvQlyvZo1xT&html=&pos=0&folt=rel)
*F Luo,H Lan,H Luo,Z Bao,JS Culpepper,S Sadiq…*

Main category: Ziniu Wu

TL;DR: CESID is a novel imputation framework for tabular data lakes that combines estimation-based and search-based methods to address missing values by considering three aspects of variety: source of imputed value, table types, and data types.


<details>
  <summary>Details</summary>
Motivation: Existing imputation methods—either estimation-based or search-based—fail to simultaneously account for the three key aspects of variety in data lakes, leading to suboptimal imputation performance in downstream tasks.

Method: CESID integrates three modules: (1) Contextual Search Module retrieves candidate values using contextual information across tables; (2) Acquisition-guided Estimation Module uses an influence function and sampling strategy to produce accurate estimates; (3) Classifier Module selects the best imputation method based on table- and column-level statistics.

Result: Experiments on three real-world data lakes show that CESID outperforms existing methods in both effectiveness and efficiency for missing value imputation.

Conclusion: By jointly leveraging estimation and search strategies while accounting for data variety, CESID provides a robust and generalizable solution for missing value imputation in heterogeneous data lakes.

Abstract: Missing values in tabular data lakes can severely impact data analysis and diminish the performance in downstream applications. We highlight that a robust imputation strategy should properly take three aspects of variety into consideration: source of imputed value, the types of tables involved, and the data types of the missing value. Existing imputation methods rely on estimation-based approaches (using a model trained on data from the same table to estimate missing values) or search-based approaches (retrieving values from other tables). Unfortunately, none of these approaches effectively incorporate all three aspects of variety. To address this gap, we propose CESID, a novel framework that uses a Combination of Estimation-based and Search-based methods for missing value Imputation in Data lakes. CESID contains three core modules: (1) the Contextual Search Module, which efficiently discovers candidate values from tables by exploiting the contextual information; (2) the Acquisition-guided Estimation Module, which introduces an influence function and a sampling-based exploration strategy to yield accurate estimated values; (3) the Classifier Module, which determines the most suitable method based on table-level and column-level statistics. Extensive experiments conducted on three data lakes demonstrate that CESID effectively and efficiently addresses the missing value problem.

</details>


### [21] [Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.02076&hl=zh-CN&sa=X&d=1915482236404124982&ei=H3ByaebiO_OlieoPkILrwAk&scisig=AHkA5jSIfa7a_J_3aEFHUin_0Jpv&oi=scholaralrt&hist=Pxo5FIAAAAAJ:11734074576040036222:AHkA5jT4qH64J8nZejvQlyvZo1xT&html=&pos=1&folt=rel)
*Y Shu,Y Tian,C Xu,Y Wang,H Chen*

Main category: Ziniu Wu

TL;DR: 提出了一种无需训练的解码策略 DCD，通过根据不确定性动态推迟高不确定度 token 的确定，缓解块状扩散语言模型中的边界诱导上下文截断问题，从而提升生成质量和推理效率。


<details>
  <summary>Details</summary>
Motivation: 块状扩散解码（block-based diffusion）在边界处强制未解码 token 提前确定，无法利用邻近的未来上下文信息，导致解码置信度下降和生成质量受损，尤其影响数学推理和代码生成等任务。为解决这一结构性限制（称为 BICT），作者提出改进方案。

Method: 提出 Deferred Commitment Decoding (DCD)，一种无需训练的解码策略：维护一个对遮蔽 token 的置信度感知滑动窗口，对低不确定性 token 早期确定，而将高不确定性 token 推迟至获得足够上下文证据后再确定，从而在保持效率的同时实现窗口内有效的双向信息流。

Result: 在多个扩散语言模型、基准任务和缓存配置下实验表明，DCD 相比固定块解码平均提升生成准确率 1.39%，最高提升达 9.0%，且推理时间相当。

Conclusion: 基于不确定性推迟 token 确定是一种简单而有效的原则，可同时提升扩散语言模型解码的质量与效率。

Abstract: Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding.

</details>
