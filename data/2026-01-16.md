<div id=toc></div>

# Table of Contents

- [learned "cost model"](#learned "cost model") [Total: 7]
- [Google Scholar](#Google Scholar) [Total: 3]
- [cs.DB](#cs.DB) [Total: 5]
- ["query optimization"](#"query optimization") [Total: 5]


<div id='learned "cost model"'></div>

# learned "cost model" [[Back]](#toc)

### [1] [Optimized Dimensionality Reduction Using Metaheuristic and Class Separability.](https://scholar.google.com/scholar_url?url=https://search.ebscohost.com/login.aspx%3Fdirect%3Dtrue%26profile%3Dehost%26scope%3Dsite%26authtype%3Dcrawler%26jrnl%3D2158107X%26AN%3D190677590%26h%3Dyspkc9HmRoywiqfVngTQu0YzM4YRRiHraJlBzCY%252BC%252BPBJXVVpTkJ4tO0cbcCPDviGqcp%252BYY3QEWNOHlDIEmvfA%253D%253D%26crl%3Dc&hl=zh-CN&sa=X&d=4450246633678404891&ei=57xoaY2BOr6Z6rQPnvC-6QQ&scisig=AHkA5jTUxZam-cNhHrSwHt-OIQDj&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=0&folt=kw-top)
*EA Ahmed,M Alzaqebah,S Jawarneh*

Main category: learned "cost model"

TL;DR: 本文提出了一种基于粒子群优化（PSO）与K近邻距离比（KDR）相结合的元启发式特征选择方法（KDR-PSO），用于高效地识别高判别性特征子集，在降低维度的同时提升模型可解释性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 高维数据带来计算成本高、模型复杂度大和过拟合风险等问题，亟需一种高效且能保留判别信息的降维方法。

Method: 将粒子群优化算法（PSO）与K近邻距离比（KDR）指标结合，构建一个过滤式目标函数。KDR通过计算样本到异类邻居与同类邻居的平均距离之比来衡量特征子空间中的类间可分性，并在优化过程中加入模型大小惩罚项，以自动搜索精简而高判别性的特征子集。

Result: 在多个基因表达和图像基准数据集上的实验表明，KDR-PSO相比现有基线和其他算法，能在使用更少特征的同时获得相同或更优的分类性能。

Conclusion: KDR-PSO是一种高效、实用的高维数据降维方法，能够有效提升模型的可解释性与泛化能力，特别适用于多类分类任务。

Abstract: The high dimensionality of modern datasets presents significant challenges for machine learning, including increased computational cost, model complexity, and risk of overfitting. This study introduces a metaheuristic framework for optimized dimensionality reduction to identify the highly discriminative feature subsets. The proposed method (KDRPSO) combines a Particle Swarm Optimization (PSO) algorithm with the K-Nearest Neighbors Distance Ratio (KDR) as a filterbased objective function. This metric quantitatively assesses class separability within a feature subspace by computing the ratio of the average distance from a sample to neighbors in other classes versus those in its own class. Maximizing this ratio with a penalty for model size, KDR-PSO automates the discovery of parsimonious feature sets that maximize inter-class discrimination. The method is computationally efficient, naturally lending itself to multi-class classification and avoiding the prohibitive cost associated with classifier-in-the-loop wrappers. Experimental results on benchmark gene expression and image datasets show that KDR-PSO can achieve better dimensionality reduction compared to baselines and other algorithms, such as winning a better or at least similar performing models with decreased features. This approach is a strong and pragmatic technique to improve the model interpretability and generalizability for high-dimensional regions.

</details>


### [2] [DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.05975&hl=zh-CN&sa=X&d=15211431049750054375&ei=57xoaY2BOr6Z6rQPnvC-6QQ&scisig=AHkA5jS-Pt_dO-LyGMiPyUV80UJC&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=1&folt=kw-top)
*K Wood,SJ Roberts,S Zohren*

Main category: learned "cost model"

TL;DR: DeePM 是一种端到端训练的深度宏观投资组合管理模型，通过因果延迟机制、宏观经济图先验和分布鲁棒优化，在2010–2025年对50种期货的大规模回测中实现了约两倍于传统动量策略和被动基准的风险调整后收益，并显著优于当前最先进的Momentum Transformer。


<details>
  <summary>Details</summary>
Motivation: 金融机器学习面临三大核心挑战：异步信息流（“锯齿滤波”问题）、低信噪比以及在极端不利市场环境下缺乏稳健性。现有方法难以同时兼顾因果性、经济结构先验与风险敏感目标，导致泛化能力受限。

Method: DeePM 采用三种关键技术：(1) 有向延迟机制（因果筛）以优先学习因果脉冲响应；(2) 宏观经济图先验，依据经济学原理正则化跨资产依赖关系；(3) 分布鲁棒目标函数，使用平滑最差窗口惩罚作为熵值风险（EVaR）的可微近似，以优化最不利历史子区间的性能。模型仅使用每日收盘价进行端到端训练，并显式建模交易成本。

Result: 在2010–2025年涵盖50种多样化期货、包含高保真交易成本的大规模回测中，DeePM 的净风险调整后收益约为经典趋势跟踪策略和被动基准的两倍，并比当前最优的 Momentum Transformer 架构提升约50%。模型在“CTA寒冬”、疫情、通胀冲击及“higher-for-longer”利率环境中均保持稳健表现。

Conclusion: DeePM 通过融合因果时序建模、经济结构先验与分布鲁棒优化，显著提升了宏观量化策略的样本外泛化能力与极端市场适应性，验证了结构化深度学习在系统性投资中的有效性。

Abstract: We propose DeePM (Deep Portfolio Manager), a structured deep-learning macro portfolio manager trained end-to-end to maximize a robust, risk-adjusted utility. DeePM addresses three fundamental challenges in financial learning: (1) it resolves the asynchronous "ragged filtration" problem via a Directed Delay (Causal Sieve) mechanism that prioritizes causal impulse-response learning over information freshness; (2) it combats low signal-to-noise ratios via a Macroeconomic Graph Prior, regularizing cross-asset dependence according to economic first principles; and (3) it optimizes a distributionally robust objective where a smooth worst-window penalty serves as a differentiable proxy for Entropic Value-at-Risk (EVaR) - a window-robust utility encouraging strong performance in the most adverse historical subperiods. In large-scale backtests from 2010-2025 on 50 diversified futures with highly realistic transaction costs, DeePM attains net risk-adjusted returns that are roughly twice those of classical trend-following strategies and passive benchmarks, solely using daily closing prices. Furthermore, DeePM improves upon the state-of-the-art Momentum Transformer architecture by roughly fifty percent. The model demonstrates structural resilience across the 2010s "CTA (Commodity Trading Advisor) Winter" and the post-2020 volatility regime shift, maintaining consistent performance through the pandemic, inflation shocks, and the subsequent higher-for-longer environment. Ablation studies confirm that strictly lagged cross-sectional attention, graph prior, principled treatment of transaction costs, and robust minimax optimization are the primary drivers of this generalization capability.

</details>


### [3] [Optimal Transport under Group Fairness Constraints](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.07144&hl=zh-CN&sa=X&d=14090813806865382357&ei=57xoaY2BOr6Z6rQPnvC-6QQ&scisig=AHkA5jQRI1_X0xz0ZdvTfGC5MO3b&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=3&folt=kw-top)
*L Bleistein,M Dagréou,F Andrade,T Boudou,A Bellet*

Main category: learned "cost model"

TL;DR: 该论文提出了一种在最优传输（OT）框架下实现群体公平的新定义，并开发了两种策略：一是通过惩罚项放松公平约束并提供有限样本复杂度保证；二是利用双层优化学习诱导公平匹配的代价函数，并在未见数据上保证公平性。


<details>
  <summary>Details</summary>
Motivation: 在资源或职位分配中，匹配算法需兼顾效率与公平。现有方法缺乏对群体间匹配概率的显式公平控制，本文旨在填补这一空白。

Method: 首先提出\texttt{FairSinkhorn}算法以精确满足公平约束；随后提出两种松弛方法：(1) 带凸惩罚项的最优传输问题，提供有限样本复杂度分析；(2) 通过双层优化学习地面代价函数，使所得OT解天然满足公平性。

Result: 理论方面，给出了带惩罚项OT问题的有限样本复杂度界，并证明所学代价函数在新数据上仍能产生公平匹配；实验验证了公平性与匹配质量之间的权衡关系。

Conclusion: 本文为最优传输中的群体公平问题提供了理论严谨且实用的解决方案，兼顾算法效率、理论保证与实际性能。

Abstract: Ensuring fairness in matching algorithms is a key challenge in allocating scarce resources and positions. Focusing on Optimal Transport (OT), we introduce a novel notion of group fairness requiring that the probability of matching two individuals from any two given groups in the OT plan satisfies a predefined target. We first propose \texttt{FairSinkhorn}, a modified Sinkhorn algorithm to compute perfectly fair transport plans efficiently. Since exact fairness can significantly degrade matching quality in practice, we then develop two relaxation strategies. The first one involves solving a penalised OT problem, for which we derive novel finite-sample complexity guarantees. This result is of independent interest as it can be generalized to arbitrary convex penalties. Our second strategy leverages bilevel optimization to learn a ground cost that induces a fair OT solution, and we establish a bound guaranteeing that the learned cost yields fair matchings on unseen data. Finally, we present empirical results that illustrate the trade-offs between fairness and performance.

</details>


### [4] [Optimal Learning Rate Schedule for Balancing Effort and Performance](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.07830&hl=zh-CN&sa=X&d=12307612327796868984&ei=57xoaY2BOr6Z6rQPnvC-6QQ&scisig=AHkA5jQkAiefuNmn9aji0AsW9nUC&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=4&folt=kw-top)
*V Njaradi,R Carrasco*

Main category: learned "cost model"

TL;DR: 该论文提出了一种规范性框架，将学习速率的调节建模为一个最优控制问题，在最大化累积性能的同时最小化学习成本，并推导出一个仅依赖当前与预期未来性能的闭环控制器形式的最优学习率解。


<details>
  <summary>Details</summary>
Motivation: 生物和人工智能体都需要在学习过程中有效调节学习速度，以在快速提升性能与付出努力、不稳定性或资源消耗之间取得平衡。现有方法缺乏统一的规范性理论来解释这一调节机制。

Method: 作者构建了一个最优控制框架，将学习速率选择视为在性能收益与学习成本之间的权衡问题；在此基础上推导出闭式解，并在简单模型中进行数学分析；此外，引入情景记忆机制来近似估计未来性能。

Result: 所提出的最优学习率策略具有任务和架构普适性，能复现数值优化的学习调度；理论分析揭示了智能体与任务参数如何影响学习率调度；情景记忆机制可有效近似所需性能预期，实现接近最优的行为。

Conclusion: 该研究提供了一个规范且生物学上合理的数学框架，统一解释了学习速度控制、自我调节学习、努力分配与情景记忆估计之间的关系。

Abstract: Learning how to learn efficiently is a fundamental challenge for biological agents and a growing concern for artificial ones. To learn effectively, an agent must regulate its learning speed, balancing the benefits of rapid improvement against the costs of effort, instability, or resource use. We introduce a normative framework that formalizes this problem as an optimal control process in which the agent maximizes cumulative performance while incurring a cost of learning. From this objective, we derive a closed-form solution for the optimal learning rate, which has the form of a closed-loop controller that depends only on the agent's current and expected future performance. Under mild assumptions, this solution generalizes across tasks and architectures and reproduces numerically optimized schedules in simulations. In simple learning models, we can mathematically analyze how agent and task parameters shape learning-rate scheduling as an open-loop control solution. Because the optimal policy depends on expectations of future performance, the framework predicts how overconfidence or underconfidence influence engagement and persistence, linking the control of learning speed to theories of self-regulated learning. We further show how a simple episodic memory mechanism can approximate the required performance expectations by recalling similar past learning experiences, providing a biologically plausible route to near-optimal behaviour. Together, these results provide a normative and biologically plausible account of learning speed control, linking self-regulated learning, effort allocation, and episodic memory estimation within a unified and tractable mathematical framework.

</details>


### [5] [OPTIMIZATION ALGORITHMS FOR LARGE-SCALE DATA SYSTEMS: A MATHEMATICAL FRAMEWORK FOR COMPUTATIONAL EFFICIENCY](https://scholar.google.com/scholar_url?url=https://ijamjournal.org/ijam/publication/index.php/ijam/article/download/1361/1240&hl=zh-CN&sa=X&d=4533262226877855442&ei=57xoaY2BOr6Z6rQPnvC-6QQ&scisig=AHkA5jQrPSj117wgaJal4QPZqfOl&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=5&folt=kw-top)
*TR Ramesh*

Main category: learned "cost model"

TL;DR: 本文提出一个统一的数学框架，将优化算法与系统级约束结合，以提升大规模数据系统中的计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统优化技术难以应对现代大规模数据系统中高维变量、异构存储、分布式计算节点和动态资源状态带来的挑战。

Method: 构建了一个整合凸优化、随机梯度方法、分布式对偶更新和自适应调度模型的统一框架，将系统各组件（内存、I/O、通信带宽、任务队列、计算吞吐）建模为优化曲面，并提出一种结合坐标下降、方差缩减梯度和异步更新规则的混合求解器。

Result: 所提方法减少了通信轮次，提高了吞吐量，并在数据划分不均或更新延迟的情况下仍能稳定收敛，对拖尾效应具有强鲁棒性。

Conclusion: 该工作通过将优化理论与分布式系统工程相结合，为数据密集型基础设施中的可扩展高效计算提供了数学基础。

Abstract: Large-scale data systems strain traditional optimization techniques because modern workloads involve high-dimensional variables, heterogeneous storage layers, distributed compute nodes, and rapidly shifting resource states. This paper develops a consolidated mathematical framework that unifies algorithmic optimization methods with system-level constraints to improve computational efficiency in such environments. The framework integrates convex optimization, stochastic gradient methods, distributed primal–dual updates, and adaptive scheduling models, allowing each component of a data system memory, I/O, communication bandwidth, task queues, and compute throughput to be expressed as an optimization surface. By formalizing computation cost as a multivariate function of latency, bandwidth contention, node failures, and parallelization overhead, the framework enables analytical comparisons of algorithmic strategies under real system conditions. The study synthesizes existing optimization models and proposes a hybrid large-scale solver that combines coordinate descent, variance-reduced gradients, and asynchronous update rules to stabilize convergence even when data is partitioned unevenly or updates are delayed. The resulting formulation reduces communication rounds, improves throughput, and demonstrates strong resilience to straggler effects. This work contributes a mathematically grounded perspective bridging optimization theory with distributed systems engineering, offering a foundation for scalable, efficient computation in data-intensive infrastructures.

</details>


### [6] [Breaking Model Lock-in: Cost-Efficient Zero-Shot LLM Routing via a Universal Latent Space](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.06220&hl=zh-CN&sa=X&d=4944790851333753156&ei=57xoaY2BOr6Z6rQPnvC-6QQ&scisig=AHkA5jTJNATCPD3wjNZpk0gPU-m6&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=7&folt=kw-top)
*C Yan,W Zhang,Z Ning,F Xu,Z Tao,L Zhang,B Yin…*

Main category: learned "cost model"

TL;DR: ZeroRouter is a novel LLM routing framework that enables zero-shot integration of new models by leveraging a universal latent space representing query difficulty, eliminating the need for costly retraining and outperforming existing methods in accuracy, cost, and latency.


<details>
  <summary>Details</summary>
Motivation: The current LLM ecosystem suffers from “model lock-in,” where integrating new models into routing systems requires exhaustive and expensive retraining, limiting scalability and adaptability.

Method: ZeroRouter introduces a universal latent space that decouples query characterization from model profiling. It uses a context-aware predictor to map queries into this space and a dual-mode optimizer to balance accuracy, cost, and latency, enabling zero-shot onboarding of new models.

Result: ZeroRouter consistently outperforms all baselines, achieving higher accuracy while simultaneously reducing both cost and latency.

Conclusion: ZeroRouter effectively breaks the model lock-in problem in LLM routing by enabling efficient, zero-shot integration of new models through a universal representation of query difficulty, offering a scalable and adaptable solution.

Abstract: The rapid proliferation of Large Language Models (LLMs) has led to a fragmented and inefficient ecosystem, a state of ``model lock-in'' where seamlessly integrating novel models remains a significant bottleneck. Current routing frameworks require exhaustive, costly retraining, hindering scalability and adaptability. We introduce ZeroRouter, a new paradigm for LLM routing that breaks this lock-in. Our approach is founded on a universal latent space, a model-agnostic representation of query difficulty that fundamentally decouples the characterization of a query from the profiling of a model. This allows for zero-shot onboarding of new models without full-scale retraining. ZeroRouter features a context-aware predictor that maps queries to this universal space and a dual-mode optimizer that balances accuracy, cost, and latency. Our framework consistently outperforms all baselines, delivering higher accuracy at lower cost and latency.

</details>


### [7] [A scientific framework for training robust and adaptable foundation models using cosmic observational streams](https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Dwayne-Samuels-2/publication/399653645_STARSCREAM_A_scientific_framework_for_training_robust_and_adaptable_foundation_models_using_cosmic_observational_streams/links/6962f4d5ee048155cffa43f8/STARSCREAM-A-scientific-framework-for-training-robust-and-adaptable-foundation-models-using-cosmic-observational-streams.pdf&hl=zh-CN&sa=X&d=12065362148775580219&ei=57xoaY2BOr6Z6rQPnvC-6QQ&scisig=AHkA5jSI9o8lC90UTdqOymDwPW79&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=8&folt=kw-top)
*D Samuels*

Main category: learned "cost model"

TL;DR: Project STARSCREAM investigates whether large-scale, naturally occurring astronomical data can enhance the robustness and adaptability of foundation models by serving as a high-entropy, non-anthropogenic source of pretraining and self-supervision, with the goal of mitigating issues caused by synthetic data feedback loops.


<details>
  <summary>Details</summary>
Motivation: Foundation models face critical limitations including high computational/energy costs, poor data quality/provenance, and fragility under distribution shifts or sensor noise. The increasing reliance on synthetic data exacerbates risks like model collapse and distribution narrowing. To address this, the paper proposes leveraging natural observational data from astronomy as a complementary, high-entropy training signal that may improve model robustness and out-of-distribution generalization.

Method: The authors propose a scientific program—Project STARSCREAM—that uses astronomical data (e.g., sky-survey images, light curves, spectroscopy) for pretraining and auxiliary self-supervision in modern architectures (transformers, state-space models, etc.). They outline a full pipeline including data acquisition from public archives and custom satellites, preprocessing with noise modeling, model integration strategies, and evaluation protocols designed to test falsifiable hypotheses about improved OOD performance, calibration, and representation learning.

Result: The paper does not present empirical results but defines a reproducible benchmark and engineering blueprint for controlled experiments. It specifies how success or failure will be measured, aiming to either validate cosmic data as a useful “nature-sourced” signal or establish rigorous negative results clarifying its limitations.

Conclusion: STARSCREAM is framed as a testable hypothesis: if validated, it offers a novel, robust data source to complement synthetic and web-based training; if refuted, it still contributes by delineating the limits of domain-general sequence learning versus domain-specific semantics in foundation models.

Abstract: Foundation models are increasingly limited by a triad of constraints:(i) compute and energy costs,(ii) data quality and provenance, and (iii) fragility under distribution shift and sensor noise. At the same time, the fastest-growing source of training data for frontier systems is synthetic (model-generated) data, which introduces feedback-loop risks such as distribution narrowing, memorization of artifacts, and" model collapse" effects under repeated self-training. Project STARSCREAM explores a complementary lever: pretraining and auxiliary self-supervision on large-scale, naturally occurring, non-anthropogenic observational streams from astronomy and astrophysics (for example, sky-survey imagery, time-domain light curves, spectroscopy, and event catalogs). This paper proposes a scientific program to test whether cosmic observational data can measurably improve robustness and adaptability of modern architectures (transformers, state-space and linear-attention sequence models, convolutional backbones, and multimodal encoders). We frame STARSCREAM as a methods hypothesis rather than a claim: that exposure to high-entropy, instrument-mediated signals and naturally structured noise can regularize representation learning, improve calibration, and increase out-of-distribution (OOD) performance on downstream tasks that involve temporal dynamics, perception, and control. We specify falsifiable hypotheses, define evaluation protocols, and detail a practical end-to-end system design spanning data acquisition (public archives, partnerships, hosted payloads, and custom small satellites), preprocessing and noise modeling, model integration, and cost and governance considerations. The intended outcome is a reproducible benchmark and an engineering blueprint that can be validated or rejected via controlled experiments. If supported, STARSCREAM would provide a new class of" nature-sourced" training signals that complements text, web, and simulation data, and reduces dependence on brittle synthetic pipelines. If unsupported, the program still yields a rigorous negative result: clarifying where cosmic data does not transfer and helping define the boundary between domain-general sequence learning and domain-specific semantics.

</details>


<div id='Google Scholar'></div>

# Google Scholar [[Back]](#toc)

### [8] [Acyclic Join Sampling under Selections: Dichotomy, Union Sampling, and Enumeration](https://scholar.google.com/scholar_url?url=http://www.cse.cuhk.edu.hk/~taoyf/paper/icdt26.pdf&hl=en&sa=X&d=4831470035080384130&ei=62NpaYfqKtrJieoPjeLumAg&scisig=AHkA5jRGY0icDpoIemtIN9CdEiPp&oi=scholaralrt&hist=Pxo5FIAAAAAJ:6303853285888070373:AHkA5jQg0xFJQ1T_sclEfqikcIfn&html=&pos=0&folt=rel)
*J Huang,Y Tao,S Wang*

Main category: Google Scholar

TL;DR: 本文系统研究了在运行时给定等值选择条件下，从无环连接结果中采样的复杂性问题，针对合取条件和析取范式分别提出了最优的O(IN)空间、O(1)采样时间的数据结构，并揭示了与随机枚举问题的联系。


<details>
  <summary>Details</summary>
Motivation: 以往关于连接采样的研究主要集中在不含选择条件的连接上，而实际数据库查询中普遍存在选择条件。因此，作者旨在系统研究在运行时给定等值选择条件下，对无环连接结果进行采样的复杂性。

Method: 针对合取条件，基于广泛接受的复杂性假设，提出一个二分判定准则，刻画是否存在O(IN)空间、O(1)采样时间的预计算结构，并在可行情形下构造最优结构；对于析取范式条件，将其核心挑战归约为集合并采样问题，并设计最优算法加以解决。

Result: 1）对合取条件情形，给出了存在O(IN)空间、O(1)采样时间结构的完整刻画，并在所有可行情况下构建了最优结构；2）对析取范式条件，提出了集合并采样的最优算法，并据此构建了最优采样结构；3）导出了关于随机枚举问题的新结果。

Conclusion: 该工作系统地解决了带等值选择条件的无环连接采样问题，为合取和析取两类条件分别提供了理论完备性和实践最优性的解决方案，同时推动了相关随机枚举问题的研究。

Abstract: Previous research on join sampling has focused on joins without selection conditions, even though such conditions are prevalent in everyday queries in database systems. Motivated by this, we undertake a systematic investigation on the complexity of sampling from the result of an acyclic join under equality conditions given only at runtime. When conditions are conjunctive, the goal is to understand when it is possible to precompute a feasible structure that uses O (IN) space and supports sampling in O (1) time, where IN is the input size. We present a dichotomy to characterize (subject to a widely-accepted conjecture) the existence of such structures based on the conditions supplied and, in every feasible scenario, give an optimal structure of O (IN) space and O (1) sample time. We then extend our investigation to conditions expressed in disjunctive normal form, where the core challenge reduces to the fundamental set union sampling problem. We overcome the challenge with an optimal algorithm and utilize it to develop optimal sampling structures. Our findings also lead to new results on the closely-related random enumeration problem.

</details>


### [9] [Lower Bounds for the Algorithmic Complexity of Learned Indexes](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.06629&hl=en&sa=X&d=5601705435374249079&ei=62NpaYfqKtrJieoPjeLumAg&scisig=AHkA5jTZGx1GkeGOJ8OP_R5Q8rWG&oi=scholaralrt&hist=Pxo5FIAAAAAJ:6303853285888070373:AHkA5jQg0xFJQ1T_sclEfqikcIfn&html=&pos=1&folt=rel)
*LA Croquevielle,R Sokolovskii,T Heinis*

Main category: Google Scholar

TL;DR: 该论文提出一个通用框架，用于证明学习型索引结构在给定空间开销和模型类下的查询时间下界，并利用逼近论工具（如量化和Kolmogorov宽度）揭示其固有的时空权衡。


<details>
  <summary>Details</summary>
Motivation: 尽管学习型索引在实践中表现良好，但其理论局限性尚未被充分理解，特别是关于查询时间与空间开销之间的权衡缺乏形式化分析。

Method: 作者构建了一个通用框架，将学习型索引建模为分段模型预测器；首先使用概率工具控制从分布中采样数据时的采样效应，然后分析如何用特定模型类最优地逼近累积分布函数，从而推导查询时间下界。

Result: 在多种建模假设和分布假设下（特别是分段线性和分段常数模型类），推导出了查询时间的理论下界，表明逼近论工具可有效刻画学习型索引的时空权衡。

Conclusion: 学习型索引存在由逼近能力与空间开销共同决定的根本性限制，其性能边界可通过逼近论方法严格刻画，为未来设计提供理论指导。

Abstract: Learned index structures aim to accelerate queries by training machine learning models to approximate the rank function associated with a database attribute. While effective in practice, their theoretical limitations are not fully understood. We present a general framework for proving lower bounds on query time for learned indexes, expressed in terms of their space overhead and parameterized by the model class used for approximation. Our formulation captures a broad family of learned indexes, including most existing designs, as piecewise model-based predictors. We solve the problem of lower bounding query time in two steps: first, we use probabilistic tools to control the effect of sampling when the database attribute is drawn from a probability distribution. Then, we analyze the approximation-theoretic problem of how to optimally represent a cumulative distribution function with approximators from a given model class. Within this framework, we derive lower bounds under a range of modeling and distributional assumptions, paying particular attention to the case of piecewise linear and piecewise constant model classes, which are common in practical implementations. Our analysis shows how tools from approximation theory, such as quantization and Kolmogorov widths, can be leveraged to formalize the space-time tradeoffs inherent to learned index structures. The resulting bounds illuminate core limitations of these methods.

</details>


### [10] [Database Theory in Action: Direct Access to Query Answers](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.06013&hl=en&sa=X&d=15361939681310771091&ei=62NpaYfqKtrJieoPjeLumAg&scisig=AHkA5jQxfzkEdYAo67lZ3g03S3Ix&oi=scholaralrt&hist=Pxo5FIAAAAAJ:6303853285888070373:AHkA5jQg0xFJQ1T_sclEfqikcIfn&html=&pos=2&folt=rel)
*J Hu,N Tziavelis*

Main category: Google Scholar

TL;DR: 该论文实现了支持多种查询和排序的直接访问功能，填补了理论研究与实际性能评估之间的空白，并探讨了数据库系统间的性能差异以及直接访问与其单次访问变体之间的关系。


<details>
  <summary>Details</summary>
Motivation: 尽管支持直接访问（即根据排名位置检索查询结果）的数据结构在时间复杂度方面已有深入研究，且针对许多查询和常见排序已存在高效算法，但其实际性能却鲜有关注。本文旨在通过实现覆盖广泛查询和排序的系统，填补这一研究空白。

Method: 作者实现了一个支持多种查询类型和排序方式的直接访问系统，借此开展实证研究，比较不同数据库系统的性能表现，并分析直接访问与单次访问之间的实际性能关系。

Result: 实验揭示了不同数据库系统在直接访问任务中的性能差异，并展示了直接访问与其单次访问形式在实践中的性能关联，提供了对理论算法实际效果的深入理解。

Conclusion: 该研究强调了将理论算法转化为实际系统的重要性，并表明直接访问的实际性能不仅依赖于算法复杂度，还受具体实现和系统环境影响，为未来优化和应用提供了实证基础。

Abstract: Direct access asks for the retrieval of query answers by their ranked position, given a query and a desired order. While the time complexity of data structures supporting such accesses has been studied in depth, and efficient algorithms for many queries and common orders are known, their practical performance has received little attention. We provide an implementation covering a wide range of queries and orders; it allows us to investigate intriguing practical aspects, including the comparative performance of database systems and the relationship between direct access and its single-access counterpart.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [11] [Multiverse: Transactional Memory with Dynamic Multiversioning](https://arxiv.org/abs/2601.09735)
*Gaetano Coccimiglio,Trevor Brown,Srivatsan Ravi*

Main category: cs.DB

TL;DR: Multiverse is a novel software transactional memory (STM) system that integrates both versioned and unversioned transactions to deliver high performance for common workloads while efficiently supporting long-running reads through multiversioning.


<details>
  <summary>Details</summary>
Motivation: Existing STMs struggle with long-running reads over frequently updated data; while multiversioning addresses this, it often degrades performance for transactions that don’t require versioning. There is a need for an STM that supports both types of workloads efficiently without compromising performance.

Method: The authors design and implement Multiverse, an STM that concurrently supports versioned and unversioned transactions. It leverages multiversioning only when necessary (for long-running reads) while maintaining the performance of state-of-the-art unversioned STMs for typical workloads.

Result: Experimental results show that Multiverse matches or exceeds the performance of existing STMs on standard workloads without long-running reads. For workloads involving long-running reads with frequent updates, Multiverse outperforms other STMs by orders of magnitude in throughput.

Conclusion: Multiverse successfully bridges the gap between unversioned and multiversioned STMs, offering high performance across diverse workloads and significantly improving scalability for long-running read scenarios.

Abstract: Software transactional memory (STM) allows programmers to easily implement concurrent data structures. STMs simplify atomicity. Recent STMs can achieve good performance for some workloads but they have some limitations. In particular, STMs typically cannot support long-running reads which access a large number of addresses that are frequently updated. Multiversioning is a common approach used to support this type of workload. However, multiversioning is often expensive and can reduce the performance of transactions where versioning is not necessary. In this work we present Multiverse, a new STM that combines the best of both unversioned TM and multiversioning. Multiverse features versioned and unversioned transactions which can execute concurrently. A main goal of Multiverse is to ensure that unversioned transactions achieve performance comparable to the state of the art unversioned STM while still supporting fast versioned transactions needed to enable long running reads. We implement Multiverse and compare it against several STMs. Our experiments demonstrate that Multiverse achieves comparable or better performance for common case workloads where there are no long running reads. For workloads with long running reads and frequent updates Multiverse significantly outperforms existing STMS. In several cases for these workloads the throughput of Multiverse is several orders of magnitude faster than other STMs.

</details>


### [12] [The "I" in FAIR: Translating from Interoperability in Principle to Interoperation in Practice](https://arxiv.org/abs/2601.10008)
*Evan Morris,Gaurav Vaidya,Phil Owen,Jason Reilly,Karamarie Fecho,Patrick Wang,Yaphet Kebede,E. Kathleen Carter,Chris Bizon*

Main category: cs.DB

TL;DR: 该论文提出了两个工具Babel和ORION，分别解决科学数据资源中标识符方案不一致和数据模型异构的问题，从而在实践中实现FAIR原则所倡导的互操作性，并构建了一个完全互操作的知识库集合。


<details>
  <summary>Details</summary>
Motivation: 尽管许多科学数据资源遵循FAIR原则并采用标准化词汇和元数据结构，但由于标识符方案和数据模型的差异，这些资源在实际应用中仍难以有效互操作。

Method: 开发了两个工具：Babel通过构建等价标识符的映射簇并提供高性能API来统一多种标识符体系；ORION则将不同知识库摄入并转换为一个社区共同维护的统一数据模型。

Result: Babel和ORION成功支持了多个知识库之间的数据互操作，并生成了一个可下载使用的完全互操作知识库库（https://robokop.renci.org）。

Conclusion: 通过Babel和ORION，可以在实践层面弥合FAIR原则中“互操作性”理念与实际系统间互操作能力之间的鸿沟，显著提升科学数据生态系统的集成效率与可用性。

Abstract: The FAIR (Findable, Accessible, Interoperable, and Reusable) data principles [1] promote the interoperability of scientific data by encouraging the use of persistent identifiers, standardized vocabularies, and formal metadata structures. Many resources are created using vocabularies that are FAIR-compliant and well-annotated, yet the collective ecosystem of these resources often fails to interoperate effectively in practice. This continued challenge is mainly due to variation in identifier schemas and data models used in these resources. We have created two tools to bridge the chasm between interoperability in principle and interoperation in practice. Babel solves the problem of multiple identifier schemes by producing a curated set of identifier mappings to create cliques of equivalent identifiers that are exposed through high-performance APIs. ORION solves the problems of multiple data models by ingesting knowledge bases and transforming them into a common, community-managed data model. Here, we describe Babel and ORION and demonstrate their ability to support data interoperation. A library of fully interoperable knowledge bases created through the application of Babel and ORION is available for download and use at https://robokop.renci.org.

</details>


### [13] [Redundancy-Driven Top-$k$ Functional Dependency Discovery](https://arxiv.org/abs/2601.10130)
*Xiaolong Wan,Xixian Han*

Main category: cs.DB

TL;DR: SDP is a novel algorithm for efficiently discovering the top-$k$ functional dependencies (FDs) in relational databases by ranking them based on redundancy count and employing pruning techniques with monotonic upper bounds, significantly outperforming exhaustive FD discovery methods in speed and memory usage.


<details>
  <summary>Details</summary>
Motivation: Existing FD discovery algorithms suffer from prohibitive computational costs and produce excessively large result sets, making it difficult to identify useful dependencies in large-scale or high-dimensional data.

Method: The authors propose SDP (Selective-Discovery-and-Prune), which discovers top-$k$ FDs ranked by redundancy count—a measure of duplicated information linked to storage overhead and update anomalies. SDP uses a monotone upper bound on redundancy to prune unpromising search branches. It incorporates three optimizations: attribute ordering by partition cardinality, use of a Partition Cardinality Matrix for tighter bounds, and a global scheduler to prioritize promising branches.

Result: Experiments on over 40 datasets demonstrate that SDP is significantly faster and consumes less memory compared to exhaustive FD discovery approaches.

Conclusion: SDP effectively addresses the scalability and usability issues of traditional FD discovery by focusing on the most informative dependencies and leveraging efficient pruning strategies grounded in theoretical guarantees of monotonicity.

Abstract: Functional dependencies (FDs) are basic constraints in relational databases and are used for many data management tasks. Most FD discovery algorithms find all valid dependencies, but this causes two problems. First, the computational cost is prohibitive: computational complexity grows quadratically with the number of tuples and exponentially with the number of attributes, making discovery slow on large-scale and high-dimensional data. Second, the result set can be huge, making it hard to identify useful dependencies. We propose SDP (Selective-Discovery-and-Prune), which discovers the top-$k$ FDs ranked by redundancy count. Redundancy count measures how much duplicated information an FD explains and connects directly to storage overhead and update anomalies. SDP uses an upper bound on redundancy to prune the search space. It is proved that this upper bound is monotone: adding attributes refines partitions and thus decreases the bound. Once the bound falls below the top-$k$ threshold, the entire branch can be skipped. We improve SDP with three optimizations: ordering attributes by partition cardinality, using pairwise statistics in a Partition Cardinality Matrix to tighten bounds, and a global scheduler to explore promising branches first. Experiments on over 40 datasets show that SDP is much faster and uses less memory than exhaustive methods.

</details>


### [14] [Improving Database Performance by Application-side Transaction Merging](https://arxiv.org/abs/2601.10596)
*Xueyuan Ren,Frank Li,Yang Wang*

Main category: cs.DB

TL;DR: 该论文提出通过在应用端合并结构相似的SQL语句或事务来提升事务处理性能，并设计了名为TransactionMerger的中间件及配套静态分析工具，在TPC-C和Spree中分别实现了最高2.65倍和3.52倍的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 传统事务处理系统在面对大量结构相似但独立提交的事务时，存在冗余操作和资源争用问题，限制了整体吞吐性能。作者旨在利用应用层语义信息，通过合并相似事务减少冗余、缓解争用，从而提升性能。

Method: 作者提出三种事务重写策略：1）利用特定SQL语义合并相似语句；2）消除冗余读取；3）跨事务预计算并合并冲突语句的聚合效果。为此，他们开发了TransactionMerger中间件用于跨客户端收集与合并事务，并构建静态分析工具以安全识别合并机会，确保不破坏事务隔离性。

Result: 在TPC-C基准测试中，事务合并使吞吐量最高提升2.65倍；在真实电商应用Spree中，吞吐量提升达3.52倍，验证了所提方法的有效性。

Conclusion: 在应用侧利用语义信息对结构相似事务进行合并是一种有效提升数据库事务处理性能的技术路径，TransactionMerger及其静态分析工具能够安全且显著地提高系统吞吐量。

Abstract: This paper explores a new opportunity to improve the performance of transaction processing at the application side by merging structurely similar statements or transactions. Concretely, we re-write transactions to 1) merge similar statements using specific SQL semantics; 2) eliminate redundant reads; and 3) merge contending statements across transactions by pre-computing their aggregated effect. Following this idea, we present the design of TransactionMerger, a middleware to collect and merge transactions across different clients. We further present a static analysis tool to identify the merging opportunity without violating isolation as well as our experience of re-writing transactions in TPC-C and Spree, a popular real-world application. Our evaluation shows that such transaction merging can improve TPC-C throughput by up to 2.65X and Spree throughput by 3.52X.

</details>


### [15] [Translating database mathematical schemes into relational database software applications with MatBase](https://arxiv.org/abs/2601.10604)
*Christian Mancas,Diana Christina Mancas*

Main category: cs.DB

TL;DR: 本文提出了一种将初等数学数据模型转换为关系模型及非关系约束的伪代码算法，并证明其高效、可靠、完备且最优，还通过家谱树子宇宙建模进行了应用示例。


<details>
  <summary>Details</summary>
Motivation: 为支持智能数据库管理系统 MatBase，需将数学数据模型自动、正确地转换为可实现的关系模型及配套的非关系约束。

Method: 设计并形式化一种伪代码算法，用于将（初等）数学数据模型方案翻译成关系模式和非关系约束集；并通过理论分析证明其性能特性；结合家谱树建模实例进行验证，并提供 SQL 与 VBA 实现样例。

Result: 算法被证明具有快速性、健壮性、完备性和最优性；成功应用于家谱树子宇宙的数学建模，并给出了若干非关系约束的 SQL 和 VBA 实现方式及通用开发指南。

Conclusion: 所提出的转换算法在理论和实践上均有效，能够支撑 MatBase 系统对复杂数学数据模型的自动化关系实现与约束维护。

Abstract: We present a pseudocode algorithm for translating our (Elementary) Mathematical Data Model schemes into relational ones and associated sets of non-relational constraints, used by MatBase, our intelligent database management system prototype. We prove that this algorithm is very fast, solid, complete, and optimal. We apply it to a mathematical scheme modeling the genealogical trees subuniverse. We also provide examples of SQL and VBA code for enforcing some of its non-relational constraints, as well as guidelines to develop code for enforcing such constraints.

</details>


<div id='"query optimization"'></div>

# "query optimization" [[Back]](#toc)

### [16] [Vextra: A Unified Middleware Abstraction for Heterogeneous Vector Database Systems](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.06727&hl=zh-CN&sa=X&d=8980995946675536176&ei=57xoadTfKdvWieoPub_VeA&scisig=AHkA5jQ12Jic4IfoET_izminIYut&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=0&folt=kw-top)
*C Suri,G Bhasin*

Main category: "query optimization"

TL;DR: Vextra is a middleware abstraction layer that provides a unified API for vector databases, addressing API fragmentation and enabling portability and interoperability with minimal performance overhead.


<details>
  <summary>Details</summary>
Motivation: The proliferation of specialized vector databases for AI applications like RAG has led to severe API fragmentation, causing vendor lock-in, reduced portability, and increased maintenance costs for developers.

Method: Vextra implements a pluggable adapter architecture that translates a single, high-level unified API—supporting upsertion, similarity search, and metadata filtering—into the native protocols of various backend vector databases.

Result: Vextra successfully abstracts away backend-specific APIs while maintaining minimal performance overhead, enabling application portability and laying the groundwork for advanced query optimization.

Conclusion: A standardized middleware abstraction like Vextra is essential for the maturation of the vector database ecosystem, promoting interoperability, reducing vendor lock-in, and supporting future innovations in query processing.

Abstract: The rapid integration of vector search into AI applications, particularly for Retrieval Augmented Generation (RAG), has catalyzed the emergence of a diverse ecosystem of specialized vector databases. While this innovation offers a rich choice of features and performance characteristics, it has simultaneously introduced a significant challenge: severe API fragmentation. Developers face a landscape of disparate, proprietary, and often volatile API contracts, which hinders application portability, increases maintenance overhead, and leads to vendor lock-in. This paper introduces Vextra, a novel middleware abstraction layer designed to address this fragmentation. Vextra presents a unified, high-level API for core database operations, including data upsertion, similarity search, and metadata filtering. It employs a pluggable adapter architecture to translate these unified API calls into the native protocols of various backend databases. We argue that such an abstraction layer is a critical step towards maturing the vector database ecosystem, fostering interoperability, and enabling higher-level query optimization, while imposing minimal performance overhead.

</details>


### [17] [AI and Machine Learning in ADB-S](https://scholar.google.com/scholar_url?url=https://link.springer.com/chapter/10.1007/979-8-8688-2068-7_4&hl=zh-CN&sa=X&d=4822484471371236532&ei=57xoadTfKdvWieoPub_VeA&scisig=AHkA5jRDU9RZExG-HGCaj3gEZHx6&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=1&folt=kw-top)
*A Chowdhury*

Main category: "query optimization"

TL;DR: Oracle ADB-S利用AI驱动的查询优化，可自动适应实时工作负载并持续优化执行策略，显著提升企业（如DataMartX）数据库性能。


<details>
  <summary>Details</summary>
Motivation: 传统数据库查询优化难以应对动态变化的工作负载，企业需要更智能、自适应的优化机制以提升性能和效率。

Method: 在Oracle Autonomous Database Serverless (ADB-S)中引入AI驱动的查询优化技术，通过实时分析工作负载并自动调整执行计划。

Result: 系统能够动态适应查询模式变化，持续改进执行策略，从而显著提高查询性能和资源利用率。

Conclusion: AI赋能的查询优化在Oracle ADB-S中为企业提供了高效、自适应的数据库性能管理能力，具有显著实用价值。

Abstract: … In summary, AI-powered query optimization in Oracle ADB-S offers a significant advantage for enterprises like DataMartX. By automatically adapting to real-time workloads and continuously evolving execution strategies, the system enhances …

</details>


### [18] [Керування даними часових рядів у системах розумного дому: баланс між аналітикою в реальному часі та зберіганням даних](https://scholar.google.com/scholar_url?url=https://oeipt.vntu.edu.ua/index.php/oeipt/article/download/796/726&hl=zh-CN&sa=X&d=2930459416488284744&ei=57xoadTfKdvWieoPub_VeA&scisig=AHkA5jR4ly1rAP_sJDd97dRcyC1O&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=3&folt=kw-top)
*МВ Талах,ВВ Дворжак,ЮО Ушенко*

Main category: "query optimization"

TL;DR: 本文提出了一种用于智能家居时序数据管理的混合架构，结合Azure CosmosDB实现实时处理与Azure Synapse支持历史数据分析，在查询性能、存储效率和可扩展性方面取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 在智能家居系统中，需在实时分析与时序历史数据高效存储之间取得平衡，传统单一数据库架构难以同时满足低延迟查询与高吞吐写入及压缩存储的需求。

Method: 采用混合架构：使用Azure CosmosDB处理实时数据流，并利用Azure Synapse进行历史数据分析；引入基于状态变化的写入策略和间隔存储机制以优化数据量并保持时间完整性。

Result: 系统实现高达165倍的分析查询加速（从3.3秒降至20毫秒），读取操作减少17倍；CosmosDB与Synapse分别达到5:1和10:1的数据压缩比，并支持每秒百万级写入及线性扩展能力。

Conclusion: 所提出的混合架构有效兼顾了实时性、存储效率与可扩展性，为大规模智能家居时序数据管理提供了高性能、高效益的解决方案。

Abstract: Стаття представляє новий підхід до управління даними часових рядів у системах розумного будинку, зосереджуючись на балансі між аналітикою в реальному часі та ефективним зберіганням історичних даних. Гібридна архітектура, яка поєднує Azure CosmosDB для обробки даних у реальному часі та Azure Synapse для аналітики історичних даних, демонструє значні переваги в продуктивності. Система досягає прискорення до 165 разів у виконанні аналітичних запитів (від 3, 3 секунди до 20 мілісекунд для запитів агрегації часових рядів) при одночасному зменшенні кількості операцій читання в 17 разів. Впровадження оптимізаційних стратегій, таких як запис на основі зміни стану та інтервальне зберігання, значно зменшує обсяг даних при збереженні темпоральної цілісності даних. Система демонструє лінійну масштабованість, здатність обробляти до 1 мільйона операцій запису на секунду та досягає коефіцієнтів стиснення до 5: 1 у CosmosDB та 10: 1 у Synapse.

</details>


### [19] [FOSS4G-ASIA 2026](https://scholar.google.com/scholar_url?url=https://talks.geoinfo-lab.org/foss4g-asia-2026/featured/&hl=zh-CN&sa=X&d=11674002329854998717&ei=57xoadTfKdvWieoPub_VeA&scisig=AHkA5jR_A1udMMlI_I38qMrwq2cx&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=4&folt=kw-top)
*R Hataitara,K Piyathamrongchai*

Main category: "query optimization"

TL;DR: 该论文研究如何通过优化矢量瓦片的数据效率、空间查询和样式感知剪枝，使瓦片服务器在不牺牲视觉保真度的前提下提供更小、更快、更相关的瓦片。


<details>
  <summary>Details</summary>
Motivation: 现有矢量瓦片服务在传输效率与视觉保真度之间存在权衡，亟需一种方法在减少数据量和提升响应速度的同时，保持地图渲染的视觉质量。

Method: 结合矢量瓦片数据效率优化、空间查询加速以及样式感知的数据剪枝技术，对瓦片内容进行智能裁剪与压缩。

Result: 实现了在不降低视觉保真度的前提下，显著减小瓦片体积、加快传输速度，并提升瓦片内容的相关性。

Conclusion: 通过融合多种优化策略，所提出的方法有效提升了矢量瓦片服务的性能与用户体验，为高效地图服务提供了可行路径。

Abstract: … Building on research into vector-tile data efficiency, spatial query optimization, and style-aware data pruning, we explore how tile servers can deliver smaller, faster, and more relevant tiles without sacrificing visual fidelity. This includes techniques …

</details>


### [20] [4 Biological Databases and Data Mining](https://scholar.google.com/scholar_url?url=https://books.google.com/books%3Fhl%3Dzh-CN%26lr%3D%26id%3DPNitEQAAQBAJ%26oi%3Dfnd%26pg%3DPT61%26dq%3D%2522query%2Boptimization%2522%26ots%3D8Nfjp2SMVp%26sig%3D8ME7ef1CurYYamZf6JyGVX2TE2E&hl=zh-CN&sa=X&d=11975453057803421153&ei=57xoadTfKdvWieoPub_VeA&scisig=AHkA5jQlQofr9YAqqKFNFHtV5Cps&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=5&folt=kw-top)
*Y Singh*

Main category: "query optimization"

TL;DR: 本文探讨了生物数据库在生物信息学中的核心作用，强调其在存储、管理和检索海量生物数据（如基因组、转录组和外显子组数据）方面的重要性，并指出传统数据处理方法已无法应对当前数据爆炸式增长的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着技术进步，生物数据呈指数级增长，传统数据处理系统难以有效管理如此庞大的数据量，因此亟需高效的数据挖掘与数据库管理方法来支持生物信息学研究。

Method: 文章综述了生物数据库的概念、设计、开发与长期管理，结合数据挖掘（KDD）、机器学习、深度学习和统计学等技术，阐述其在生物信息学中的应用。

Result: 明确了生物数据库作为生物信息学核心基础设施的地位，能够系统化存储、访问和更新来自基因组学等领域的海量实验数据。

Conclusion: 生物数据库对于现代生物科学和生物信息学至关重要，其发展依赖于先进的数据管理与挖掘技术，以应对日益增长的生物数据挑战。

Abstract: The advancement in the field of technology significantly impacts the lives of individuals, primarily through business data processing and scientific computing. It opens the path for exponential growth in collecting data. The concept of data mining arises since traditional data mining systems fail to process large amounts of data. Data mining is the process called knowledge discovery from databases (KDD). Data mining has roots in databases, machine learning (ML), deep learning, and statistics and has contributions toward many other areas [1]. In this chapter, we explore the biological database, which is a repository used to store biological information. Biology and biological data are computed in the field of bioinformatics, an interdisciplinary science of analyzing biological data using information technology and computer science. The collection of data from the biological world is known as biological data, and the system that is used to store, search, and retrieve biological data is known as a biological database. Biological database design, development, and long-term management are core areas of the discipline of bioinformatics (http://en. wikipedia. org/wiki/Biological_database). It is the repository of biological information [2].
A database is a platform where all the data is stored systematically and can be accessed and updated occasionally. It stores a massive amount of data generated in experiments of genome, transcriptome, and exome sequences of various organisms in current times; the biological data is stored exponentially [3]. The availability of enormous amounts of biological data (sequences and structural data) has generated a need for managing, storing, and retrieving these vast data. These data have played a fundamental role in bioscience, particularly in bioinformatics, where we can

</details>
