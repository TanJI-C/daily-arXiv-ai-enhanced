<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 6]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [From Metadata to Meaning: A Semantic Units Knowledge Graph for the Biodiversity Exploratories](https://arxiv.org/abs/2601.00002)
*Tarek Al Mustafa*

Main category: cs.DB

TL;DR: 该论文提出“语义单元”（Semantic Units, SUs）作为知识图谱中具有语义意义的命名子图，以提升生态与生物多样性研究者对知识图谱的认知互操作性，并通过在德国Biodiversity Exploratories平台构建的知识图谱上首次实现SUs，同时探索大语言模型和嵌入模型在结构化元数据提取与增强中的应用。


<details>
  <summary>Details</summary>
Motivation: 当前知识图谱虽在生态与生物多样性研究中具有整合、推理和机器互操作潜力，但其SPARQL查询语言对多数用户不友好，且图谱内容常因强调机器可读性而缺乏对终端用户的语义意义，导致用户需求与技术需求之间存在断层。

Method: 作者基于Biodiversity Exploratories的出版物与数据集元数据构建知识图谱，引入并实现语义单元（SUs）作为提升用户认知互操作性的手段；同时利用大语言模型从标题和摘要中抽取结构化元数据类别，并使用嵌入模型丰富元数据以支持FAIR原则。

Result: 成功在真实知识图谱上实现了语义单元，并展示了其对查询体验的改进；同时验证了大语言模型和嵌入模型在自动化生成结构化、FAIR兼容元数据方面的有效性。

Conclusion: 语义单元为弥合知识图谱技术能力与用户语义需求之间的鸿沟提供了可行路径，结合大语言模型与嵌入技术可有效支持生态学领域知识图谱的构建与应用，推动研究数据的FAIR化。

Abstract: Knowledge Graphs (KGs) bear great potential for ecology and biodiversity researchers in their ability to support synthesis and integration efforts, meta-analyses, reasoning tasks, and overall machine interoperability of research data. However, this potential is yet to be realized as KGs are notoriously difficult to interact with via their query language SPARQL for many user groups alike. Additionally, a further hindrance for user-KG interaction is the fundamental disconnect between user requirements and requirements KGs have to fulfill regarding machine-interoperability, reasoning tasks, querying, and further technical requirements. Thus, many statements in a KG are of no semantic significance for end users. In this work, we investigate a potential remedy for this challenge: Semantic Units (SUs) are semantically significant, named subgraphs in a KG with the goal to enhance cognitive interoperability for users, and to provide responses to common KG modelling challenges. We model and construct a KG from publication and dataset metadata of the Biodiversity Exploratories (BE), a research platform for functional biodiversity research across research plots in Germany to contribute to biodiversity research from the perspective of computer science. We contribute further by delivering the first implementation of semantic units on a knowledge graph and investigate how SUs impact KG querying. Finally, we present two implementations of tasks that show how large language models (LLMs) can be used to extract structured metadata categories from publication and dataset titles and abstracts, and how embedding models can be used to enrich metadata with latent information, in an effort to support the creation of structured and FAIR (findable, accessible, interoperable, and reusable) metadata.

</details>


### [2] [Database Theory in Action: Yannakakis' Algorithm](https://arxiv.org/abs/2601.00098)
*Paraschos Koutris,Stijn Vansummeren,Qichen Wang,Yisu Remy Wang,Xiangyao Yu*

Main category: cs.DB

TL;DR: 本文简要综述了近期在提升Yannakakis算法实用性方面的进展，包括效率和实现简易性，并指出了若干未来研究方向。


<details>
  <summary>Details</summary>
Motivation: Yannakakis的经典算法虽在理论上对无环连接是最优的，但由于其在实际应用中性能不佳，尚未被广泛采用。

Method: 综述近期改进Yannakakis算法实用性的技术进展。

Result: 总结了提高该算法效率与易实现性的若干方法。

Conclusion: 尽管已有显著进展，Yannakakis算法的广泛应用仍需进一步研究，文中提出了多个潜在的研究方向。

Abstract: Yannakakis' seminal algorithm is optimal for acyclic joins, yet it has not been widely adopted due to its poor performance in practice. This paper briefly surveys recent advancements in making Yannakakis' algorithm more practical, in terms of both efficiency and ease of implementation, and points out several avenues for future research.

</details>


### [3] [Avoiding Thread Stalls and Switches in Key-Value Stores: New Latch-Free Techniques and More](https://arxiv.org/abs/2601.00208)
*David Lomet,Rui Wang*

Main category: cs.DB

TL;DR: 该论文提出了一种名为“notices”的新型无锁机制，通过结合增量记录更新显著减少无锁技术中的无效计算，从而缓解键值存储中因资源争用导致的线程切换或停顿问题，并有效解决B树索引维护中的并发挑战。


<details>
  <summary>Details</summary>
Motivation: 键值存储在高负载下因资源争用导致频繁的线程切换或停顿，严重影响性能；传统基于锁的方法会阻塞线程，而现有无锁方法可能因执行大量无效工作而收益有限。

Method: 引入一种新的无锁机制“notices”，结合增量记录（delta record）更新策略，在避免线程阻塞的同时显著减少无锁操作中的无效计算。

Result: 所提方法能有效解决B树索引维护中的并发问题，并避免线程切换或停顿，提升系统在高负载下的性能表现。

Conclusion: 通过“notices”机制，无锁技术可在减少无效工作的前提下高效处理资源争用，为高性能键值存储提供可行路径，并为其他避免线程切换/停顿的场景提供思路。

Abstract: A significant impediment to high performance in key-value stores is the high cost of thread switching or stalls. While there are many sources for this, a major one is the contention for resources. And this cost increases with load as conflicting operations more frequently try to access data concurrently. Traditional latch-based approaches usually handle these situations by blocking one or more contending threads. Latch-free techniques can avoid this behavior. But the payoff may be limited if latch-free techniques require executing wasted work. In this paper, we show how latch-free techniques exploit delta record updating and can significantly reduce wasted work by using notices, a new latch-free approach. This paper explains how notices work and can solve B-tree index maintenance problems, while avoiding thread switches or stalls. Other opportunities for avoiding thread switches or stalls are also discussed.

</details>


### [4] [Combining Time-Series and Graph Data: A Survey of Existing Systems and Approaches](https://arxiv.org/abs/2601.00304)
*Mouna Ammar,Marvin Hofer,Erhard Rahm*

Main category: cs.DB

TL;DR: 本文综述了图数据与时序数据融合的现有方法与系统，将其划分为四类架构，并分析了各类系统在跨模型集成度、成熟度和开放性等方面的特性与权衡。


<details>
  <summary>Details</summary>
Motivation: 当前图数据与时序数据的融合需求日益增长，但缺乏对现有系统架构及其特性的系统性梳理，因此需要一个全面的综述以帮助研究者和实践者理解并评估不同方案。

Method: 对现有图与时序数据融合系统进行分类，归纳为四种架构类型，并从跨模型集成程度、系统成熟度、开放性等维度进行分析和比较。

Result: 明确了四类系统架构的特征及其在支持图与时序数据统一处理方面的差异，揭示了各类系统在实际应用中的优势与局限。

Conclusion: 该综述有助于读者理解当前图与时序数据融合系统的格局，为系统选型和未来研究提供参考依据。

Abstract: We provide a comprehensive overview of current approaches and systems for combining graphs and time series data. We categorize existing systems into four architectural categories and analyze how these systems meet different requirements and exhibit distinct implementation characteristics to support both data types in a unified manner. Our overview aims to help readers understand and evaluate current options and trade-offs, such as the degree of cross-model integration, maturity, and openness.

</details>


### [5] [KELP: Robust Online Log Parsing Through Evolutionary Grouping Trees](https://arxiv.org/abs/2601.00633)
*Satyam Singh,Sai Niranjan Ramachandran*

Main category: cs.DB

TL;DR: KELP is a novel real-time log parser based on an Evolutionary Grouping Tree that continuously adapts to schema drifts via online clustering, outperforming static heuristic parsers on a new realistic benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing online log parsers rely on static template models that break silently under minor schema changes in dynamic production environments, causing missed alerts and operational overhead.

Method: KELP uses an Evolutionary Grouping Tree data structure to perform continuous online clustering of incoming logs, dynamically splitting, merging, and re-evaluating tree nodes based on evolving frequency distributions.

Result: KELP achieves high parsing accuracy on a newly introduced benchmark that captures realistic structural ambiguity in production logs, while maintaining high throughput—outperforming traditional heuristic methods that fail under schema drift.

Conclusion: KELP effectively addresses the brittleness of current log parsers by enabling adaptive, real-time template discovery, and sets a new standard for evaluation with a more realistic benchmark.

Abstract: Real-time log analysis is the cornerstone of observability for modern infrastructure. However, existing online parsers are architecturally unsuited for the dynamism of production environments. Built on fundamentally static template models, they are dangerously brittle: minor schema drifts silently break parsing pipelines, leading to lost alerts and operational toil. We propose \textbf{KELP} (\textbf{K}elp \textbf{E}volutionary \textbf{L}og \textbf{P}arser), a high-throughput parser built on a novel data structure: the Evolutionary Grouping Tree. Unlike heuristic approaches that rely on fixed rules, KELP treats template discovery as a continuous online clustering process. As logs arrive, the tree structure evolves, nodes split, merge, and re-evaluate roots based on changing frequency distributions. Validating this adaptability requires a dataset that models realistic production complexity, yet we identify that standard benchmarks rely on static, regex-based ground truths that fail to reflect this. To enable rigorous evaluation, we introduce a new benchmark designed to reflect the structural ambiguity of modern production systems. Our evaluation demonstrates that KELP maintains high accuracy on this rigorous dataset where traditional heuristic methods fail, without compromising throughput. Our code and dataset can be found at codeberg.org/stonebucklabs/kelp

</details>


### [6] [DeXOR: Enabling XOR in Decimal Space for Streaming Lossless Compression of Floating-point Data](https://arxiv.org/abs/2601.00695)
*Chuanyi Lv,Huan Li,Dingyu Yang,Zhongle Xie,Lu Chen,Christian S. Jensen*

Main category: cs.DB

TL;DR: DeXOR 是一种面向流式浮点数的新型压缩框架，通过十进制异或（decimal XOR）机制高效编码十进制空间中的最长公共前缀与后缀，在多种极端条件下仍保持高压缩率和快速解压性能。


<details>
  <summary>Details</summary>
Motivation: 随着流式浮点数据日益普遍，亟需能够有效利用连续数值之间相似性（平滑性）并应对高精度或非平滑等极端情况的压缩方案。

Method: DeXOR 引入十进制异或操作以编码十进制空间中的最长公共前缀和后缀；采用带误差容忍舍入的缩放截断策略及针对十进制异或优化的位管理策略，确保低开销、高精度解压；并通过鲁棒的异常处理机制管理浮点指数，提升极端条件下的稳定性。

Result: 在22个数据集上的评估表明，DeXOR 相较于现有最先进方法，压缩率提高15%，解压速度提升20%，压缩速度具有竞争力，并在极端场景下展现出更强的鲁棒性和可扩展性。

Conclusion: DeXOR 在保证高效压缩与解压的同时，显著提升了对高精度和非平滑浮点流数据的适应能力，是当前浮点数据压缩领域的一项重要进展。

Abstract: With streaming floating-point numbers being increasingly prevalent, effective and efficient compression of such data is critical. Compression schemes must be able to exploit the similarity, or smoothness, of consecutive numbers and must be able to contend with extreme conditions, such as high-precision values or the absence of smoothness. We present DeXOR, a novel framework that enables decimal XOR procedure to encode decimal-space longest common prefixes and suffixes, achieving optimal prefix reuse and effective redundancy elimination. To ensure accurate and low-cost decompression even with binary-decimal conversion errors, DeXOR incorporates 1) scaled truncation with error-tolerant rounding and 2) different bit management strategies optimized for decimal XOR. Additionally, a robust exception handler enhances stability by managing floating-point exponents, maintaining high compression ratios under extreme conditions. In evaluations across 22 datasets, DeXOR surpasses state-of-the-art schemes, achieving a 15% higher compression ratio and a 20% faster decompression speed while maintaining a competitive compression speed. DeXOR also offers scalability under varying conditions and exhibits robustness in extreme scenarios where other schemes fail.

</details>
