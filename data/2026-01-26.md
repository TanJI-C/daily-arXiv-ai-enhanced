<div id=toc></div>

# Table of Contents

- [Ziniu Wu](#Ziniu Wu) [Total: 3]
- ["query optimization"](#"query optimization") [Total: 4]
- [learned "cost model"](#learned "cost model") [Total: 5]
- [cs.DB](#cs.DB) [Total: 4]
- [Google Scholar](#Google Scholar) [Total: 2]


<div id='Ziniu Wu'></div>

# Ziniu Wu [[Back]](#toc)

### [1] [Is Quantum Computing Ready for Real-Time Database Optimization?](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.12123&hl=zh-CN&sa=X&d=9285225533576205973&ei=fkp1aeqDJ9_YieoP1b6liQE&scisig=AHkA5jRDfK0IRE9TLF2LMuEpLbGM&oi=scholaralrt&hist=Pxo5FIAAAAAJ:11734074576040036222:AHkA5jT4qH64J8nZejvQlyvZo1xT&html=&pos=0&folt=rel)
*H Liu,I Sabek*

Main category: Ziniu Wu

TL;DR: 本文提出Q2O——首个实时量子增强查询优化器，通过将连接顺序问题编码为非线性模型并利用低延迟量子退火求解器（NL-Solver）生成计划提示，引导PostgreSQL优化器生成完整执行计划。


<details>
  <summary>Details</summary>
Motivation: 随着数据量和工作负载复杂度增加，传统数据库优化任务（如连接顺序选择）变得计算困难；尽管量子退火在理论上可高效探索大规模搜索空间，但早期研究因高开销未能实现与数据库系统的实际集成。近期低延迟量子求解器（如NL-Solver）的出现使实时集成成为可能，但需解决效率与解质量之间的权衡问题。

Method: 将连接顺序问题建模为非线性优化模型，利用真实数据库统计信息进行编码，并通过D-Wave的NL-Solver求解；所得解转化为计划提示（plan hint），引导PostgreSQL优化器生成最终执行计划。

Result: Q2O能实时处理真实查询，在保持解质量的同时实现与现有DBMS（PostgreSQL）的实际集成，验证了量子-经典混合优化在数据库系统中的可行性。

Conclusion: 借助新兴低延迟量子退火服务，量子增强查询优化可在实际数据库系统中实现，且在效率与解质量之间取得可行平衡，为未来数据库优化提供了新范式。

Abstract: Database systems encompass several performance-critical optimization tasks, such as join ordering and index tuning. As data volumes grow and workloads become more complex, these problems have become exponentially harder to solve efficiently. Quantum computing, especially quantum annealing, is a promising paradigm that can efficiently explore very large search spaces through quantum tunneling. It can escape local optima by tunneling through energy barriers rather than climbing over them. Earlier works mainly focused on providing an abstract representation (e.g., Quadratic Unconstrained Binary Optimization (QUBO)) for the database optimization problems (e.g., join order) and overlooked the real integration within database systems due to the high overhead of quantum computing services (e.g., a minimum 5s runtime for D-Wave's CQM-Solver). Recently, quantum annealing providers have offered more low-latency solutions, e.g., NL-Solver, which paves the road to actually realizing quantum solutions within DBMSs. However, this raises new systems research challenges in balancing efficiency and solution quality. In this talk, we show that this balance is possible to achieve. As a proof of concept, we present Q2O, the first real Quantum-augmented Query Optimizer. We show the end-to-end workflow: we encode the join order problem as a nonlinear model, a format solvable by the NL-Solver, using actual database statistics; the solution is translated into a plan hint that guides PostgreSQL's optimizer to produce a complete plan. Q2O is capable of handling actual queries in real time.

</details>


### [2] [Improving Database Performance by Application-side Transaction Merging](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.10596&hl=zh-CN&sa=X&d=18293936042235188727&ei=y4t2abaEPd_YieoP1b6liQE&scisig=AHkA5jTGI98Q1lCqUsxFiAPn79AS&oi=scholaralrt&hist=Pxo5FIAAAAAJ:11734074576040036222:AHkA5jT4qH64J8nZejvQlyvZo1xT&html=&pos=0&folt=rel)
*X Ren,F Li,Y Wang*

Main category: Ziniu Wu

TL;DR: 该论文提出通过在应用端合并结构相似的SQL语句或事务来提升事务处理性能，并设计了中间件TransactionMerger实现跨客户端事务合并，在TPC-C和Spree中分别实现最高2.65倍和3.52倍的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 传统事务处理系统在高并发场景下常因冗余操作和事务竞争导致性能瓶颈。作者观察到许多应用中的事务具有结构相似性，因此希望通过在应用侧合并这些事务，减少冗余读写、利用SQL语义优化执行逻辑，从而提升整体吞吐量。

Method: 作者提出TransactionMerger中间件，其核心方法包括：1）利用特定SQL语义合并结构相似的语句；2）消除冗余读操作；3）跨事务合并存在竞争的语句，通过预计算其聚合效果。同时开发了静态分析工具，用于在不违反隔离性的前提下识别可合并的事务。

Result: 在TPC-C基准测试中，事务合并使吞吐量提升最高达2.65倍；在真实电商应用Spree中，吞吐量提升高达3.52倍，验证了该方法在实际场景中的有效性。

Conclusion: 通过在应用层对结构相似事务进行语义感知的合并，可显著提升数据库系统的事务处理性能，且无需修改底层数据库引擎，具备良好的实用性和可部署性。

Abstract: This paper explores a new opportunity to improve the performance of transaction processing at the application side by merging structurely similar statements or transactions. Concretely, we re-write transactions to 1) merge similar statements using specific SQL semantics; 2) eliminate redundant reads; and 3) merge contending statements across transactions by pre-computing their aggregated effect. Following this idea, we present the design of TransactionMerger, a middleware to collect and merge transactions across different clients. We further present a static analysis tool to identify the merging opportunity without violating isolation as well as our experience of re-writing transactions in TPC-C and Spree, a popular real-world application. Our evaluation shows that such transaction merging can improve TPC-C throughput by up to 2.65X and Spree throughput by 3.52X.

</details>


### [3] [Towards Privacy-Preserving Top-k Location-Based Dominating Queries over Encrypted Data](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11347495/&hl=zh-CN&sa=X&d=13932546995748641347&ei=y4t2abaEPd_YieoP1b6liQE&scisig=AHkA5jRPXi6O8uZVssFfBgY_EwZx&oi=scholaralrt&hist=Pxo5FIAAAAAJ:11734074576040036222:AHkA5jT4qH64J8nZejvQlyvZo1xT&html=&pos=1&folt=rel)
*Z Wang,X Ding,W Song,P Zhou,L Chen,Y Tian…*

Main category: Ziniu Wu

TL;DR: 本文提出STLD方案，通过构建安全聚合R树（SAR-tree）和配套子协议，在保障数据、查询、结果及访问模式隐私的前提下，高效支持加密数据上的安全Top-k位置主导查询，相比现有方法降低40%-60%的查询开销。


<details>
  <summary>Details</summary>
Motivation: 随着中小企业将数据迁移到云平台，出于隐私保护需求常对数据加密，但加密后难以高效执行兼具Top-k与Skyline优势的位置主导查询。现有隐私保护方案在处理此类查询时效率低下，限制了其在位置服务中的实用性。

Method: 设计基于Paillier密码系统和精心构造噪声的安全聚合R树（SAR-tree），融合聚合R树与半盲R树思想；在此基础上开发一系列安全子协议，并引入优化技术以减少主导操作带来的计算延迟，从而实现高效且隐私保护的Top-k位置主导查询。

Result: STLD在理论上保证了数据集、查询、结果和访问模式的隐私安全，并通过实验验证其可行性，查询成本较多种对比方法降低40%-60%。

Conclusion: STLD为加密云数据上的Top-k位置主导查询提供了一种高效且安全的解决方案，显著提升了查询性能，同时满足强隐私保护要求，适用于实际的位置服务场景。

Abstract: With the growth of cloud computing infrastructure, its cost-efficient paradigm is driving a growing wave of small and medium-sized enterprises to migrate data and services to cloud based platforms. However, due to privacy concerns, data encryption prior to outsourcing is an important means of protection, which in turn requires performing queries over the encrypted data. While several approaches do offer support for privacy preserving skyline or top-k queries, they typically struggle with efficiency when extended to secure top-k dominating queries, due to the inherent nature of combining the advantages of both top k and skyline queries. Nevertheless, this nature renders them as a more practical and promising alternative for location-based services. To address this, we introduce STLD, a secure top-k location-based dominating query scheme. Specifically, we develop an innovative index structure called Secure Aggregate R-tree (SAR-tree) by utilizing the Paillier cryptosystem and introducing meticulously crafted noise, while also incorporating principles from aggregate R-trees and semi-blind R-trees. Leveraging this structure, we propose a series of secure sub-protocols to facilitate top-k dominating queries, accompanied by optimization techniques to mitigate latency associated with computationally intensive dominating operations. Given an encrypted query, STLD not only efficiently answers the query but also guarantees the privacy of data(sets), results, queries and access patterns. Finally, STLD undergoes rigorous theoretical security and complexity analysis, complemented by empirical evaluations that demonstrate its performance and feasibility, achieving a reduction in query cost by 40%-60% compared to multiple competing methods.

</details>


<div id='"query optimization"'></div>

# "query optimization" [[Back]](#toc)

### [4] [Optimization of Acceleration for Multi-Table Association Queries in Data Warehouse Based on CNN-LSTM](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11324470/&hl=zh-CN&sa=X&d=2124117267657143476&ei=w5V1acivNcyQieoPxcX9yAo&scisig=AHkA5jTU5vtjn3L2paD0vwOkVfN5&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=0&folt=kw-top)
*G Deng,H Fan*

Main category: "query optimization"

TL;DR: 本文提出一种基于CNN-LSTM混合深度学习模型的智能优化框架，用于提升数据仓库中多表关联查询的性能，在TPC-H基准测试中最高实现1.5倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统查询优化方法在动态查询负载和复杂数据分布下适应性差，难以满足实际应用中对高效数据分析的需求，亟需更智能的优化策略。

Method: 构建CNN-LSTM混合模型，利用CNN提取查询计划的空间特征，LSTM捕捉历史负载的时间规律，通过端到端训练从多维数据中自动学习最优查询策略。

Result: 在TPC-H基准测试中，该方法在简单查询和复杂多表关联场景下均显著提升性能，最大加速比达1.5倍。

Conclusion: 基于深度学习的查询优化方法为智能查询处理提供了新路径，兼具理论价值与实践意义。

Abstract: The performance optimization of multi-table association query in data warehouse is a key challenge to improve the efficiency of enterprise data analysis. Traditional optimization methods are often limited in the face of dynamic query loads and data distributions, and struggle to adapt to complex practical application scenarios. By introducing the CNN-LSTM hybrid deep learning model, an intelligent optimization framework is constructed, which can simultaneously process the spatial characteristics of query plans and the temporal rules of historical loads. The model is trained in an end-to-end manner and acquires the capability of automatically learning the optimal query strategy from multi-dimensional data. The experimental results in the TPC-H benchmark show that the proposed method can achieve significant performance improvement in both simple query and complex multi-table association scenarios, with a maximum acceleration of 1.5 times. This optimization method based on deep learning provides a new technical path for intelligent query processing, which has important theoretical value and practical significance.

</details>


### [5] [Optimization in Information Retrieval: A Systematic Review of Techniques for Performance and Scalability](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/6287639/11323511/11314520.pdf&hl=zh-CN&sa=X&d=15890888558882500856&ei=w5V1acivNcyQieoPxcX9yAo&scisig=AHkA5jRjm_8X_oiWQffT9T_sD4q1&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=1&folt=kw-top)
*AT Bocces,A Baldassin,DCG Pedronette…*

Main category: "query optimization"

TL;DR: 本文系统综述了2018至2024年间信息检索（IR）系统在内存管理、查询执行和算法效率方面的优化方法，强调其对提升响应时间、资源利用率和可扩展性的关键作用。


<details>
  <summary>Details</summary>
Motivation: 随着数据规模和计算需求不断增长，信息检索系统不仅需保证检索效果，还需在效率与可扩展性方面持续优化，以支持实时和大规模处理需求。

Method: 通过系统性综述2018–2024年相关研究，归纳并分析提升IR性能的关键优化技术，包括内存管理、查询执行改进和算法效率提升等方向。

Result: 识别出多种显著改善IR系统响应时间、资源利用效率和可扩展性的优化策略，为研究者和实践者提供全面的技术概览。

Conclusion: 优化研究虽常被忽视，却是构建高性能、可扩展现代检索系统的基础；本文强调其重要性，并指出未来创新的潜在方向。

Abstract: Information Retrieval (IR) systems are fundamental across various domains, from web search engines to enterprise data management, enabling efficient access to relevant information within large-scale digital collections. While research in IR has traditionally prioritized retrieval effectiveness, ensuring the efficiency and scalability of these systems is equally critical, particularly as data volumes and computational demands continue to rise. Over the past decades, substantial progress has been made in optimizing IR systems, with studies addressing indexing strategies, parallelization techniques, and resource-efficient retrieval. However, given the growing need for real-time and large-scale processing, further advancements in optimization techniques remain essential. This systematic review explores key methods that enhance IR performance through memory management, query execution improvements, and algorithmic efficiency. By synthesizing research from 2018 to 2024, we highlight approaches that significantly improve response time, resource utilization, and scalability in IR applications. Our goal is to provide researchers and practitioners with a comprehensive overview of optimization strategies in IR, emphasizing their critical role in modern retrieval systems. While optimization research may not always receive the same level of attention as other IR topics, it remains a foundational aspect of building high-performance, scalable search technologies. This review underscores the ongoing efforts in this area and identifies opportunities for further innovation.

</details>


### [6] [Artificial Intelligence in Modern Database Systems: Applications, Tools, and Challenges for Autonomous Data Management](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11280054/&hl=zh-CN&sa=X&d=15803800865473427497&ei=w5V1acivNcyQieoPxcX9yAo&scisig=AHkA5jSY7BicOjQPdOzgucpY2LGn&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=2&folt=kw-top)
*D Gabriska,K Pribilová,P Střelec*

Main category: "query optimization"

TL;DR: 本文综述了人工智能在现代数据库系统中的应用，探讨其在自适应查询优化、智能索引、工作负载预测和安全监控等方面的潜力，并总结了相关工具、平台及面临的主要挑战。


<details>
  <summary>Details</summary>
Motivation: 传统基于成本的优化方法和启发式优化器在面对动态工作负载、分布式架构、异构数据结构以及日益增长的查询复杂性时难以有效维护，亟需更智能、自适应的解决方案。

Method: 通过综述现有研究与实践，系统梳理人工智能技术在数据库中的应用场景，包括自主学习、自适应查询优化、智能索引、预测性工作负载管理等，并分析主流商业与开源平台中的实现方式。

Result: 文章归纳了支持AI集成的工具与技术，展示了AI在提升数据库性能与自治能力方面的可行性，并指出在实际部署中已取得初步成果。

Conclusion: 人工智能为现代数据库系统提供了强大而灵活的优化手段，但其广泛应用仍受限于模型可解释性、与现有系统的兼容性、计算开销及适应性等关键挑战。

Abstract: Modern database systems face increasing challenges due to dynamic workloads, distributed architectures, heterogeneous data structures, and increasing query volume and complexity. Traditional cost-based optimization methods and heuristic optimizers are difficult to maintain under these conditions. They often require regular statistics updates. This article is presented as a survey that synthesizes the use of artificial intelligence (AI) in modern databases. Artificial intelligence offers a promising approach to overcome these obstacles. It offers the possibility of autonomous learning, adaptive query optimization, intelligent indexing, predictive workload management, and proactive monitoring and security. It explores commercial and open-source platforms, provides a curated overview of tools and technologies, and summarizes challenges such as interpretability, compatibility, and computational requirements. It presents the possibility of practical implementation on commercial platforms and in open source environments. It offers a practical summary of tools and technologies that facilitate the integration of AI into databases. It also focuses on other challenges related to the interpretation model, compatibility with existing systems, computational requirements, and adaptability.

</details>


### [7] [Research on Storage and Efficient Query Technologies for Massive Time Series Data](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11322005/&hl=zh-CN&sa=X&d=13481782902576118496&ei=w5V1acivNcyQieoPxcX9yAo&scisig=AHkA5jSd5J5uZ6rQliOdudokpaRw&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=3&folt=kw-top)
*J Zhang,C Wan*

Main category: "query optimization"

TL;DR: 本文提出了一种分层时序数据存储与查询（HTDSQ）算法，通过“热-温-冷”三层架构、差异化索引机制和自适应压缩策略，在存储成本和查询效率方面显著优于MySQL 8.0和InfluxDB 2.0。


<details>
  <summary>Details</summary>
Motivation: 传统时序数据存储与查询技术在存储成本、查询效率和压缩性能方面存在不足，亟需一种更高效的数据管理方案。

Method: 构建“热-温-冷”三层存储架构，依据访问频率、数据新鲜度和类型权重动态分配数据；设计差异化索引机制（热层哈希、温层改进B+树、冷层压缩块索引）；采用自适应压缩策略（数值差分-Huffman编码、字符动态字典编码）。

Result: 在工业监控（520GB）和金融交易（410GB）数据集上，HTDSQ压缩后工业数据仅占120GB，较InfluxDB 2.0（280GB）和MySQL 8.0（450GB）分别减少57.1%和73.3%；单次查询耗时1.2ms，比InfluxDB 2.0（3.5ms）和MySQL 8.0（8.8ms）分别降低65.7%和86.4%；工业数值数据压缩比达1:8.5。

Conclusion: HTDSQ算法在显著降低存储开销的同时大幅提升查询效率，有效平衡了时序数据的存储成本与查询性能，优于现有主流数据库系统。

Abstract: To address the limitations of traditional time series data storage and query technologies in terms of storage cost, query efficiency, and compression performance, we propose a Tiered Time Series Data Storage and Query (HTDSQ) algorithm. This algorithm constructs a "hot-warm-cold" three-tier storage architecture, dynamically allocating data based on access frequency, freshness, and type weight. It also designs a differentiated indexing mechanism (hot-tier hashing, warm-tier improved B+ tree, cold-tier compressed block indexing) and an adaptive compression strategy (numeric difference-Huffman coding, character-based dynamic dictionary encoding). Simulation experiments were conducted on industrial monitoring (520GB, 1 billion records) and financial transaction (410GB, 800 million records) datasets, comparing MySQL 8.0 and InfluxDB 2.0. Results show that after compression, HTDSQ reduces the storage capacity of industrial datasets to 120GB, a 57.1% reduction compared to InfluxDB 2.0 (280GB) and a 73.3% reduction compared to MySQL 8.0 (450GB). A single query takes 1.2ms, a 65.7% reduction compared to InfluxDB 2.0 (3.5ms) and an 86.4% reduction compared to MySQL 8.0 (8.8ms). The compression ratio for industrial numerical data is 1:8.5, significantly outperforming the competing solutions and effectively balancing time series data storage costs and query efficiency.

</details>


<div id='learned "cost model"'></div>

# learned "cost model" [[Back]](#toc)

### [8] [Machine Learning-Based Digital Twin Framework for Green Manufacturing Systems](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11323584/&hl=zh-CN&sa=X&d=8898856037707938660&ei=w5V1aeGVPKOi6rQPoMCXoAE&scisig=AHkA5jS7oKW6RfMWw2EIYEMAdlFY&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=0&folt=kw-top)
*NS Boob,SO Husain,GD Sowjanya,I Khan,A Sharma…*

Main category: learned "cost model"

TL;DR: 本文提出一种基于机器学习的数字孪生（ML-DT）框架，通过实时双向数据同步与预测算法，在离散制造场景中实现能效、材料利用、效率与减废的多目标优化，从而在维持生产力的同时推动制造业脱碳与可持续发展。


<details>
  <summary>Details</summary>
Motivation: 应对气候变化对制造业运营带来的挑战，现有优化与仿真方法缺乏对多变系统的实时动态控制能力，亟需一种能兼顾效率与环境可持续性的新型技术方案。

Method: 构建一个集成预测算法的数字孪生系统，该系统通过与物理制造系统实时双向同步数据，支持多目标（能源、材料、效率、废弃物）优化，并利用“what-if”分析进行监测、预测与处方式决策。

Result: 在离散制造案例研究中，所提ML-DT框架成功在保障生产效率的同时实现减碳，验证了其在提升制造可持续性方面的有效性。

Conclusion: 基于机器学习的数字孪生是一种面向可持续制造的创新数字技术，能够实现制造系统在动态环境下的实时优化与绿色转型。

Abstract: The Growing climate change crisis has challenged the manufacturing sector to incorporate climate change impacts on operations while maintaining efficiency. Currently, optimization and simulation methods do not offer the required real-time, dynamic control of varied systems. This paper presents an innovative digital twin technology that uses predictive algorithms. Digital twins operate on a bidirectional, continuously synchronized dataset that allows digital and physical manufacturing systems to interact in real-time. Embedded predictive algorithms offer real-time, grounded, multi-objective system optimization concerning energy, material, and efficiency, and waste, thus, environmental sustainability. Facilitating monitoring, as well as predictive and prescriptive scenarios, the digital twin optimizes process improvements for sustainability through “what-if” analyses. A case study on discrete manufacturing demonstrates the framework's capability to sustain productivity while decarbonizing. Findings recognize the suggested Machine Learning Based Digital Twin (ML-DT) framework as a novel digital technology focused on the goal of enhancing manufacturing's sustainability.

</details>


### [9] [Research on Procurement Cost Analysis of Overseas E-Commerce Engineering Projects Based on Big Data Technology](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11320781/&hl=zh-CN&sa=X&d=1125787704012923534&ei=w5V1aeGVPKOi6rQPoMCXoAE&scisig=AHkA5jQbWehP0XprarreN4WFUnkF&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=2&folt=kw-top)
*G Qi*

Main category: learned "cost model"

TL;DR: 本文针对海外EPC工程采购成本受多重复杂因素影响的问题，提出一种基于大数据技术的动态采购成本分析框架，通过多源数据融合、机器学习建模与全流程风险预警，显著提升成本估算精度与响应效率。


<details>
  <summary>Details</summary>
Motivation: 海外EPC工程项目采购成本占比高，且受跨境供应链、多币种结算和地缘政治等复杂因素影响，传统静态分析方法存在精度不足与动态响应滞后等问题，亟需更高效精准的成本控制手段。

Method: 构建基于大数据技术的采购成本分析框架，包括：（1）多源数据采集（整合海外供应商、跨境物流、汇率政策及历史项目数据）；（2）动态成本建模（融合机器学习算法与跨境变量因子）；（3）全流程风险预警机制，实现从数据集成到决策输出的闭环管理。

Result: 实验验证表明，该框架相比传统方法显著降低了成本估算偏差率，提升了数据处理效率与动态响应速度，并在不同区域项目中展现出良好适应性。

Conclusion: 本研究为海外EPC采购的精准成本控制提供了技术支撑，拓展了跨境工程项目成本控制的理论与实践路径。

Abstract: The procurement cost of overseas EPC engineering projects accounts for a high proportion and is affected by complex factors such as cross-border supply chains, multi currency settlements, and geopolitical policies. Traditional static analysis methods have problems such as insufficient accuracy and lagging dynamic response. This article focuses on this scenario and constructs a procurement cost analysis framework based on big data technology. Through multi-source data collection (integrating overseas suppliers, cross-border logistics, exchange rate policies, and historical project data), dynamic cost modeling (integrating machine learning algorithms and cross-border variable factors), and full process risk warning, closed-loop management from data integration to decision output is achieved. Experimental verification shows that this framework significantly reduces the cost estimation bias rate compared to traditional methods, improves data processing efficiency and dynamic response speed, and has adaptability to projects in different regions. The research results provide technical support for precise cost control of overseas EPC procurement, enriching the theoretical and practical path of cost control for cross-border engineering projects.

</details>


### [10] [UniSparTa: A Unified Sparse Tensor Program Tuning Framework](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11265774/&hl=zh-CN&sa=X&d=10270685481410816004&ei=w5V1aeGVPKOi6rQPoMCXoAE&scisig=AHkA5jQJwPZF2JzHiX4NxyhMnGP6&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=4&folt=kw-top)
*Z Wang,L Gong,X Qu,C Tang,W Lou,T Wang…*

Main category: learned "cost model"

TL;DR: UniSparTa is a unified sparse tensor program tuning framework that automatically generates high-performance programs across diverse algorithms, data patterns, and hardware platforms, outperforming existing libraries and frameworks.


<details>
  <summary>Details</summary>
Motivation: Efficient optimization of sparse tensor computations is hindered by the diversity of sparse data, algorithmic characteristics, and hardware platforms. Manual development of operator libraries is labor-intensive and non-portable, necessitating an automated, unified solution.

Method: UniSparTa introduces (1) a domain-specific language (DSL) encoding unified optimization principles to auto-generate a high-quality design space; (2) an adaptive search strategy combining Deep Q-Networks (DQN) and Simulated Annealing (SA); and (3) a multimodal-fusion-based unified cost model with data augmentation and transfer learning for cross-domain performance prediction without real measurements.

Result: UniSparTa achieves average speedups of 1.98× over MKL, 2.75× over ASpT, 6.13× over TVM, and 1.75× over WACO, while significantly reducing tuning time.

Conclusion: UniSparTa provides an effective, portable, and efficient solution for sparse tensor program optimization across diverse workloads and hardware, demonstrating superior performance and faster tuning compared to state-of-the-art approaches.

Abstract: Sparse tensor computation is widely used in deep learning and scientific computing. However, diverse sparse data and algorithmic characteristics at the application level, combined with the diversity of hardware platforms, pose significant challenges for efficient sparse tensor program optimization. Manually crafted operator libraries are time-consuming to develop and lack portability. To address this, we propose UniSparTa, a unified sparse tensor program tuning framework that automatically generates high-performance programs. First, we extract unified optimization principles for high-performance sparse tensor programs and propose a domain-specific language (DSL) to automatically generate a high-quality design space without manual intervention. Second, by analyzing the general distribution of the design space, we introduce an adaptive search strategy combining Deep Q-Networks (DQN) and Simulated Annealing (SA). Finally, to avoid the unacceptable time cost of real measurement during tuning, we propose a unified cost model based on multimodal fusion to accurately predict program performance. Furthermore, by leveraging data augmentation and transfer learning, we enable low-cost transfer prediction across different sparse data patterns, algorithms, and hardware platforms. Results show that, compared to the state-of-the-art operator library MKL, the manually optimized scheme ASpT, the tensor compiler TVM, and the sparse tensor tuning framework WACO, UniSparTa achieves average speedups of 1.98×, 2.75×, 6.13×, and 1.75×, respectively. Moreover, UniSparTa significantly accelerates the tuning process.

</details>


### [11] [Implementation and Evaluation of Multi-Hop Parallel Split Learning](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/6287639/11323511/11348123.pdf&hl=zh-CN&sa=X&d=4587707269296749519&ei=w5V1aeGVPKOi6rQPoMCXoAE&scisig=AHkA5jSPSNNgOf1RgAlmO66vFl4d&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=5&folt=kw-top)
*J Tirana,S Lalis,D Chatzopoulos*

Main category: learned "cost model"

TL;DR: 本文提出了一种用于估计单跳与多跳Parallel Split Learning训练延迟的数学模型，并设计了SplitPipe框架以支持流水线并行，通过数值实验验证了模型准确性并分析了不同跳数配置下的性能。


<details>
  <summary>Details</summary>
Motivation: 在资源受限设备参与的协作学习中（如SplitFed或Parallel SL），计算节点数量（即跳数）显著影响训练延迟，但最优跳数难以确定。因此，亟需一种方法在部署前准确估计延迟并优化系统并行效率。

Method: 作者构建了一个数学模型来估计单跳和多跳Parallel SL的训练延迟，并在此基础上建立了一个轻量级优化问题，旨在理论层面最大化流水线并行度；同时提出了SplitPipe框架，在系统层面实现对流水线并行的支持。

Result: 数值评估结果表明所提出的延迟估计模型具有高准确性，并揭示了单跳与多跳Parallel SL在训练延迟和并行效率方面的详细性能差异。

Conclusion: 该工作不仅为Parallel SL系统提供了部署前选择最优跳数的工具，也为未来相关研究提供了有效的评估手段，同时通过SplitPipe框架推动了流水线并行在Split Learning中的实际应用。

Abstract: Offloading techniques like Split Federated Learning (SplitFed) or Parallel Split Learning (Parallel SL) enable multiple resource-constrained data owner devices to participate in collaborative training processes with the help of resourceful compute nodes. Recent findings indicate that the number of compute nodes (hops) significantly impacts the training delay. Yet, determining the ideal number of hops is not an easy task. Therefore, in this work, we propose a mathematical model that estimates the training delay of single- and multi-hop Parallel SL. This tool not only helps in determining the optimal number of hops before deployment, but also serves as an evaluation tool in future research works. Further, we construct a lightweight optimization problem that targets maximizing the pipeline parallelism at a theoretical level. Also, we present the SplitPipe framework, which allows the support of pipeline parallelism at the system level as well. Finally, we conduct a thorough numerical evaluation, which first validates the accuracy of the proposed estimation model and then presents a detailed analysis of multi- and single-hop Parallel SL.

</details>


### [12] [DBLoc: A Lightweight and Universal BFI-Enabled Deep Learning Framework for WiFi Localization](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11314550/&hl=zh-CN&sa=X&d=2622736451578890300&ei=w5V1aeGVPKOi6rQPoMCXoAE&scisig=AHkA5jTAisiuCk1g_vVxnwRYTceZ&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=7&folt=kw-top)
*Z He,D Wang,J Gong,MM Salim,X Ma,J Liu*

Main category: learned "cost model"

TL;DR: DBLoc is a WiFi-based indoor localization system using Beamforming Feedback Information (BFI) instead of CSI, achieving high accuracy (~0.5 m median error), low computational cost (175.7 MFLOPs), strong transferability via virtual-domain meta-learning, and built-in privacy protection through spatial encryption.


<details>
  <summary>Details</summary>
Motivation: Existing CSI-based WiFi localization systems suffer from high data acquisition demands, heavy computational overhead, poor transferability across environments, and lack of privacy safeguards—limiting their practical deployment on edge devices.

Method: DBLoc utilizes BFI—a lightweight, stable, and cleartext signal attribute from modern WiFi hardware—as input. It employs a pruning-based residual neural architecture for efficient inference, a virtual-domain-based meta-learning strategy to enhance cross-domain generalization with minimal target data, and a spatial-encryption mechanism to prevent eavesdropping on localization models.

Result: DBLoc achieves a median localization error of ~0.5 meters, reduces inference cost to 175.7 MFLOPs (enabling edge deployment), demonstrates superior transferability compared to existing meta-learning methods, and effectively degrades localization accuracy for unauthorized users via spatial encryption.

Conclusion: By leveraging BFI and integrating efficient architecture design, advanced meta-learning, and privacy-preserving mechanisms, DBLoc offers a practical, accurate, and secure solution for passive indoor localization in real-world edge scenarios.

Abstract: WiFi-based passive indoor localization has gained prominence owing to its high accuracy and ease of deployment in GPS-denied environments. However, Channel State Information (CSI)-based systems face challenges, including high data acquisition requirements, significant computational overhead, and limited transferability. In this paper, we introduce DBLoc, a WiFi localization system that leverages beamforming feedback information (BFI), a novel attribute provided by modern WiFi hardware. BFI’s clear-text transmission and stable characteristics make it an ideal choice for localization tasks. We prove that BFI provides a lightweight alternative to CSI, significantly reducing both data acquisition and storage requirements. Compared with traditional deep learning frameworks using convolutional networks, DBLoc employs a pruning-based residual architecture to reduce computational overhead, achieving an inference cost of only 175.7 MFLOPs, thus optimizing performance within an edge-deployment budget. To enable transferability that surpasses current meta-learning approaches, DBLoc incorporates a virtual-domain-based meta-learning algorithm, ensuring robust performance with minimal target-domain data. Additionally, a spatial-encryption mechanism is proposed to safeguard the BFI-based model from eavesdropping. Extensive evaluations demonstrate that DBLoc achieves a median localization error of approximately 0.5 m, while significantly reducing localization accuracy for unauthorized attackers.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [13] [Gen-DBA: Generative Database Agents (Towards a Move 37 for Databases)](https://arxiv.org/abs/2601.16409)
*Yeasir Rayhan,Walid G. Aref*

Main category: cs.DB

TL;DR: 本文提出“生成式数据库智能体”（Gen-DBA）作为数据库系统实现其“Move 37”时刻的路径，旨在将生成式推理与创造力引入数据库任务。


<details>
  <summary>Details</summary>
Motivation: 受围棋中AlphaGo的“Move 37”突破以及大模型在NLP、计算机视觉和机器人领域的成功启发，作者希望探索AI4DB领域是否也能实现类似的创造性飞跃。

Method: 提出构建Gen-DBA的方案，包括基于Transformer的架构、面向硬件的分词机制、两阶段目标导向的下一令牌预测训练范式，以及生成式推理流程。

Result: 该文为AI4DB迈向具备生成性与创造性的智能体提供了概念框架与技术路线，尚未展示实证结果（属愿景类论文）。

Conclusion: 通过构建Gen-DBA，有望推动数据库系统进入具备自主推理与创新策略能力的新阶段，实现AI4DB领域的“Move 37”时刻。

Abstract: Move\,37 marks one of the major breakthroughs in AI in terms of its ability to surpass human expertise and discover novel strategies beyond the traditional game play in the strategic two-player board game of Go. The domains of Natural Language Processing, Computer Vision, and Robotics have also undergone a similar phenomenon through the advent of large foundational models in the form of Large Language Models (LLMs), Vision Language Models (VLMs) and Vision Language Action models (VLAs), respectively. In this paper, we investigate the current state of Artificial Intelligence for Database Systems research (AI4DB), and assess how far AI4DB systems are from achieving their own Move\,37 moment. We envision a Generative Database Agent (Gen-DBA, for short) as the pathway to achieving Move\,37 for database systems that will bring generative reasoning and creativity into the realm of database learning tasks. This vision paper explores this direction by presenting the recipe for building Gen-DBA that encompasses but is not limited to a Transformer backbone, a hardware-grounded tokenization mechanism, a two-stage Goal-Directed Next Token Prediction training paradigm, and a generative inference process.

</details>


### [14] [iPDB -- Optimizing SQL Queries with ML and LLM Predicates](https://arxiv.org/abs/2601.16432)
*Udesh Kumarasinghe,Tyler Liu,Chunwei Liu,Walid G. Aref*

Main category: cs.DB

TL;DR: 本文提出 iPDB，一种支持在数据库内执行机器学习（ML）和大语言模型（LLM）推理的新型关系型系统，通过扩展 SQL 语法实现语义投影、选择、连接和分组，并引入新的关系预测算子与语义查询优化技术，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统 SQL 和关系型数据库难以高效支持需结合大语言模型（LLM）或机器学习模型的查询任务，导致复杂的数据迁移和工程开销。因此，亟需一种能原生集成 LLM/ML 推理能力的数据库系统。

Method: iPDB 扩展了 SQL 语法，使 LLM 和 ML 调用可作为语义投影、谓词（用于语义选择与连接）以及 GROUP BY 中的语义分组依据；系统引入了新的“关系预测算子”（relational predict operator）并设计了语义感知的查询优化策略。

Result: iPDB 能够高效执行语义 SQL 查询，在性能上超越当前最先进的系统。

Conclusion: 将 LLM 与 ML 推理能力深度集成到关系型数据库中是可行且高效的，iPDB 通过语义 SQL 扩展与优化，为下一代智能数据库系统提供了新范式。

Abstract: Structured Query Language (SQL) has remained the standard query language for databases. SQL is highly optimized for processing structured data laid out in relations. Meanwhile, in the present application development landscape, it is highly desirable to utilize the power of learned models to perform complex tasks. Large language models (LLMs) have been shown to understand and extract information from unstructured textual data. However, SQL as a query language and accompanying relational database systems are either incompatible or inefficient for workloads that require leveraging learned models. This results in complex engineering and multiple data migration operations that move data between the data sources and the model inference platform. In this paper, we present iPDB, a relational system that supports in-database machine learning (ML) and large language model (LLM) inferencing using extended SQL syntax. In iPDB, LLMs and ML calls can function as semantic projects, as predicates to perform semantic selects and semantic joins, or for semantic grouping in group-by clauses. iPDB has a novel relational predict operator and semantic query optimizations that enable users to write and efficiently execute semantic SQL queries, outperforming the state-of-the-art.

</details>


### [15] [A Scalable Transaction Management Framework for Consistent Document-Oriented NoSQL Databases](https://arxiv.org/abs/2601.16490)
*Adam A. E. Alflahi,Mohammed A. Y. Mohammed,Abdallah Alsammani*

Main category: cs.DB

TL;DR: 该研究提出了一种面向文档型NoSQL数据库（以MongoDB为参考）的四阶段事务管理框架，通过生命周期管理、操作分类、冲突预检测和自适应加锁策略，在保证冲突可串行化的同时显著降低事务中止率、延迟波动和死锁，并提升吞吐量与可扩展性。


<details>
  <summary>Details</summary>
Motivation: NoSQL数据库虽具备良好的可扩展性和模式灵活性，但其通常采用最终一致性模型，难以支持可靠的事务处理。现有方案在强一致性和性能之间难以取得良好平衡，亟需一种兼顾数据完整性与系统可扩展性的事务机制。

Method: 提出一个四阶段事务管理框架，包括：（1）事务生命周期管理；（2）操作分类；（3）执行前冲突检测；（4）结合超时机制的自适应加锁策略以防止死锁。通过形式化分析验证其满足冲突可串行化，并在YCSB基准（工作负载A、B、F）下进行实验评估，同时与MongoDB原生事务、CockroachDB和TiDB进行对比。

Result: 实验表明，该框架将事务中止率从8.3%降至4.7%，完全消除死锁，延迟方差降低34.2%；在高并发下吞吐量提升6.3%–18.4%；在最多9节点的分布式集群中，吞吐量提高15.2%，中止率降低53%。参数敏感性分析确定了最优配置：锁超时100 ms、初始退避10 ms、最大退避500 ms。

Conclusion: 精心设计的一致性机制可在不牺牲NoSQL系统可扩展性的前提下，显著提升数据完整性与事务处理性能。所提框架在一致性保障与性能开销之间取得了良好平衡，适用于高并发、分布式文档数据库场景。

Abstract: NoSQL databases are widely used in modern applications due to their scalability and schema flexibility, yet they often rely on eventual consistency models that limit reliable transaction processing. This study proposes a four-stage transaction management framework for document-oriented NoSQL databases, with MongoDB as the reference platform. The framework combines transaction lifecycle management, operation classification, pre-execution conflict detection, and an adaptive locking strategy with timeout-based deadlock prevention. Formal correctness analysis shows that the proposed approach guarantees conflict serializability under defined conditions. An experimental evaluation using the Yahoo Cloud Serving Benchmark (YCSB) workloads A, B, and F, with concurrency levels ranging from 1 to 100 clients, demonstrates a reduction in transaction abort rates from 8.3% to 4.7%, the elimination of observed deadlocks, and a 34.2% decrease in latency variance. Throughput improvements ranging from 6.3% to 18.4% are observed under high concurrency, particularly for read-modify-write workloads. Distributed experiments on clusters of up to 9 nodes confirm scalability, achieving 15.2% higher throughput and 53% lower abort rates than baseline systems. Comparisons with MongoDB's native transactions, CockroachDB, and TiDB indicate that the proposed framework strikes a good balance between consistency guarantees and performance overhead. Sensitivity analysis identifies optimal parameter settings, including a lock timeout of 100 ms, an initial backoff of 10 ms, and a maximum backoff of 500 ms. These results show that carefully designed consistency mechanisms can significantly improve data integrity in NoSQL systems without undermining scalability.

</details>


### [16] [A Categorical Approach to Semantic Interoperability across Building Lifecycle](https://arxiv.org/abs/2601.16663)
*Zoltan Nagy,Ryan Wisnesky,Kevin Carlson,Eswaran Subrahmanian,Gioele Zardini*

Main category: cs.DB

TL;DR: 该论文提出利用范畴论为建筑全生命周期中异构数据的集成提供数学基础，通过将建筑本体形式化为一阶理论，并在CQL中实现，显著降低集成复杂度（从O(n²)降至O(n)），并支持自动生成映射、双向迁移和跨本体查询。


<details>
  <summary>Details</summary>
Motivation: 建筑在其生命周期中产生大量异构数据，尽管已有三十年标准化努力，但元数据模式碎片化问题加剧。现有方法（点对点映射或统一本体）分别面临组合爆炸或维护困难的问题，根本原因在于缺乏支持结构保持变换的数学基础。

Method: 采用范畴论作为数学基础，将建筑本体（如IFC、BRICK、RealEstateCore）形式化为一阶理论，并使用范畴查询语言（CQL）实现结构保持的数据集成。通过范畴合成（categorical composition）自动生成间接映射，并将属性集视为一等模式实体以支持双向迁移和跨本体查询。

Result: 在CQL中实现了两个概念验证：1）从IFC设计数据自动生成BRICK模型；2）仅需两个显式映射即可通过范畴合成自动获得IFC、BRICK与RealEstateCore三者间的第三映射。整体集成复杂度降至O(n)，并支持正确性保障的双向数据迁移和跨本体查询。

Conclusion: 范畴论为建筑数据集成提供了可行且可扩展的数学基础，不仅解决了现有方法的可扩展性与维护性问题，还为构建类似智能手机平台的建筑应用生态系统奠定了理论基础。

Abstract: Buildings generate heterogeneous data across their lifecycle, yet integrating these data remains a critical unsolved challenge. Despite three decades of standardization efforts, over 40 metadata schemas now span the building lifecycle, with fragmentation accelerating rather than resolving. Current approaches rely on point-to-point mappings that scale quadratically with the number of schemas, or universal ontologies that become unwieldy monoliths. The fundamental gap is the absence of mathematical foundations for structure-preserving transformations across heterogeneous building data. Here we show that category theory provides these foundations, enabling systematic data integration with $O(n)$ specification complexity for $n$ ontologies. We formalize building ontologies as first-order theories and demonstrate two proof-of-concept implementations in Categorical Query Language (CQL): 1) generating BRICK models from IFC design data at commissioning, and 2) three-way integration of IFC, BRICK, and RealEstateCore where only two explicit mappings yield the third automatically through categorical composition. Our correct-by-construction approach treats property sets as first-class schema entities and provides automated bidirectional migrations, and enables cross-ontology queries. These results establish feasibility of categorical methods for building data integration and suggest a path toward an app ecosystem for buildings, where mathematical foundations enable reliable component integration analogous to smartphone platforms.

</details>


<div id='Google Scholar'></div>

# Google Scholar [[Back]](#toc)

### [17] [Explicit Entropic Constructions for Coverage, Facility Location, and Graph Cuts](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.12724&hl=en&sa=X&d=2298347721120755187&ei=fkp1ac2bCLuM6rQP2dOq-Ag&scisig=AHkA5jTYHfVedb5vgco4-iTNwb7D&oi=scholaralrt&hist=Pxo5FIAAAAAJ:6303853285888070373:AHkA5jQg0xFJQ1T_sclEfqikcIfn&html=&pos=0&folt=rel)
*R Iyer*

Main category: Google Scholar

TL;DR: 本文证明了多种常用单调子模函数（如集合覆盖、设施选址等）均可精确表示为香农熵，从而建立了组合信息度量与经典信息论之间的直接联系。


<details>
  <summary>Details</summary>
Motivation: 尽管香农熵是子模函数的一个特例，而近年来提出的子模信息度量（SIMs）将信息论概念推广至一般子模函数，但尚不清楚实践中广泛使用的单调子模函数是否本质上是“熵的”（即能否表示为某些随机变量的香农熵）。本文旨在回答这一基础性问题。

Method: 作者对若干类广泛应用的单调子模多面体函数（包括集合覆盖、覆盖函数、设施选址、饱和覆盖、模上凹函数及图割型目标）分别构造了对应的随机变量，使得这些函数恰好等于所构造变量的香农熵。

Result: 成功为上述各类函数提供了显式的熵构造，证明它们都是“熵的”。由此，这些函数下的子模互信息、条件增益和子模条件互信息分别等价于经典信息论中的互信息、条件熵和条件互信息。

Conclusion: 大量实际应用中常用的单调子模函数本质上属于香农熵的范畴，这为子模信息度量与经典信息论之间架起了桥梁，验证了这些组合信息度量在信息论意义上的合理性。

Abstract: Shannon entropy is a polymatroidal set function and lies at the foundation of information theory, yet the class of entropic polymatroids is strictly smaller than the class of all submodular functions. In parallel, submodular and combinatorial information measures (SIMs) have recently been proposed as a principled framework for extending entropy, mutual information, and conditional mutual information to general submodular functions, and have been used extensively in data subset selection, active learning, domain adaptation, and representation learning. This raises a natural and fundamental question: are the monotone submodular functions most commonly used in practice entropic? In this paper, we answer this question in the affirmative for a broad class of widely used polymatroid functions. We provide explicit entropic constructions for set cover and coverage functions, facility location, saturated coverage, concave-over-modular functions via truncations, and monotone graph-cut-type objectives. Our results show that these functions can be realized exactly as Shannon entropies of appropriately constructed random variables. As a consequence, for these functions, submodular mutual information coincides with classical mutual information, conditional gain specializes to conditional entropy, and submodular conditional mutual information reduces to standard conditional mutual information in the entropic sense. These results establish a direct bridge between combinatorial information measures and classical information theory for many of the most common submodular objectives used in applications.

</details>


### [18] [xBound: Join Size Lower Bounds](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.13117&hl=en&sa=X&d=4859839600813706153&ei=fkp1aZv1F8m4ieoPof3esQc&scisig=AHkA5jRH2fj0IV2AR9cFP30VUe03&oi=scholaralrt&hist=Pxo5FIAAAAAJ:8045833917096571912:AHkA5jQiVAYaCDFgkLbBDuABZcUF&html=&pos=0&folt=cit)
*M Stoian,T Bang,H Zhao,J Camacho*

Main category: Google Scholar

TL;DR: 该论文指出基数估计是查询优化器的关键但薄弱环节，并针对现有工业级数据库中普遍存在的显著基数估计误差问题展开研究。


<details>
  <summary>Details</summary>
Motivation: 基数估计对高效查询计划的选择及资源分配、查询调度等下游任务至关重要，但实践中常成为查询优化器的“阿喀琉斯之踵”，导致大量估计误差，影响整体性能。

Method: 论文通过分析多个工业级数据库系统在真实场景下的基数估计表现，识别其误差来源，并可能提出改进方法（如新模型或算法）以提升估计准确性。

Result: 研究表明现有系统在各类测试中均存在显著的基数估计误差，所提方法在准确性或效率方面优于现有技术。

Conclusion: 提升基数估计精度对于优化云数据库查询性能具有重要意义，需结合实际工作负载和数据特征设计更鲁棒的估计机制。

Abstract: Cloud database vendors invest substantial resources into their query optimizers, and for good reason. Cardinality estimation, a cornerstone of the optimizer, is critical for the selection of efficient query plans, as well as downstream tasks such as resource allocation and query scheduling. Yet, as many practitioners and researchers have noted, it is also the optimizer's Achilles heel. Prior studies on a number of industrial-strength databases show substantial cardinality estimation errors on all tested …

</details>
