<div id=toc></div>

# Table of Contents

- ["query optimization"](#"query optimization") [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [learned "cost model"](#learned "cost model") [Total: 4]


<div id='"query optimization"'></div>

# "query optimization" [[Back]](#toc)

### [1] [The Impact of Distributed Data Query Optimization on Large-Scale Data Processing](https://scholar.google.com/scholar_url?url=http://scholar-press.com/uploads/papers/MqdQVq1D1qRjjKwMLwfUca9PGUaGXxdTmWlltc1C.pdf&hl=zh-CN&sa=X&d=17354762022946573543&ei=hKtlaaWUIbux6rQP76-2-Qc&scisig=AHkA5jTdYYxy8ewfQL1lqWCldK0J&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=0&folt=kw-top)
*J Li*

Main category: "query optimization"

TL;DR: 本文针对分布式大数据系统中的查询性能瓶颈，提出包括优化数据分布、升级硬件和采用分布式锁等改进措施，有效提升了系统响应速度、数据准确性并降低了成本。


<details>
  <summary>Details</summary>
Motivation: 随着大数据技术的发展，分布式架构在处理海量数据时面临查询效率与系统性能的挑战，亟需解决存储延迟、硬件限制及数据一致性等问题。

Method: 分析分布式查询的关键性能瓶颈，并针对性地提出优化数据分布策略、定期升级硬件资源以及采用分布式锁定机制等方法。

Result: 实施优化措施后，系统查询响应速度加快，数据准确性得到保障，同时硬件与维护成本降低。

Conclusion: 所提出的优化方法能有效提升大规模数据处理系统的整体性能。

Abstract: With the rapid advancement of big data technology, distributed architecture has become the mainstream in the industry when processing massive amounts of information. However, when dealing with such large datasets, the query efficiency and performance of the system become key factors that constrain its response speed and accuracy. This study analyzed the key performance bottlenecks of distributed data queries, such as storage response latency, hardware processing capacity limits, and data consistency assurance. Based on this, a series of targeted improvement measures were proposed. Specifically, in terms of distributed storage network latency, hardware resource upgrade requirements, and data consistency maintenance, research has proposed solutions such as optimizing data distribution, regularly upgrading hardware facilities, and adopting distributed locking strategies. After implementing these optimization measures, query response can be accelerated, data accuracy can be ensured, and hardware costs and maintenance expenses can be reduced. The research results show that these optimization methods can enhance the overall performance of processing large-scale data systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [2] [CSQL: Mapping Documents into Causal Databases](https://arxiv.org/abs/2601.08109)
*Sridhar Mahadevan*

Main category: cs.DB

TL;DR: CSQL is a system that automatically converts unstructured text documents into a causal database (CDB) enabling SQL-style causal queries, supporting analyses such as identifying key causal influences or hubs across large document collections.


<details>
  <summary>Details</summary>
Motivation: Traditional retrieval systems (e.g., RAG or knowledge graphs) support only associative queries, not causal reasoning; there is a need for scalable tools that can answer “why” questions over textual corpora using formal causal models.

Method: CSQL extends the DEMOCRITUS system by compiling unstructured documents into local causal models derived from causal discourse, then aggregates them into a unified causal database with a query algebra that supports interventions and structured causal queries.

Result: CSQL successfully processes large-scale corpora like the Testing Causal Claims (TCC) dataset—265,656 causal claims from 45,319 economics papers over 44 years—and enables corpus-level and longitudinal causal queries, such as identifying dominant causal influences or hub variables.

Conclusion: CSQL provides a general-purpose framework for transforming unstructured text into queryable causal databases, offering a novel capability for domain-agnostic causal analysis in fields including science, business, and the humanities.

Abstract: We describe a novel system, CSQL, which automatically converts a collection of unstructured text documents into an SQL-queryable causal database (CDB). A CDB differs from a traditional DB: it is designed to answer "why'' questions via causal interventions and structured causal queries. CSQL builds on our earlier system, DEMOCRITUS, which converts documents into thousands of local causal models derived from causal discourse. Unlike RAG-based systems or knowledge-graph based approaches, CSQL supports causal analysis over document collections rather than purely associative retrieval. For example, given an article on the origins of human bipedal walking, CSQL enables queries such as: "What are the strongest causal influences on bipedalism?'' or "Which variables act as causal hubs with the largest downstream influence?'' Beyond single-document case studies, we show that CSQL can also ingest RAG/IE-compiled causal corpora at scale by compiling the Testing Causal Claims (TCC) dataset of economics papers into a causal database containing 265,656 claim instances spanning 45,319 papers, 44 years, and 1,575 reported method strings, thereby enabling corpus-level causal queries and longitudinal analyses in CSQL. Viewed abstractly, CSQL functions as a compiler from unstructured documents into a causal database equipped with a principled algebra of queries, and can be applied broadly across many domains ranging from business, humanities, and science.

</details>


### [3] [SVFusion: A CPU-GPU Co-Processing Architecture for Large-Scale Real-Time Vector Search](https://arxiv.org/abs/2601.08528)
*Yuchen Peng,Dingyu Yang,Zhongle Xie,Ji Sun,Lidan Shou,Ke Chen,Gang Chen*

Main category: cs.DB

TL;DR: SVFusion is a GPU-CPU-disk collaborative framework for real-time approximate nearest neighbor search that supports dynamic updates and achieves high throughput and low latency by combining hierarchical indexing, workload-aware caching, CUDA multi-stream optimization, and concurrency control.


<details>
  <summary>Details</summary>
Motivation: Existing ANNS systems face a trade-off: CPU-based methods support updates but have low throughput, while GPU-accelerated systems offer high performance but struggle with dynamic updates and limited GPU memory—creating a performance gap for large-scale, continuous vector search requiring both speed and accuracy.

Method: SVFusion employs a hierarchical vector index architecture with CPU-GPU co-processing, a workload-aware vector caching mechanism to optimize GPU memory usage, CUDA multi-stream optimization, adaptive resource management, and concurrency control to ensure consistency during concurrent queries and updates.

Result: SVFusion achieves 20.9x higher average throughput and 1.3x–50.7x lower query latency than baseline methods while maintaining high recall across large-scale datasets under diverse streaming workloads.

Conclusion: SVFusion effectively bridges the gap between high-performance GPU computation and dynamic update support in real-time ANNS, offering a scalable and efficient solution for large-scale vector search with strong accuracy, throughput, and latency guarantees.

Abstract: Approximate Nearest Neighbor Search (ANNS) underpins modern applications such as information retrieval and recommendation. With the rapid growth of vector data, efficient indexing for real-time vector search has become rudimentary. Existing CPU-based solutions support updates but suffer from low throughput, while GPU-accelerated systems deliver high performance but face challenges with dynamic updates and limited GPU memory, resulting in a critical performance gap for continuous, large-scale vector search requiring both accuracy and speed. In this paper, we present SVFusion, a GPU-CPU-disk collaborative framework for real-time vector search that bridges sophisticated GPU computation with online updates. SVFusion leverages a hierarchical vector index architecture that employs CPU-GPU co-processing, along with a workload-aware vector caching mechanism to maximize the efficiency of limited GPU memory. It further enhances performance through real-time coordination with CUDA multi-stream optimization and adaptive resource management, along with concurrency control that ensures data consistency under interleaved queries and updates. Empirical results demonstrate that SVFusion achieves significant improvements in query latency and throughput, exhibiting a 20.9x higher throughput on average and 1.3x to 50.7x lower latency compared to baseline methods, while maintaining high recall for large-scale datasets under various streaming workloads.

</details>


<div id='learned "cost model"'></div>

# learned "cost model" [[Back]](#toc)

### [4] [Real-Time Quantized YOLO Object Detection on Serverless Cloud Functions: An Experimental and Analytical Study](https://scholar.google.com/scholar_url?url=https://www.researchsquare.com/article/rs-8446241/latest.pdf&hl=zh-CN&sa=X&d=18049560817206778172&ei=hKtlae3BJbK16rQPitqooQg&scisig=AHkA5jS_wwHQE8rS0Ye0TRWWmKnW&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=0&folt=kw-top)
*MDR HASAN,S Biswas*

Main category: learned "cost model"

TL;DR: 本文系统研究了在无服务器平台（AWS Lambda）上部署量化目标检测模型（基于YOLO，使用ONNX Runtime）的延迟与成本权衡，发现INT8量化显著降低热启动推理延迟，但冷启动仍是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算因其自动扩展、细粒度计费和低运维开销，适用于机器学习推理等突发性负载；然而，在无服务器架构上部署实时计算机视觉流水线仍面临CPU-only执行、内存限制和冷启动开销等挑战。

Method: 在AWS Lambda上使用ONNX Runtime部署量化YOLO模型，通过实验与分析方法研究INT8量化对目标检测任务在实际部署约束下的延迟与成本影响。

Result: 实验结果表明，INT8量化显著降低了热启动推理延迟，但冷启动行为仍然是性能的主要瓶颈。

Conclusion: 尽管量化能有效优化热启动推理性能，但在当前无服务器平台上部署实时视觉应用仍受限于冷启动问题，需进一步优化启动机制或平台支持。

Abstract: Serverless computing has emerged as an attractive execution model for event-driven applicationsdue to its automatic scalability, fine-grained billing, and minimal operational overhead. Theseproperties make serverless platforms appealing for machine learning inference workloads with variableor bursty demand. However, deploying real-time computer vision pipelines on serverless infrastructuresremains challenging due to CPU-only execution, memory constraints, and cold-start overheads [5, 6]. This paper presents a systematic experimental and analytical study of quantized object detectiondeployed on serverless cloud functions. Using quantized YOLO-based models executed on AWSLambda with ONNX Runtime, we investigate latency–cost trade-offs under practical deploy-ment constraints. Experimental results show that INT8 quantization substantially reduces warm-startinference latency, while cold-start behavior remains the dominant bottleneck.

</details>


### [5] [Enhancing Elementary Literacy Through the'Class Lit-eracy Tree': A Strategic Case Study of the'One Week One Book'Program](https://scholar.google.com/scholar_url?url=https://jse.rezkimedia.org/index.php/jse/article/download/707/258&hl=zh-CN&sa=X&d=2159830123818094372&ei=hKtlae3BJbK16rQPitqooQg&scisig=AHkA5jReZdsox4Zw76HkptOp1z7O&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=1&folt=kw-top)
*N Sumartina,H Hidayat*

Main category: learned "cost model"

TL;DR: 本研究提出并验证了一种名为“班级识字树”（POLIKEL）的低成本、创新教学策略，结合“一周一书”（SAMI SAKU）项目，在印尼苏门当县两所小学五年级实施，显著提升了学生的阅读兴趣、文本理解能力和基础写作技能。


<details>
  <summary>Details</summary>
Motivation: 正式教育常因资源限制难以持续培养学生的读写素养，因此亟需一种低成本且可持续的教学策略来弥合这一差距。

Method: 采用质性描述性案例研究设计，在印尼苏门当县两所小学开展；通过观察、深度访谈和文档分析收集数据，并运用互动模型进行分析。

Result: “班级识字树”策略成功营造了愉悦、参与性强且目标导向的学习环境，有效促进学生持续阅读与写作习惯的养成；学生在阅读兴趣、对文本内在要素的理解及基础写作能力方面均有显著提升。

Conclusion: 该策略的成功依赖于强有力的学校领导与支持性的管理循环，验证了PDCA循环作为可持续教学创新框架的有效性，为小学基础读写教育提供了一种高效、可持续且可推广的替代模式。

Abstract: Creating a literate generation requires a sustained pedagogical process, a goal often challenged by resource limitations in formal education. This study addresses this gap by examining an innovative, low-cost strategy. Introduction This research describes a literacy learning strategy using a'Class Literacy Tree'(POLIKEL) to support the'One Week One Book'(SAMI SAKU) program and its impact on the reading interest and literacy skills of fifth-grade elementary students. Method A qualitative, descriptive case study design was employed at two elementary schools in Sumedang Regency, Indonesia. Data were collected via observation, in-depth interviews, and documentation analysis, then analyzed using an interactive model. Results The'Class Literacy Tree'strategy successfully creates a joyful, participatory, and goal-oriented learning environment. The program effectively fosters continuous reading and writing habits, with teachers acting as crucial facilitators. Students' consistent, tangible engagement with the'tree'demonstrated marked improvement in reading interest, comprehension of intrinsic text elements, and basic writing skills. Discussion The strategy's success is contingent on a supporting management cycle strong school leadership, validating the PDCA cycle as a framework for sustainable pedagogical innovation. This strategy serves as an effective, sustainable, and scalable alternative model to enhance foundational literacy in elementary education.





</details>


### [6] [Design under uncertainty of an isolated microgrid powered by offshore renewable energy sources](https://scholar.google.com/scholar_url?url=https://theses.hal.science/tel-05450784/file/EBNOU.pdf&hl=zh-CN&sa=X&d=13400850301655297292&ei=hKtlae3BJbK16rQPitqooQg&scisig=AHkA5jTGW6DaqZEaEEkeV6Xkf7PH&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=3&folt=kw-top)
*H Ebnou*

Main category: learned "cost model"

TL;DR: 本文提出了一种结合TimeGAN深度学习与多目标随机优化的孤立微电网优化设计方法，通过频率分解预处理提升长序列生成质量，并在留尼汪岛案例中验证了其在低碳、抗不确定性微电网设计中的有效性。


<details>
  <summary>Details</summary>
Motivation: 为应对离岸可再生能源微电网在电力需求与发电方面存在的不确定性，需开发一种能够兼顾多时间尺度特征、生成高质量场景并支持鲁棒系统设计的方法。

Method: 提出一种集成方法：首先对气候与用电数据进行频率分解预处理，再利用TimeGAN生成长期合成场景；随后基于统计与谱分析验证场景质量，并将其作为多目标随机优化模型的输入，用于微电网组件（风电、光伏、储能、柴油）的容量配置。

Result: 在留尼汪岛的实际案例中成功应用该方法，生成的场景有效捕捉了多尺度时序特征，优化结果展示了所设计微电网具备自主性、低碳性和对不确定性的强鲁棒性。

Conclusion: 所提出的结合频率分解、TimeGAN场景生成与多目标随机优化的框架，能有效支持高比例可再生能源孤立微电网的稳健、低碳设计，具有实际工程应用价值。

Abstract: This thesis presents a methodology for the optimal design of an isolated microgrid powered by offshore renewable resources (wind, solar, storage, diesel), while accounting for uncertainties in electricity demand and production. A comprehensive tool is developed, combining long-term scenario generation using the TimeGAN deep learning model with multi-objective stochastic optimization of the energy system. To address TimeGAN’s limitations on long sequences, a dedicated preprocessing step based on frequency decomposition is introduced, enabling the capture of multi-scale patterns in climatic and consumption data. The generated synthetic scenarios are rigorously validated using statistical and spectral criteria, and are then used as inputs for the sizing phase. The methodology is applied to a real case study on La Réunion Island, demonstrating its effectiveness for designing autonomous, low-carbon, and uncertainty-resilient energy systems.

</details>


### [7] [Microsoft Dynamics 365 Business Central Essentials](https://scholar.google.com/scholar_url?url=https://link.springer.com/content/pdf/10.1007/979-8-8688-2229-2.pdf&hl=zh-CN&sa=X&d=4295444228145093959&ei=S9JmaYjRO-6TieoP57SlmQ4&scisig=AHkA5jT5orCbQcUONRsnAo7dbiI-&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=0&folt=kw-top)
*CCM Simple,S Gomathi*

Main category: learned "cost model"

TL;DR: Apress Pocket Guides提供科技行业前沿发展和实践的简明摘要，内容涵盖安全、人工智能、机器学习、云计算、Web开发、产品设计、编程技术及商业主题，适合时间紧张的专业人士快速阅读。


<details>
  <summary>Details</summary>
Motivation: 为时间有限的科技从业者提供易于吸收、快速阅读的行业关键主题指南。

Method: 通过出版短小精悍的系列图书，对科技行业多个核心领域进行简洁总结。

Result: 形成覆盖现代科技行业广泛主题的便携式专业指南系列。

Conclusion: Apress Pocket Guides成功满足了专业人士对高效获取前沿技术与实践知识的需求。

Abstract: Apress Pocket Guides present concise summaries of cutting-edge developments and working practices throughout the tech industry. Shorter in length, books in this series aims to deliver quick-to-read guides that are easy to absorb, perfect for the time-poor professional. This series covers the full spectrum of topics relevant to the modern industry, from security, AI, machine learning, cloud computing, web development, product design, to programming techniques and business topics too.

</details>
