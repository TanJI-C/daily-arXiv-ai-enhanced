<div id=toc></div>

# Table of Contents

- ["query optimization"](#"query optimization") [Total: 1]
- [learned "cost model"](#learned "cost model") [Total: 4]
- [Ziniu Wu](#Ziniu Wu) [Total: 2]


<div id='"query optimization"'></div>

# "query optimization" [[Back]](#toc)

### [1] [Optimus: Deployable Query Optimization via Novel SQL Rewrites](https://scholar.google.com/scholar_url?url=https://openreview.net/forum%3Fid%3DZVzYv7Qois&hl=zh-CN&sa=X&d=8376193066075901600&ei=6ZFuaenOO7ui6rQPrda9qQc&scisig=AHkA5jTLjnx1W_57zUcFKN3puylq&oi=scholaralrt&hist=Pxo5FIAAAAAJ:10189913451943534390:AHkA5jTEDuZMpn0kzIrVKhRpZyjA&html=&pos=0&folt=kw-top)
*R Lone*

Main category: "query optimization"

TL;DR: Optimus improves query optimization by expanding the action space with novel execution plan rewrites, achieving a 1.16x speedup over vanilla PostgreSQL on the extended JOB benchmark without engine modifications.


<details>
  <summary>Details</summary>
Motivation: Traditional learned query optimizers are limited to optimizing over a fixed set of configurations, restricting the potential plan space and performance gains.

Method: Optimus mines novel execution plan rewrites and selects among them online using graph-based inductive matrix completion and a multilayer perceptron, aiming to minimize query latency. It is designed for deployment without requiring database engine changes and includes guard-checked compilation for safety.

Result: On the extended JOB benchmark, Optimus achieves a 1.16x speedup compared to vanilla PostgreSQL using only its novel rewrites.

Conclusion: By expanding the optimizer’s action space with safely deployable, learned rewrites, Optimus demonstrates improved query performance without modifying the underlying database engine.

Abstract: Learned database query optimizers typically optimize over a set of configurations, which limits the attainable plan space. Optimus expands the action space itself by mining novel execution plan rewrites and learns to select among these actions online. Optimus utilizes graph-based inductive matrix completion and a multilayer perceptron with the objective of minimizing latency. Crucially, the system is deployable by design: it requires no engine modifications and its rules include guard-checked compilation. On the extended JOB benchmark, Optimus yields a speedup of 1.16x over vanilla PostgreSQL solely using novel rewrites.

</details>


<div id='learned "cost model"'></div>

# learned "cost model" [[Back]](#toc)

### [2] [Chiplet Marketplace](https://scholar.google.com/scholar_url?url=https://chiplet-marketplace.com/library/article/exploring-the-potential-of-wireless-enabled-multi-chip-ai-accelerators&hl=zh-CN&sa=X&d=15905730971111581745&ei=6pFuadGBDbK16rQP872hoQc&scisig=AHkA5jSmEA-IqNXHZLGuwcPOmbJJ&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=2&folt=kw-top)
*E Irabor,M Musavi,A Das,S Abadal*

Main category: learned "cost model"

TL;DR: 该论文提出了一种针对机器学习模型异构性需求的架构评估方法，基于SET映射工具并定制了成本模型。


<details>
  <summary>Details</summary>
Motivation: 现有架构评估方法难以满足日益多样化和异构的机器学习模型的需求。

Method: 采用基于SET的映射工具（如SCAR中使用的映射器），并定制成本模型以评估架构成本。

Result: 成功实现了对面向机器学习模型的架构的成本评估，提升了对异构模型支持的适配能力。

Conclusion: 通过结合SET映射工具与定制成本模型，该方法有效应对了机器学习模型异构性带来的架构设计挑战。

Abstract: … needs of the evolving and heterogeneous pool of Machine Learning (ML) models in the literature. In … SET [30] mapper tool, and a cost model was customized to evaluate the cost of architecture. … The mapper used in SCAR is also based on SET …

</details>


### [3] [GRANII: Selection and Ordering of Primitives in GRAph Neural Networks using Input Inspection](https://scholar.google.com/scholar_url?url=https://iacoma.cs.uiuc.edu/iacoma-papers/cgo26.pdf&hl=zh-CN&sa=X&d=14789439206722274806&ei=6pFuadGBDbK16rQP872hoQc&scisig=AHkA5jQhTAP8TFaFo7sH4CZZPq-l&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=3&folt=kw-top)
*D Lenadora,V Sathia,G Gerogiannis,S Yesil…*

Main category: learned "cost model"

TL;DR: GRANII is a system that accelerates GNNs by exploring different matrix re-associations of computations, selecting the optimal sparse-dense matrix composition based on input characteristics, achieving up to 1.56× inference and 1.4× training speedups.


<details>
  <summary>Details</summary>
Motivation: Existing GNN acceleration frameworks overlook performance opportunities from different matrix re-associations, which exhibit input-sensitive behavior; this work aims to exploit such re-associations for better performance.

Method: GRANII uses a two-stage approach: (1) an offline stage that enumerates and prunes unprofitable matrix re-associations using input-oblivious techniques, and (2) an online runtime stage that selects the best re-association via lightweight cost models based on input graph and embedding sizes.

Result: GRANII achieves a geometric mean speedup of 1.56× for inference and 1.4× for training across various GNN models, systems, and configurations, and works effectively with sampling and diverse implementations.

Conclusion: Matrix re-association in GNN computations offers significant, input-dependent performance gains, and GRANII effectively harnesses this opportunity through a hybrid offline–online optimization framework.

Abstract: Over the years, many frameworks and optimization techniques have been proposed to accelerate graph neural networks (GNNs). In contrast to the optimizations explored in these systems, we observe that different matrix re-associations of GNN computations lead to novel input-sensitive performance behavior. We leverage this observation to propose GRANII, a system that exposes different compositions of sparse and dense matrix primitives based on different matrix re-associations of GNN computations and selects the best among them based on input attributes. GRANII executes in two stages:(1) an offline compilation stage that enumerates all valid re-associations leading to different sparse-dense matrix compositions and uses inputoblivious pruning techniques to prune away clearly unprofitable candidates, and (2) an online runtime system that explores the remaining candidates and uses light-weight cost models to select the best re-association based on the input graph and the embedding sizes. On a wide range of configurations, GRANII achieves a geo-mean speedup of 1.56× for inference and 1.4× for training across multiple GNN models and systems. We also show GRANII’s technique functions on diverse implementations and with techniques such as sampling.

</details>


### [4] [Algorithm-Hardware Co-Design of AdderNet Based Accelerators for Edge Intelligences](https://scholar.google.com/scholar_url?url=https://search.proquest.com/openview/e98a830fca19cabb52ea92608e50c2f1/1%3Fpq-origsite%3Dgscholar%26cbl%3D18750%26diss%3Dy&hl=zh-CN&sa=X&d=9359701451023318854&ei=6pFuadGBDbK16rQP872hoQc&scisig=AHkA5jS8McOBXXsSAZQmxcsYAmhJ&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=5&folt=kw-top)
*Y Zhang*

Main category: learned "cost model"

TL;DR: 该论文提出一种算法-硬件协同设计框架，以提升资源受限的可穿戴与边缘设备中近传感器和传感器内处理的效率。


<details>
  <summary>Details</summary>
Motivation: 现有算法与硬件独立开发的方式在资源受限的可穿戴和边缘设备中导致效率低下，难以满足实时数据处理对能效、延迟和性能的需求。

Method: 构建一个集成的算法-硬件协同设计框架，使定制化算法能够充分利用专用硬件架构的优势。

Result: 该框架显著提升了处理效率，降低了能耗并减少了延迟。

Conclusion: 算法与硬件的协同设计对于优化边缘和可穿戴设备中的近/在传感器处理至关重要，是实现高效、低功耗、低延迟系统的关键路径。

Abstract: The rapid growth of wearable and edge devices has revolutionized the way we collect and analyze data, enabling real-time insights across various applications, from health monitoring to smart environment management. This dissertation addresses the challenges associated with efficient near and in-sensor processing, emphasizing the critical role of algorithm-hardware co-design in optimizing performance within resource-constrained environments. Through a comprehensive examination of existing methodologies, this research highlights the inefficiencies that arise when algorithms and hardware are developed in isolation. By proposing an integrated co-design framework, the study demonstrates how tailored algorithms can leverage specialized hardware architectures to significantly enhance processing efficiency, reduce energy consumption, and minimize latency.

</details>


### [5] [Advanced Techniques for Hall Thruster Research and Development](https://scholar.google.com/scholar_url?url=https://search.proquest.com/openview/74c2087cd9103554c88e2786f10dd3c5/1%3Fpq-origsite%3Dgscholar%26cbl%3D18750%26diss%3Dy&hl=zh-CN&sa=X&d=13511889157824109156&ei=6pFuadGBDbK16rQP872hoQc&scisig=AHkA5jRWGUncZsHn2RHmPasitIDN&oi=scholaralrt&hist=Pxo5FIAAAAAJ:15658717702251462117:AHkA5jRs58Sh7gAGjO8tMulsiOso&html=&pos=6&folt=kw-top)
*P Thoreau*

Main category: learned "cost model"

TL;DR: 本文综述了霍尔推进器运行表征的四种先进方法，旨在提升其在研发与飞行任务中的测试效率与精度。


<details>
  <summary>Details</summary>
Motivation: 尽管霍尔推进器自1972年首次商业飞行以来已取得显著发展，但其运行表征方法仍相对传统，亟需更高效、快速和精确的新型技术以支持进一步开发与在轨应用。

Method: 提出了四种主要的先进表征方法，用于更有效地评估霍尔推进器的工作状态。

Result: 通过这四种方法，可实现对霍尔推进器操作特性更高效、准确和快速的表征。

Conclusion: 采用先进表征技术能够显著提升霍尔推进器在开发和飞行阶段的测试能力，为未来电推进系统优化提供支撑。

Abstract: Hall thrusters are the most common form of electric propulsion on spacecraft currently in Earth orbit, they offer excellent efficiency, high thrust to power, and have relatively simple power processing units. They have seen significant development since their first commercial flight in 1972, however, methods for characterizing their operation for both development and flight have remained relatively consistent. Therefore, advanced techniques have been developed to more effectively, efficiently, and expeditiously characterize thruster operation. These methods have been advanced through four primary approaches.

</details>


<div id='Ziniu Wu'></div>

# Ziniu Wu [[Back]](#toc)

### [6] [Hint Based Query Optimization with LLM Agent and Plan Similarity](https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Nikita-Vasilenko-5/publication/398845655_Hint_Based_Query_Optimization_with_LLM_Agent_and_Plan_Similarity/links/69461f6c27359023a00e42b0/Hint-Based-Query-Optimization-with-LLM-Agent-and-Plan-Similarity.pdf&hl=zh-CN&sa=X&d=11322770814657841974&ei=VjRvaZepE5Sw6rQPxrTySQ&scisig=AHkA5jTWTluCMyndgUwojVgLjKLT&oi=scholaralrt&hist=Pxo5FIAAAAAJ:11734074576040036222:AHkA5jT4qH64J8nZejvQlyvZo1xT&html=&pos=0&folt=rel)
*N Vasilenko,A Demin,V Burlakov*

Main category: Ziniu Wu

TL;DR: 该论文提出了一种基于提示（hint-based）的查询优化架构，包含一个快速模块和一个由大语言模型驱动的智能体引导模块，前者通过预训练语言模型嵌入物理计划并生成安全提示向量，后者在数据库反馈下迭代优化提示；初步实验表明两个模块分别能减少约20%平均运行时间并降低试错次数。


<details>
  <summary>Details</summary>
Motivation: 传统基于代价的SQL优化器在真实世界数据和多样化执行引擎下表现脆弱，微小的基数或代价估计误差可能导致次优执行计划。

Method: 提出一种基于提示的查询优化架构：1）快速模块将物理计划编码为文本，利用预训练语言模型嵌入并检索最近邻，生成带安全校验的二进制提示向量；2）智能体引导模块使用微调的大语言模型，在严格评估预算下通过与数据库交互进行迭代提示优化。两模块通过标准提示机制集成，无需修改数据库引擎。

Result: 在JOB CEB基准上的初步模块级实验显示，快速模块平均运行时间减少约20%，智能体引导模块所需的试错执行次数少于贪婪基线。

Conclusion: 论文聚焦于提示驱动优化的架构设计与模块行为，验证了其在减少运行时间和优化效率方面的潜力，但未进行端到端系统级测试，将其留作未来工作。

Abstract: Cost based SQL optimizers remain brittle under real world data and engine diversity; small errors in cardinality or cost estimates can cascade into poor plans. We propose a conceptual architecture for hint based query optimization and describe two modules in detail: a fast module which encodes physical plans as text, embeds with a pretrained language model, retrieves nearest neighbors, and produces a compact binary hint vector with a safety check against the default plan; and an agent guided module powered by a fine tuned Large Language Model, which refines hints through iterative search with database feedback under a strict evaluation budget. We specify interfaces, data flow, and decision policies for both modules, and outline integration through standard hint mechanisms without engine changes. Preliminary module level results on JOB CEB indicate roughly twenty percent average runtime reduction for the fast module and fewer trial executions than a greedy baseline for the agent guided module. The paper focuses on architecture and module behavior rather than a single end to end deployment, so system wide testing is intentionally out of scope.

</details>


### [7] [Complexity of Evaluating GQL Queries](https://scholar.google.com/scholar_url?url=https://hal.science/hal-05462926/&hl=zh-CN&sa=X&d=13181890233502293177&ei=VjRvaZepE5Sw6rQPxrTySQ&scisig=AHkA5jQ1oRfmaxjV5ti4lWHhTpv2&oi=scholaralrt&hist=Pxo5FIAAAAAJ:11734074576040036222:AHkA5jT4qH64J8nZejvQlyvZo1xT&html=&pos=1&folt=rel)
*D Figueira,A Lin,L Peterfreund*

Main category: Ziniu Wu

TL;DR: 该论文确定了图查询语言GQL核心片段及其带路径限制符扩展的数据复杂性：一般情况下为P^NP[log]-完全，无限制符时降为NL-完全，即使涉及实数等无限数据域亦成立。


<details>
  <summary>Details</summary>
Motivation: GQL作为图数据库的标准查询语言，其查询求值的复杂性问题尚未解决，尤其缺乏对核心语言片段的系统复杂性分析。

Method: 利用嵌入有限模型理论（embedded finite model theory）技术，建立GQL与关系数据库查询语言（如带传递闭包的关系演算和二阶逻辑片段）之间的紧密联系，并据此进行复杂性分析。

Result: 证明了GQL核心片段及其带路径限制符扩展的数据复杂性分别为NL-完全和P^NP[log]-完全，且该结果在包含实数等无限具体域的情况下依然成立。

Conclusion: GQL查询求值的复杂性已在其核心片段上得到完整刻画，揭示了其与关系数据库查询语言在表达能力和复杂性上的深刻联系。

Abstract: GQL has recently emerged as the standard query language over graph databases, particularly, property graphs. Indeed, this is analogous to the role of SQL for relational databases. Unlike SQL, however, fundamental problems regarding GQL are still unsolved, most notably the complexity of query evaluation. In this paper we provide a complete solution to this problem for the core fragment of GQL and for its extension with path restrictors. In particular, we show that the data complexity of these fragments is P^NP[log]-complete in general, and drops to NL-complete when restrictors are disallowed. Using techniques from embedded finite model theory, we show that this is true, even when the queries use data from infinite concrete domains such as real numbers with arithmetic. In proving these results, we establish and exploit tight connections between GQL and query languages over relational databases, especially extensions of relational calculus with transitive closure operators and fragments of second-order logic.

</details>
